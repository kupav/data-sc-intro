{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Statistics\n",
    "\n",
    "Part of [\"Introduction to Data Science\" course](https://github.com/kupav/data-sc-intro) by Pavel Kuptsov, [kupav@mail.ru](mailto:kupav@mail.ru)\n",
    "\n",
    "Recommended reading for this section:\n",
    "\n",
    "1. Grus, J. (2019). Data Science From Scratch: First Principles with Python (Vol. Second edition). Sebastopol, CA: Oâ€™Reilly Media\n",
    "\n",
    "The following Python modules will be required. Make sure that you have them installed.\n",
    "- `matplotlib`\n",
    "- `requests`\n",
    "- `numpy`\n",
    "- `scipy`\n",
    "- `collections`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Statistics?\n",
    "\n",
    "Statistics is a strict elder sister of Data Science. This is a branch of mathematics that uses strict mathematical methods for extraction information from data. \n",
    "\n",
    "Data Science uses statistics but this is not a branch of Mathematics. In addition to statistical formulas it uses many new computer methods like machine learning. Often the approaches of Data Science are not well founded mathematically.\n",
    "\n",
    "One needs to distinguish Statistics as a science and statistic as a numerical characteristic of a set of data. \n",
    "\n",
    "Statistics (science) computes statistics (numerical values that describes date sets) using statistical algorithms.\n",
    "\n",
    "Statistics (numbers) are required to describe data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First look at the data and trivial statistics\n",
    "\n",
    "If a dataset is small we do not need a special efforts to describe it. We can merely observe it. \n",
    "\n",
    "For example let some student has the examination grades (5, 4, 5, 3), on a scale of 2 to 5. It is obvious from mere observation that the last exam was not very successful. \n",
    "\n",
    "But what if we have several hundred exam results? We can not observe these data. We need to describe them somehow. Statistical methods help. They allow to extract essential features from data that describe the data meaningfully. \n",
    "\n",
    "We will experiment with a dataset `unif_state_exam.csv` that contains a sample of Unified State Examination grades in physics (column `Phys`), mathematics (`Math`) and Russian language (`Lang`) applied to some university (the dataset is not synthetic, it is taken from a real university). Recall that each grade scale ranges from 1 to 100. One more column `Ach` contains marks added for personal achievements, for example, in sports. It ranges from 0 to 10. Finally, column `Tot` is a total grade.\n",
    "\n",
    "Now we will download this dataset and take only totals for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module allows to work with web pages\n",
    "import requests\n",
    "\n",
    "# This is an URL of a repository\n",
    "base_url = \"https://raw.githubusercontent.com/kupav/data-sc-intro/main/data/\"\n",
    "\n",
    "# We need this file\n",
    "file_name = \"unif_state_exam.csv\"\n",
    "\n",
    "# Here we downlaod the file\n",
    "web_data = requests.get(base_url + file_name)\n",
    "assert web_data.status_code == 200\n",
    "\n",
    "# Take a look at the data\n",
    "print(web_data.text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by line ends\n",
    "str_data = web_data.text.splitlines()\n",
    "print(str_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop out the header and split grades\n",
    "lst_data = [s.split(',') for s in str_data[1:]]\n",
    "print(lst_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only totals\n",
    "data_grd = [int(s[0]) for s in lst_data]\n",
    "print(data_grd[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first question to answer when describing data is how many of them we have. We can answer it with the help of `len` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Data size={len(data_grd)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is large enough to observe the data just by printing and looking at them.\n",
    "\n",
    "We are probably also interested in the largest and the smallest values. There are functions `max` and `min` for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Largest  grade={max(data_grd)}')\n",
    "print(f'Smallest grade={min(data_grd)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length, the largest and the smallest values are trivial statistics describing the data.\n",
    "\n",
    "Before going further let us plot a histogram.\n",
    "\n",
    "Since our data are integers we can compute an exact number of bins: one for each particular value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = max(data_grd) - min(data_grd) + 1\n",
    "print(nbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_grd, bins=max(data_grd) - min(data_grd) + 1);\n",
    "ax.set_xlabel(\"Total grade\")\n",
    "ax.set_ylabel(\"# of applicants\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more dataset will be `stud_activ.csv`. It describes student activity during one year: how many times he/she raised a hand (column `RH`), visited course resource (`Res`) and participated on discussion groups (`Disc`). We will take a number of rising hands `RH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module allows to work with web pages\n",
    "import requests\n",
    "\n",
    "# This is an URL of a repository\n",
    "base_url = \"https://raw.githubusercontent.com/kupav/data-sc-intro/main/data/\"\n",
    "\n",
    "# We need this file\n",
    "file_name = \"stud_activ.csv\"\n",
    "\n",
    "# Here we downlaod the file\n",
    "web_data = requests.get(base_url + file_name)\n",
    "assert web_data.status_code == 200\n",
    "\n",
    "# Take a look at the data\n",
    "print(web_data.text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by line ends\n",
    "str_data = web_data.text.splitlines()\n",
    "print(str_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop out the header and split\n",
    "lst_data = [s.split(',') for s in str_data[1:]]\n",
    "print(lst_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only RH\n",
    "data_rh = [int(s[0]) for s in lst_data]\n",
    "print(data_rh[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us have a first look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Data size     ={len(data_rh)}')\n",
    "print(f'Largest value ={max(data_rh)}')\n",
    "print(f'Smallest value={min(data_rh)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_rh, bins=max(data_rh) - min(data_rh) + 1);\n",
    "ax.set_xlabel(\"# of raised hand\")\n",
    "ax.set_ylabel(\"# of students\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe two clusters: active students and those who prefer to stay still."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central tendencies\n",
    "\n",
    "If we want to describe a dataset with only one value, this value obviously must be the most important in some sense.\n",
    "\n",
    "For example, inspecting the histogram of grades above we notice that there is an area where the most of data are located: many people have a grade near 200. \n",
    "\n",
    "These most important values, in the other words the values somehow typical for the dataset are called central tendencies.\n",
    "\n",
    "There are different definition of the central tendencies. We will consider the following:\n",
    "- arithmetic mean (or simply, mean)\n",
    "- median\n",
    "- mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mean\n",
    "\n",
    "Mean value is the sum of the data divided by its count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mean(data):\n",
    "    \"\"\"\n",
    "    Mean value of dataset.\n",
    "    \"\"\"\n",
    "    return sum(data) / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean by hands\n",
    "mean_by_hand = (12 + 13 + 18 + 11) / 4\n",
    "\n",
    "# gather the data into a list\n",
    "test_data = [12, 13, 18, 11]\n",
    "\n",
    "# use our function\n",
    "mean_by_func = my_mean(test_data)\n",
    "\n",
    "print(f'mean_by_hand={mean_by_hand}')\n",
    "print(f'mean_by_func={mean_by_func}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that mean depends on each value of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 12 to 12.5\n",
    "test_data = [12.5, 13, 18, 11]\n",
    "\n",
    "# observe that the mean is changed\n",
    "print(f'mean={my_mean(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find mean value for our main dataset and show it on the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = my_mean(data_grd)\n",
    "print(f'data_mean={data_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_grd, bins=max(data_grd) - min(data_grd) + 1);\n",
    "ax.set_xlabel(\"Total grade\")\n",
    "ax.set_ylabel(\"# of applicants\")\n",
    "\n",
    "data_mean = my_mean(data_grd)\n",
    "\n",
    "# this function plots a vertical line through the whole figure\n",
    "ax.axvline(data_mean, color='red', linewidth=3, label=f'mean at {int(data_mean)}')\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student activity dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = my_mean(data_rh)\n",
    "print(f'data_mean={data_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_rh, bins=max(data_rh) - min(data_rh) + 1);\n",
    "ax.set_xlabel(\"# of raised hand\")\n",
    "ax.set_ylabel(\"# of students\")\n",
    "\n",
    "data_mean = my_mean(data_rh)\n",
    "\n",
    "ax.axvline(data_mean, color='red', linewidth=3, label=f'mean at {int(data_mean)}')\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Median\n",
    "\n",
    "The median is a value in the middle of the dataset found after its sorting.\n",
    "\n",
    "Let us see how it is computed manually. Consider a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_data = [2, 14, 5, 8, 3, 12, 15, 6, 22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a value in the middle we have to sort this array first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_data = sorted(tst_data)\n",
    "size = len(srt_data)\n",
    "print(srt_data)\n",
    "print(f'size={size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has size 9. Numbering starts from 0 so that the middle number is `9//2=4`. Thus the median is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = srt_data[size//2]\n",
    "print(f'median={median}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a dataset has an even size the median is an average of its two middle values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_data = [2, 14, 5, 8, 3, 12, 15, 22]\n",
    "srt_data = sorted(tst_data)\n",
    "size = len(srt_data)\n",
    "print(srt_data)\n",
    "print(f'size={size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size is 8, numbering from 0:\n",
    "# first half:   0 1 2 3 \n",
    "# second half:  4 5 6 7\n",
    "\n",
    "# higher midpoint is 8 // 2 -> 4\n",
    "hi_midpoint = size // 2\n",
    "\n",
    "# lower midpoint is hi_midpoint - 1\n",
    "lo_midpoint = hi_midpoint - 1\n",
    "\n",
    "print(f'hi_midpoint={hi_midpoint}')\n",
    "print(f'lo_midpoint={lo_midpoint}')\n",
    "\n",
    "# now compute the median\n",
    "median = (srt_data[lo_midpoint] + srt_data[hi_midpoint]) / 2\n",
    "print(f'median={median}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_median(data):\n",
    "    \"\"\"\n",
    "    Median of a dataset.\n",
    "    \"\"\"\n",
    "    size = len(data)\n",
    "    srt_data = sorted(data)\n",
    "    if size % 2 != 0:\n",
    "        # odd length\n",
    "        midpoint = size // 2\n",
    "        return srt_data[midpoint]\n",
    "    else:\n",
    "        # even length\n",
    "        hi_midpoint = size // 2\n",
    "        lo_midpoint = hi_midpoint - 1\n",
    "        return (srt_data[lo_midpoint] + srt_data[hi_midpoint]) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we use the median in addition or instead of the mean? \n",
    "\n",
    "It is useful when the data is largely skewed by a small number of very large or very small values. In this case the mean becomes misleading. \n",
    "\n",
    "Consider a dataset of salaries. Both non-management and management employees are included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salaries, non-management and management employees\n",
    "salaries = [1200, 950, 1150, 1300, 950, 1150, 800, 1000, 900, 1100, 1200, 120000, 230000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is skewed: there are two extremely large values. \n",
    "\n",
    "Obviously the mean salary is not relevant. Most of the actual values are much less:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sal = my_mean(salaries)\n",
    "print(f'mean salary={mean_sal:8.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median salary is more adequate description of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_sal = my_median(salaries)\n",
    "print(f'median salary={median_sal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the mean, the median is not so sensitive to data variation. For example, if the largest value becomes larger or the smallest value smaller, the median remain unchanged. It is said that this is robust.\n",
    "\n",
    "Let us compute and show the median for our dataset of grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_grd, bins=max(data_grd) - min(data_grd) + 1);\n",
    "ax.set_xlabel(\"Total grade\")\n",
    "ax.set_ylabel(\"# of applicants\")\n",
    "\n",
    "data_mean = my_mean(data_grd)\n",
    "data_median = my_median(data_grd)\n",
    "\n",
    "print(f\"mean   = {data_mean:6.2f}\")\n",
    "print(f\"median = {data_median}\")\n",
    "\n",
    "ax.axvline(data_mean, color='red', linewidth=3, label=f'mean at {data_mean:6.2f}')\n",
    "ax.axvline(data_median, color='green', linewidth=3, label=f'median at {data_median}')\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the mean and the median are very close to each other. It indicates that our dataset is symmetrical: large and small values appear with similar frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our second dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_rh, bins=max(data_rh) - min(data_rh) + 1);\n",
    "ax.set_xlabel(\"# of raised hand\")\n",
    "ax.set_ylabel(\"# of students\")\n",
    "\n",
    "data_mean = my_mean(data_rh)\n",
    "data_median = my_median(data_rh)\n",
    "\n",
    "print(f\"mean   = {data_mean:5.2f}\")\n",
    "print(f\"median = {data_median}\")\n",
    "\n",
    "ax.axvline(data_mean, color='red', linewidth=3, label=f'mean at {data_mean:5.2f}')\n",
    "ax.axvline(data_median, color='green', linewidth=3, label=f'median at {data_median}')\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is not so symmetric and the median differs essentially form the mean. \n",
    "\n",
    "The median tells more about the situation: we observe that half of all students are very active, even though the mean activity is not so high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mode\n",
    "\n",
    "The mode is the most frequent value in a dataset.\n",
    "\n",
    "Let us first consider a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what number is the most frequent? \n",
    "nums = [5, 1, 2, 3, 2, 2, 2, 1, 2, 2, 2, 4, 1, 5, 2]\n",
    "\n",
    "# it will be easy to see if we sort it\n",
    "srt_nums = sorted(nums)\n",
    "print(srt_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mode of this set of numbers is `2`. It occurs `8` times, it is more often then the others.\n",
    "\n",
    "To compute the mode we need `Counter` from the module `collections`. It creates an object that counts how many times each value is encountered in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bins = Counter(nums)\n",
    "print(f'bins={bins}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way notice that we call a variable `bins`. This is because `Counter` actually produces the data for a histogram: it puts each value into its own bin and count them.\n",
    "\n",
    "The `Counter` class provides a method `.most_common(n)` that returns a list of two-items tuples with the `n` most frequent  elements and their respective counts. If `n` is omitted, then `.most_common()` returns all of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the most frequent elemnt\n",
    "print(bins.most_common(1))\n",
    "\n",
    "# these are two most frequent elements\n",
    "print(bins.most_common(2))\n",
    "\n",
    "# all elements\n",
    "print(bins.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the mode we just take the first most frequent element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_mode = bins.most_common(1)[0][0]\n",
    "print(f'nums_mode={nums_mode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if a dataset has several elements with identical frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new list with 1s and 2s that occur five times\n",
    "nums = [5, 1, 3, 2, 5, 2, 5, 2, 1, 2, 3, 1, 2, 1, 4, 1, 5]\n",
    "srt_nums = sorted(nums)\n",
    "print(srt_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach above will find only one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bins = Counter(nums)\n",
    "print(f'bins={bins}')\n",
    "print(f'most_common={bins.most_common()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the mode is a lits of two values: 1 and 2.\n",
    "\n",
    "The idea is the following: first take the largest frequency (this is five in our example), then select only those elements that have this frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq = bins.most_common(1)\n",
    "print(most_freq)\n",
    "most_freq = most_freq[0][1]\n",
    "print(most_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object `Counter` is a sort of dictionary. Iterations over it are organized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to check how we iterate over bins\n",
    "[[key, val] for key, val in bins.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add here the filtering: take only most frequent elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the most frequent elements\n",
    "[[key, val] for key, val in bins.items() if val == most_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable val stores frequency, key holds the elements.\n",
    "# We need only elements\n",
    "[key for key, val in bins.items() if val == most_freq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it. We have a list of modes for our dataset. Let us collect all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def my_mode(data):\n",
    "    \"\"\"\n",
    "    Returns a list of the most frequent elements\n",
    "    \"\"\"\n",
    "    bins = Counter(data)\n",
    "    most_freq = bins.most_common(1)[0][1]\n",
    "    return [key for key, val in bins.items() if val == most_freq]\n",
    "\n",
    "my_mode(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find the mode for our grades dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_grd, bins=max(data_grd) - min(data_grd) + 1);\n",
    "ax.set_xlabel(\"Total grade\")\n",
    "ax.set_ylabel(\"# of applicants\")\n",
    "\n",
    "data_mean = my_mean(data_grd)\n",
    "data_median = my_median(data_grd)\n",
    "data_mode = my_mode(data_grd)\n",
    "\n",
    "print(f\"mean   = {data_mean:6.2f}\")\n",
    "print(f\"median = {data_median}\")\n",
    "print(f\"mode   = {data_mode}\")\n",
    "\n",
    "# take each mode element if there are many of them\n",
    "for md in data_mode:\n",
    "    ax.axvline(md, color='cyan', linewidth=3, label=f'mode at {md}')\n",
    "    \n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our dataset has only one most frequent element and it almost coincides with the mean and the median. This is again due to the symmetry of the data distribution.\n",
    "\n",
    "The other dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_rh, bins=max(data_rh) - min(data_rh) + 1);\n",
    "ax.set_xlabel(\"# of raised hand\")\n",
    "ax.set_ylabel(\"# of students\")\n",
    "\n",
    "data_mean = my_mean(data_rh)\n",
    "data_median = my_median(data_rh)\n",
    "data_mode = my_mode(data_rh)\n",
    "\n",
    "print(f\"mean   = {data_mean:5.2f}\")\n",
    "print(f\"median = {data_median}\")\n",
    "print(f\"mode   = {data_mode}\")\n",
    "\n",
    "\n",
    "ax.axvline(data_mean, color='red', linewidth=3, label=f'mean at {data_mean:5.2f}')\n",
    "ax.axvline(data_median, color='green', linewidth=3, label=f'median at {data_median}')\n",
    "\n",
    "for md in data_mode:\n",
    "    ax.axvline(md, color='cyan', linewidth=3, label=f'mode at {md}')\n",
    "    \n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the most common activity is at level 80.\n",
    "\n",
    "The above function for the mode works properly only for datasets of integers. If the data are real numbers the result will be incorrect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantiles\n",
    "\n",
    "The quantile is a value under which a certain percent of data lies. The mode is 50% quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_quantile(data, q):\n",
    "    \"\"\"\n",
    "    The quantile, q in (0,1)\n",
    "    \"\"\"\n",
    "    inx = int(round(q * len(data)))\n",
    "    return sorted(data)[inx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simplified version of a function for quantiles. If `q * len(data)` is not integer we just round it. More accurate version interpolate values between two points. We did it for the median when the dataset had even length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often 25% quantiles are used. Since the whole range of data is split into four areas they are called the quartiles. \n",
    "- The first quartile (Q1): the middle number between the smallest one and the median.\n",
    "- The second quartile (Q2) is the median.\n",
    "- The third quartile (Q3) is the middle value between the median and the highest value.\n",
    "\n",
    "Often the quartile refers to a range of data:\n",
    "- quartile Q1: range from the smallest values to the point Q1\n",
    "- Q2: the range between Q1 and Q2 points\n",
    "and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_grd, bins=max(data_grd) - min(data_grd) + 1);\n",
    "ax.set_xlabel(\"Total grade\")\n",
    "ax.set_ylabel(\"# of applicants\")\n",
    "\n",
    "Q1 = my_quantile(data_grd, 0.25)\n",
    "Q2 = my_median(data_grd)\n",
    "Q3 = my_quantile(data_grd, 0.75)\n",
    "\n",
    "ax.axvline(Q1, color='C1', linewidth=3, label=f'Q1 at {Q1}')\n",
    "ax.axvline(Q2, color='C2', linewidth=3, label=f'Q2 at {Q2}')\n",
    "ax.axvline(Q3, color='C3', linewidth=3, label=f'Q3 at {Q3}')\n",
    "    \n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset represents examination grades: the higher values the better. Thus applicants from Q1 are the worst. \n",
    "\n",
    "Often for datasets where higher means better the quantiles and so the quartiles are defined with respect to the reversed sorting. In the other words the dataset can be sorted in the descending order.\n",
    "\n",
    "Let us redefine our quantile function to take it into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_quantile(data, q, reverse=False):\n",
    "    \"\"\"\n",
    "    The quantile, q in (0,1)\n",
    "    reverse=True means sorting the array in descending order\n",
    "    \"\"\"\n",
    "    inx = int(round(q * len(data)))\n",
    "    return sorted(data, reverse=reverse)[inx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_grd, bins=max(data_grd) - min(data_grd) + 1);\n",
    "ax.set_xlabel(\"Total grade\")\n",
    "ax.set_ylabel(\"# of applicants\")\n",
    "\n",
    "Q1 = my_quantile(data_grd, 0.25, reverse=True)\n",
    "Q2 = my_median(data_grd)\n",
    "Q3 = my_quantile(data_grd, 0.75, reverse=True)\n",
    "\n",
    "ax.axvline(Q1, color='C1', linewidth=3, label=f'Q1 at {Q1}')\n",
    "ax.axvline(Q2, color='C2', linewidth=3, label=f'Q2 at {Q2}')\n",
    "ax.axvline(Q3, color='C3', linewidth=3, label=f'Q3 at {Q3}')\n",
    "    \n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the best applicants belong to the first quartile Q1, those that are not so bad are in Q2. Applicants form Q3 have a chance to be not enrolled, and finally those from Q4 probably will not be enrolled. \n",
    "\n",
    "Quartiles (also for sorting in the descending order) for the student activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_rh, bins=max(data_rh) - min(data_rh) + 1);\n",
    "ax.set_xlabel(\"Total grade\")\n",
    "ax.set_ylabel(\"# of applicants\")\n",
    "\n",
    "Q1 = my_quantile(data_rh, 0.25, reverse=True)\n",
    "Q2 = my_median(data_rh)\n",
    "Q3 = my_quantile(data_rh, 0.75, reverse=True)\n",
    "\n",
    "ax.axvline(Q1, color='C1', linewidth=3, label=f'Q1 at {Q1}')\n",
    "ax.axvline(Q2, color='C2', linewidth=3, label=f'Q2 at {Q2}')\n",
    "ax.axvline(Q3, color='C3', linewidth=3, label=f'Q3 at {Q3}')\n",
    "    \n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the quartiles correspond to the data clustering. \n",
    "\n",
    "Real example of using quartiles: each scientific journals gets a rank according to its popularity: downloading, citing, submitted papers and so on. Then this rank is sorted in the descending order and split into quartiles. \n",
    "\n",
    "It is very prestigious for scientists to publish their papers in Q1 Journals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. File `\"bank_customers.csv\"` that you can find at `\"https://raw.githubusercontent.com/kupav/data-sc-intro/main/data/\"` describes bank clinets. It contains three columns: customer age (`CustAge`), period of relationship with the bank in months (`Months`), and credit limit (`CredLim`). Plot a histogram for the column `Month` and compute its mean, median and the mode. Show these values on the histogram. Make a second copy of the histogram, compute quartiles and show them on the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Download the dataset from the previous exercise and do the same as in the exercise 1 for the column `CredLim`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data variability\n",
    "\n",
    "When we have described a data with the central tendency what can be said more? The second important value describing the data is their variability.  \n",
    "\n",
    "Do we have a lot of different values or all of them are basically the same?\n",
    "\n",
    "Data variability indicates the strength of the central tendency: the higher data variability the less strong is the central tendency.\n",
    "\n",
    "We will consider the following variability measures:\n",
    "- range\n",
    "- dispersion and standard deviation\n",
    "- interquartile range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Range\n",
    "\n",
    "The range is the largest value minus the smallest value. It can be called the full width of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_range(data):\n",
    "    return max(data) - min(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranges of our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'data_grd range={my_range(data_grd)}')\n",
    "print(f'data_rh  range={my_range(data_rh)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest value and the least informative. \n",
    "\n",
    "Consider two datasets of random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate two random datasets \n",
    "rng = np.random.default_rng()\n",
    "rand_data1 = np.concatenate([rng.normal(loc=0, scale=10, size=10000), \n",
    "                             rng.normal(loc=0, scale=1,  size=100000), \n",
    "                             rng.normal(loc=0, scale=2,  size=100000)])\n",
    "rand_data2 = np.concatenate([rng.normal(loc=0, scale=10, size=110000),\n",
    "                             rng.normal(loc=0, scale=7,  size=100000)])\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(5,6))\n",
    "\n",
    "# compute ranges\n",
    "rn1 = my_range(rand_data1)\n",
    "rn2 = my_range(rand_data2)\n",
    "\n",
    "axs[0].hist(rand_data1, bins=300, label=f\"range={rn1:5.2f}\");\n",
    "axs[1].hist(rand_data2, bins=300, label=f\"range={rn2:5.2f}\");\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although ranges of these two datasets are close to each other they have quite different variability. \n",
    "\n",
    "The range is robust value. It does not sensitive to each data value. Only the largest and smallest values matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Variance and the standard deviation\n",
    "\n",
    "The variance is a mean squared deviation of a value from its mean.\n",
    "\n",
    "In the other words\n",
    "- we compute the mean of the dataset\n",
    "- we compute the deviations: subtract each value and the mean\n",
    "- we compute the squared deviations\n",
    "- we compute the mean of the squared deviations, but \n",
    "divide by $N-1$\n",
    "\n",
    "The same procedure using mathematical notation. First we compute the mean:\n",
    "$$\n",
    "\\overline x = \\frac{1}{N}\\sum_{i=1}^N x_i.\n",
    "$$\n",
    "Then find the variance:\n",
    "$$\n",
    "\\text{Var}=\\frac{1}{N-1}\\sum_{i=1}^N (x_i - \\overline x)^2.\n",
    "$$\n",
    "\n",
    "Why divide by $N-1$? This not obvious. \n",
    "\n",
    "When we deal with a dataset this is typically a sample from a\n",
    "larger population. Thus $\\overline x$ is not the actual mean, this is its estimate. It results to\n",
    "an underestimate of the variance. To fix it we divide by $N-1$ instead of $N$.\n",
    "\n",
    "The variance is measured in squared data units. If, for example, the data are in meters the variance is in squared meters. To describe the data variability we need the same units. This is the case for the range. \n",
    "\n",
    "To obtain correct units we need to computed squared root of the variance. This is called a standard deviation and usually is denoted by $\\sigma$:\n",
    "$$\n",
    "\\sigma=\\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N (x_i - \\overline x)^2}.\n",
    "$$\n",
    "Let us first compute the standard deviation manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_data = [10.2, 12.3, 9.2, 8.4, 15.4, 11.2]\n",
    "\n",
    "# the mean \n",
    "mean = sum(tst_data) / len(tst_data)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squared deviations\n",
    "sqr_dev = [(x - mean)**2 for x in tst_data]\n",
    "print(sqr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance\n",
    "variance = sum(sqr_dev) / (len(sqr_dev) -  1)\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation\n",
    "sig = variance**0.5\n",
    "print(sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now wrap it to a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_stddev(data):\n",
    "    \"\"\"\n",
    "    Standard deviation.\n",
    "    \"\"\"\n",
    "    size = len(data)\n",
    "    mean = sum(data) / size\n",
    "    sqr_dev = [(x - mean)**2 for x in data]\n",
    "    variance = sum(sqr_dev) / (size - 1)\n",
    "    return variance**0.5\n",
    "\n",
    "print(my_stddev(tst_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute standard deviations for random data considered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(5,6))\n",
    "\n",
    "# compute ranges\n",
    "sig1 = my_stddev(rand_data1)\n",
    "sig2 = my_stddev(rand_data2)\n",
    "\n",
    "axs[0].hist(rand_data1, bins=300);\n",
    "axs[1].hist(rand_data2, bins=300);\n",
    "\n",
    "axs[0].axvspan(-sig1, sig1, color='red', alpha=0.3, label=f'$\\sigma={sig1:4.2f}$')\n",
    "axs[1].axvspan(-sig2, sig2, color='red', alpha=0.3, label=f'$\\sigma={sig2:4.2f}$')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim([-25, 25]);\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the standard deviation more properly describes the data extent.\n",
    "\n",
    "Now we compute the standard deviations for grades and student activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_grd, bins=max(data_grd) - min(data_grd) + 1);\n",
    "ax.set_xlabel(\"Total grade\")\n",
    "ax.set_ylabel(\"# of applicants\")\n",
    "\n",
    "data_mean = my_mean(data_grd)\n",
    "data_std = my_stddev(data_grd)\n",
    "\n",
    "ax.axvline(data_mean, color='red', linewidth=3, label=f'mean at {int(data_mean)}')\n",
    "ax.axvspan(-data_std + data_mean, data_std + data_mean, color='yellow', alpha=0.3, label=f'$\\sigma={data_std:4.2f}$')\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_rh, bins=max(data_rh) - min(data_rh) + 1);\n",
    "ax.set_xlabel(\"# of raised hand\")\n",
    "ax.set_ylabel(\"# of students\")\n",
    "\n",
    "data_mean = my_mean(data_rh)\n",
    "data_std = my_stddev(data_rh)\n",
    "\n",
    "ax.axvline(data_mean, color='red', linewidth=3, label=f'mean at {int(data_mean)}')\n",
    "ax.axvspan(-data_std + data_mean, data_std + data_mean, color='yellow', alpha=0.3, label=f'$\\sigma={data_std:4.2f}$')\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that if the histogram of a data looks like a hump (or bell) the standard deviation properly describes data variability. \n",
    "\n",
    "But for a complicated histograms the range is better, while the standard deviation looks irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interquartile range\n",
    "\n",
    "One more variability characteristic is the interquartile range. This is the range between 75% an 25% quantiles, or Q3 - Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_interquart(data):\n",
    "    return my_quantile(data, 0.75) - my_quantile(data, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interquartile range for grades and student activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_grd, bins=max(data_grd) - min(data_grd) + 1);\n",
    "ax.set_xlabel(\"Total grade\")\n",
    "ax.set_ylabel(\"# of applicants\")\n",
    "\n",
    "data_mean = my_mean(data_grd)\n",
    "data_std = my_stddev(data_grd)\n",
    "ax.axvspan(-data_std + data_mean, data_std + data_mean, color='yellow', alpha=0.3, label=f'$2\\sigma={2*data_std:4.2f}$')\n",
    "\n",
    "Q1 = my_quantile(data_grd, 0.25)\n",
    "Q3 = my_quantile(data_grd, 0.75)\n",
    "ax.axvspan(Q1, Q3, color='gray', alpha=0.3, label=f'iqr={Q3-Q1:4.2f}')\n",
    "\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_rh, bins=max(data_rh) - min(data_rh) + 1);\n",
    "ax.set_xlabel(\"Total grade\")\n",
    "ax.set_ylabel(\"# of applicants\")\n",
    "\n",
    "data_mean = my_mean(data_rh)\n",
    "data_std = my_stddev(data_rh)\n",
    "ax.axvspan(-data_std + data_mean, data_std + data_mean, color='yellow', alpha=0.3, label=f'$2\\sigma={2*data_std:4.2f}$')\n",
    "\n",
    "Q1 = my_quantile(data_rh, 0.25)\n",
    "Q3 = my_quantile(data_rh, 0.75)\n",
    "ax.axvspan(Q1, Q3, color='gray', alpha=0.3, label=f'iqr={Q3-Q1:4.2f}')\n",
    "    \n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hump-like histogram of grades the interquartile range is as good as the standard deviation, at least visually. But for the complicated histogram of the activities the interquartile range provides intuitively more appropriate estimate. In particular, because it is located between Q1 and Q3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "Given two sequences of data the question is do they depend on each other somehow or not. \n",
    "\n",
    "Assume that we register daily an air temperature and an air pressure. When our records becomes sufficiently long we can try to reveal the mutual dependency of these two features. \n",
    "\n",
    "It can be done via Pearson correlation coefficient.\n",
    "\n",
    "First we compute covariance.\n",
    "- we find the means of each data sequences\n",
    "- we find the deviations from the mean for both of them\n",
    "- we find a dot product: multiply component by component and them sum\n",
    "- we divide the sum by $N-1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just two sets of random numbers\n",
    "xs = [-7.6, -9.5, -5.3, 2.0, 8.6, -0.5, -3.2, -7.4, 0.2, -0.7]\n",
    "ys = [-92, -112, -69, 20, 89, 11, -43, -88, 2, -18]\n",
    "\n",
    "# find the mean values\n",
    "x_mean = sum(xs) / len(xs)\n",
    "y_mean = sum(ys) / len(ys)\n",
    "print(x_mean, y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the deviations\n",
    "xd = [x - x_mean for x in xs]\n",
    "yd = [y - y_mean for y in ys]\n",
    "print(xd)\n",
    "print(yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find dot product\n",
    "dot = [x * y for x, y in zip(xd, yd)]\n",
    "print(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the covariance\n",
    "cov = sum(dot) / (len(dot) - 1)\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute the dot product of `xd` and `yd` and corresponding elements of `x` and `y` are either both above their means\n",
    "or both below their means, a positive number enters the sum. \n",
    "\n",
    "When one is above its mean and the other below, a negative number enters the sum. \n",
    "\n",
    "Accordingly, a large positive covariance means that `x` tends to be large when `y` is large and small when `y` is small. \n",
    "\n",
    "A large negative covariance means the opposite that `x` tends to be small when `y` is large and vice versa.\n",
    "\n",
    "A covariance close to zero means that no such relationship exists. \n",
    "\n",
    "But the covariance has arbitrary scale that depends on scales of `xs` and `ys`. It is unclear what means large or small covariance. \n",
    "\n",
    "But if we divide it by the standard deviations of the input datasets it will always fit the range \\[-1,1\\]. This is called Pearson correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = cov / (my_stddev(xs) * my_stddev(ys))\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now wrap all of this into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_corrcoef(xs, ys):\n",
    "    \"\"\"\n",
    "    Pearson correlaton coefficient\n",
    "    \"\"\"\n",
    "    x_mean = my_mean(xs)\n",
    "    y_mean = my_mean(ys)\n",
    "    x_std = my_stddev(xs)\n",
    "    y_std = my_stddev(ys)\n",
    "    xd = [x - x_mean for x in xs]\n",
    "    yd = [y - y_mean for y in ys]\n",
    "    dot = [x * y for x, y in zip(xd, yd)]\n",
    "    cov = sum(dot) / (len(dot) - 1)\n",
    "    return cov / (x_std * y_std)\n",
    "\n",
    "print(my_corrcoef(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that `xs` and `ys` are highly correlated since the correlation coefficient is close to its highest value 1.\n",
    "\n",
    "It means that when `x` is large `y` is also large and vice versa. \n",
    "\n",
    "Lets check what if we change the sign of `ys`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_corrcoef(xs, [-y for y in ys]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that now these to sequences are anti-correlated: when `x` is large positive `y` is large negative and vice versa.\n",
    "\n",
    "Consider two independent random sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "rx = rng.random(size=100)\n",
    "ry = rng.random(size=100)\n",
    "\n",
    "print(my_corrcoef(rx, ry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their correlation coefficient is small with respect to 1. Thus we conclude that the two sequences are uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations examples\n",
    "\n",
    "Consider correlations of grades in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module allows to work with web pages\n",
    "import requests\n",
    "\n",
    "# This is an URL of a repository\n",
    "base_url = \"https://raw.githubusercontent.com/kupav/data-sc-intro/main/data/\"\n",
    "\n",
    "# We need this file\n",
    "file_name = \"unif_state_exam.csv\"\n",
    "\n",
    "# Here we downlaod the file\n",
    "web_data = requests.get(base_url + file_name)\n",
    "assert web_data.status_code == 200\n",
    "\n",
    "# Take a look at the data\n",
    "print(web_data.text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by line ends\n",
    "str_data = web_data.text.splitlines()\n",
    "print(str_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop out the header and split grades\n",
    "lst_data = [s.split(',') for s in str_data[1:]]\n",
    "print(lst_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Phys, Math, Lang\n",
    "data_pml = [[int(s[2]), int(s[3]), int(s[4])] for s in lst_data]\n",
    "print(data_pml[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations\n",
    "cor = my_corrcoef([x[0] for x in data_pml], [x[1] for x in data_pml])\n",
    "print(f\"Phys-Math grades correlation {cor:5.2f}\")\n",
    "\n",
    "cor = my_corrcoef([x[0] for x in data_pml], [x[2] for x in data_pml])\n",
    "print(f\"Phys-Lang grades correlation {cor:5.2f}\")\n",
    "\n",
    "cor = my_corrcoef([x[1] for x in data_pml], [x[2] for x in data_pml])\n",
    "print(f\"Lang-Math grades correlation {cor:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the correlations are small. But nevertheless the highest correlation is between Physics and Mathematics grades. The smallest correlation is between Physics and Language.\n",
    "\n",
    "Now check student activities. Recall that it contains three columns: how many times he/she raised a hand (column `RH`), visited course resource (`Res`) and participated on discussion groups (`Disc`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module allows to work with web pages\n",
    "import requests\n",
    "\n",
    "# This is an URL of a repository\n",
    "base_url = \"https://raw.githubusercontent.com/kupav/data-sc-intro/main/data/\"\n",
    "\n",
    "# We need this file\n",
    "file_name = \"stud_activ.csv\"\n",
    "\n",
    "# Here we downlaod the file\n",
    "web_data = requests.get(base_url + file_name)\n",
    "assert web_data.status_code == 200\n",
    "\n",
    "# Take a look at the data\n",
    "print(web_data.text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by line ends\n",
    "str_data = web_data.text.splitlines()\n",
    "print(str_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop out the header and split\n",
    "lst_data = [s.split(',') for s in str_data[1:]]\n",
    "print(lst_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ints\n",
    "data_rrd = [[int(s[0]), int(s[1]), int(s[2])] for s in lst_data]\n",
    "print(data_rrd[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations\n",
    "cor = my_corrcoef([x[0] for x in data_rrd], [x[1] for x in data_rrd])\n",
    "print(f\"Raised hand vs Visited res {cor:5.2f}\")\n",
    "\n",
    "cor = my_corrcoef([x[0] for x in data_rrd], [x[2] for x in data_rrd])\n",
    "print(f\"Raised hand vs Discussions {cor:5.2f}\")\n",
    "\n",
    "cor = my_corrcoef([x[1] for x in data_rrd], [x[2] for x in data_rrd])\n",
    "print(f\"Visited res vs Discussions {cor:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that class activity (raised hands count) highly correlate with the number of the course resource visits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation caveats\n",
    "\n",
    "Consider two sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [-3, -2, -1, 0, 1, 2, 3]\n",
    "ys = [ 3,  2,  1, 0, 1, 2, 3]\n",
    "print(my_corrcoef(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient is zero, but we definitely observe a relationship between `xs` and `ys`: absolute values of their corresponding elements coincide. \n",
    "\n",
    "The reason why the correlation coefficient vanishes is the following: if one knows how $x_i$ deviates from the mean $\\overline x$ one cannot say how the corresponding $y_i$ deviates from $\\overline y$. That is the sort of relationship that correlation reveals.\n",
    "\n",
    "Another ambiguous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [-3, -2, -1, 0, 1, 2, 3]\n",
    "ys = [100.01, 100.02, 100.03, 100.04, 100.05, 100.06, 100.07]\n",
    "print(my_corrcoef(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation is the highest so that technically the data are perfectly correlated. But since the data are so different their actual relationship is questionable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and causation\n",
    "\n",
    "Correlation is not causation. \n",
    "\n",
    "High correlation indicates that two data sequences vary similarly. But it says nothing why they vary similarly. If $x$ and $y$ are strongly correlated, that might mean that $x$ causes $y$, that $y$ causes $x$, that each causes the other, that some third factor\n",
    "causes both, or nothing at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy and SciPy functions\n",
    "\n",
    "Most of the statistical functions discussed above are available in NumPy except the mode. The mode is computed using another module `SciPy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "rx = rng.integers(10, size=1000)\n",
    "ry = rng.integers(100, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "data_mean = np.mean(rx)\n",
    "print(data_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median\n",
    "data_median = np.median(rx)\n",
    "print(data_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode\n",
    "from scipy import stats\n",
    "data_mode = stats.mode(rx)\n",
    "print(data_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quartiles\n",
    "Q1, Q2, Q3 = np.percentile(rx, [25, 50, 75])\n",
    "print(Q1, Q2, Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation\n",
    "std = np.std(rx)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation coefficient - returns a matrix with pairwise correlation coefficients\n",
    "cor = np.corrcoef(rx, ry)\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. File `\"bank_customers.csv\"` that you can find at `\"https://raw.githubusercontent.com/kupav/data-sc-intro/main/data/\"` describes bank clinets. It contains three columns: customer age (`CustAge`), period of relationship with the bank in months (`Months`), and credit limit (`CredLim`). Plot a histogram for the column `CustAge` and compute its standard deviation and the interquartile range. Show them on the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Download the dataset from the previous exercise and compute correlation coefficients for each pair of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "ru",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "113px",
    "width": "228px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217.567px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
