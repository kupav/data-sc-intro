{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Probability\n",
    "\n",
    "Part of [\"Introduction to Data Science\" course](https://github.com/kupav/data-sc-intro) by Pavel Kuptsov, [kupav@mail.ru](mailto:kupav@mail.ru)\n",
    "\n",
    "Recommended reading for this section:\n",
    "\n",
    "1. Grus, J. (2019). Data Science From Scratch: First Principles with Python (Vol. Second edition). Sebastopol, CA: Oâ€™Reilly Media\n",
    "\n",
    "The following Python modules will be required. Make sure that you have them installed.\n",
    "- `matplotlib`\n",
    "- `requests`\n",
    "- `numpy`\n",
    "- `scipy`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty\n",
    "\n",
    "Determinism: if we know exact initial conditions of a system we can predict its future states. If there is a small error in the initial condition the prediction error is also small.\n",
    "\n",
    "Consider a spaceship launch. Given its initial coordinates, its direction and its acceleration engineers are able to predict its trajectory. Since the initial conditions are known with errors and this is unavoidable an actual trajectory will deviate from the predicted one. But a small error in initial conditions result in a small deviation.\n",
    "\n",
    "Uncertainty: future states of the system can not be predicted with a reasonable precision, no matter how exactly we know the initial condition. \n",
    "\n",
    "Weather is an example of uncertainty. No matter how hard we try to measure the initial conditions: air temperature, atmospheric pressure, wind speed and so on. Actual weather conditions will rapidly deviate form the prediction anyway. \n",
    "\n",
    "Uncertainty also appears when people's decisions are somehow involved. \n",
    "\n",
    "Uncertainty often manifests itself as stochasticity: behavior of an observed system looks random, not obeyed to some laws.\n",
    "\n",
    "Possible sources of uncertainty\n",
    "\n",
    "- **Inherent uncertainty.** Quantum mechanics describes the dynamics of subatomic particles as being probabilistic. Weather conditions and people's decisions are also examples of inherent stochasticity.\n",
    "\n",
    "- **Incomplete observability.** Even deterministic systems can appear stochastic when one cannot observe all of the variables that drive their behavior. When one throws a stone its trajectory endpoint is predictable only if an initial velocity and its angle is registered and a wing is taken into account. It looks stochastic if one can not observe the whole set of the initial conditions.\n",
    "\n",
    "- **Incomplete modeling.** When the deterministic description requires a huge amount of information one can purposefully discard some information. Although dicing playing cards and tossing a coin are deterministic processes and in principle can be predicted, one usually consider these processes as being stochastic because the prediction is extremely complicated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability. Basic properties\n",
    "\n",
    "A deterministic experiment: no matter how many times we repeat it the result is always the same provided that the initial conditions are kept unchanged. \n",
    "\n",
    "A stochastic experiment: even if the initial conditions are unchanged the repeated experiments produce different results. Closer look at the results revels that they belongs to a certain set of possibilities. And it is this set of possibilities that remains unchanged.\n",
    "\n",
    "For example when we roll a die we know in advance that there are 6 possible outcomes: 1, 2, 3, 4, 5, and 6. But the particular result is random and can not be predicted.\n",
    "\n",
    "Probability is introduced to quantify the uncertainty. \n",
    "\n",
    "A probability of an event $E$ is usually denoted as $P(E)$.\n",
    "\n",
    "A probability is always a number between 0 and 1. \n",
    "\n",
    "Probability 0 means that an event will never occur, and 1 means that the event will surely occur.\n",
    "\n",
    "If $E$ denotes a certain event, $\\bar E$ denotes all others events except this one. The probability of an event not occurring, called the complement: $P(\\bar E)=1-P(E)$\n",
    "\n",
    "Sum of probabilities of all possibilities must be equal to 1. It means that we consider a full set of possibilities: one of them definitely occurs.\n",
    "\n",
    "Sometimes initial probabilities and the results are represented as percentages: a number in the range from 0 to 100%. Obviously this is done by multiplication of the probability by 100%. \n",
    "\n",
    "But notice that all computations are performed for probabilities defined as fractions of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postulated probability\n",
    "\n",
    "There are two ways to define probabilities.\n",
    "\n",
    "We can assign probabilities to each experiment outcome according to our intuition on the basis of reasonable expectations. \n",
    "\n",
    "The simplest and the most reliable case is when we are **equally uncertain** of all possible outcomes. \n",
    "\n",
    "This is the case for example for a fair die: there are no reasons to think that some numbers on its edges \n",
    "are more preferable then others. ('Fair' here means that it is full symmetric, neither side is any different from the other.)\n",
    "\n",
    "![die-1.svg](fig/die-1.svg)\n",
    "\n",
    "The same situation is with a fair coin: landing it with a head or a tail on the top are equally uncertain. \n",
    "\n",
    "Let the number of all possible outcomes is $N$ ($N=6$ for a die and $N=2$ for a coin). Since all of them are equally uncertain we assign to each event a probability \n",
    "\n",
    "$$\n",
    "P=1/N\n",
    "$$\n",
    "\n",
    "In particular for a fair die $P=1/6$, for a fair coin $P=1/2$.\n",
    "\n",
    "Such equally uncertain outcomes are called elementary events.\n",
    "\n",
    "When elementary events have been revealed we can consider more complicated situations that include their combinations.\n",
    "\n",
    "For example: what is the probability of a fair die landing with an even number on top? \n",
    "\n",
    "The probability in this case is computed as a number $n$ of \n",
    "the elementary events that fulfill the condition divided by the total number $N$ of the elementary events.\n",
    "\n",
    "$$\n",
    "P=n/N\n",
    "$$\n",
    "\n",
    "In our example there are 3 even numbers on edges: 2, 4, and 6, and totally there are 6 elementary events. Thus\n",
    "\n",
    "$$\n",
    "P=3/6=1/2\n",
    "$$\n",
    "\n",
    "This way to introduce the probabilities is also called **classical probability**.\n",
    "\n",
    "If equally uncertain elementary events cannot be reveled the probabilities can be assigned according to an expert opinion.\n",
    "\n",
    "For example a doctor believes with 90% certainty that the diagnosis is correct. Thus we assign to this diagnosis a probability\n",
    "\n",
    "$$\n",
    "P=0.9\n",
    "$$\n",
    "\n",
    "Another example comes from gambling. Probability of results of a competition can be assigned proportional to bets.\n",
    "\n",
    "The probability defined in this way is also called **subjective or Bayesian probability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist probability\n",
    "\n",
    "This is an experimental probability: we repeat a stochastic experiment as many times as possible and count occurrences of each outcome. \n",
    "\n",
    "Let the $i$th outcome occurs $m_i$ times in a series of $M$ experiments. Then the frequency of this outcome is\n",
    "\n",
    "$$\n",
    "\\nu_i = m_i / M\n",
    "$$\n",
    "\n",
    "The problem with the frequency is that it is not stable: if we repeat this series of $M$ experiments the frequency will be different.\n",
    "\n",
    "Let us simulate this experiments numerically. We are going to model a die.\n",
    "\n",
    "First define a function that counts frequencies. Notice that for practical applications this is not needed since class `Counter` from module `collections` does the same job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_prob(seq):\n",
    "    \"\"\"\n",
    "    Assuming that seq is an array of integers, compute frequencies of each number\n",
    "    \"\"\"\n",
    "    prob = {}  # initialize an empty dictionary\n",
    "    for n in seq:\n",
    "        try:\n",
    "            prob[int(n)] += 1  # try to increase the counter\n",
    "        except KeyError:\n",
    "            prob[int(n)] = 1   # initialize the counter if this is the first appearance of n\n",
    "           \n",
    "    size = len(seq)    \n",
    "    for n in prob:  # iterate over dict keys\n",
    "        prob[n] /= size  # compute frequencies\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the function above\n",
    "ser = [1,1,1,2,2]\n",
    "prob = freq_prob(ser)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic definition of probability assign each number on a die the probability \n",
    "\n",
    "$$\n",
    "p=1/6=0.1666\\ldots\n",
    "$$\n",
    "\n",
    "This value is based on our intuition since each edge of a fair die is expected to appear with equal probabilities. Let us compare it with frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 6  # number of die edges and also a range of emerging numbers\n",
    "M = 10000  # length of the experiment series\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# the first series of experiments\n",
    "ser1 = rng.integers(1, N, endpoint=True, size=M)\n",
    "# the second series of experiments\n",
    "ser2 = rng.integers(1, N, endpoint=True, size=M)\n",
    "# the third series of experiments\n",
    "ser3 = rng.integers(1, N, endpoint=True, size=M)\n",
    "\n",
    "prob1 = freq_prob(ser1)\n",
    "prob2 = freq_prob(ser2)\n",
    "prob3 = freq_prob(ser3)\n",
    "print(prob1)\n",
    "print(prob2)\n",
    "print(prob3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the values are close to each other and to $1/6$. But they vary from experiment to experiment. Let us repeat everything with longer series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 6 \n",
    "M = 1000000  # increase it by 100\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "ser1 = rng.integers(1, N, endpoint=True, size=M)\n",
    "ser2 = rng.integers(1, N, endpoint=True, size=M)\n",
    "ser3 = rng.integers(1, N, endpoint=True, size=M)\n",
    "\n",
    "prob1 = freq_prob(ser1)\n",
    "prob2 = freq_prob(ser2)\n",
    "prob3 = freq_prob(ser3)\n",
    "print(prob1)\n",
    "print(prob2)\n",
    "print(prob3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variability is lower but still visible.\n",
    "\n",
    "This is typical situation: the variability of relative frequencies become small only for really huge series. \n",
    "\n",
    "Thus the frequentist probability is defined as a limit of relative frequencies when the length of the series becomes infinite.\n",
    "\n",
    "$$\n",
    "P_i = \\lim_{M\\to\\infty} \\frac{m_i}{M}\n",
    "$$\n",
    "\n",
    "In practice it is usually unclear how to find this limit using strict mathematics. In this case we can guess where the relative frequencies tends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 6 \n",
    "M0 = 100\n",
    "dM = 100\n",
    "M1 = dM * M0\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Three series of experiments\n",
    "ser1 = rng.integers(1, N, endpoint=True, size=M1)\n",
    "ser2 = rng.integers(1, N, endpoint=True, size=M1)\n",
    "ser3 = rng.integers(1, N, endpoint=True, size=M1)\n",
    "\n",
    "# Collect frequencies computed for different series lengths\n",
    "data1, data2, data3 = [], [], []\n",
    "for Mx in range(M0, M1+1, dM):\n",
    "    prob1 = freq_prob(ser1[:Mx])\n",
    "    prob2 = freq_prob(ser2[:Mx])\n",
    "    prob3 = freq_prob(ser3[:Mx])\n",
    "    data1.append([Mx, prob1[1]])\n",
    "    data2.append([Mx, prob2[1]])\n",
    "    data3.append([Mx, prob3[1]])\n",
    "    \n",
    "# Convert lists to numpy arrays for convenience\n",
    "data1 = np.array(data1)\n",
    "data2 = np.array(data2)\n",
    "data3 = np.array(data3)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(data1[:, 0], data1[:, 1], label='ser1')\n",
    "ax.plot(data2[:, 0], data2[:, 1], label='ser2')\n",
    "ax.plot(data3[:, 0], data3[:, 1], label='ser3')\n",
    "ax.axhline(1/6, color='k', linestyle='--', label='prob')\n",
    "ax.set_xlabel(r'$M$')\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots demonstrate that the frequencies approach classic probability $1/6$ very slowly. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent random events and joint probability\n",
    "\n",
    "Usually compound events are considered that include two or more simple events. \n",
    "\n",
    "These simple events can be independent or can depend on each other.\n",
    "\n",
    "Probabilities of particular events are called marginal probabilities.\n",
    "\n",
    "Consider two dice. We roll them and expect that both land with 1 on top. What is the probability? \n",
    "\n",
    "In this situation we have two events, $A$ and $B$. Probability that they occur simultaneously is called *joint probability* and is denoted as $P(A,B)$.\n",
    "\n",
    "The joint probability is different for independent and dependent events.\n",
    "\n",
    "Independent events $A$ and $B$: probability $P(B)$ of the event $B$ is not changed whether or not the event $A$ occurs.\n",
    "\n",
    "Landing of dice with 1 on top are independent events. Both have probabilities $P(A)=P(B)=1/6$.\n",
    "\n",
    "![die-2.svg](fig/die-2.svg)\n",
    "\n",
    "The joint probability of two independent events is a product of their probabilities:\n",
    "\n",
    "$$\n",
    "P(A,B) = P(A) P(B)\n",
    "$$\n",
    "\n",
    "For two dice the joint probability for 1 to show up is\n",
    "\n",
    "$$\n",
    "P(A, B) = \\frac{1}{6}\\times\\frac{1}{6}=\\frac{1}{36}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent random events and conditional probability\n",
    "\n",
    "Two events $A$ and $B$ are dependent if knowing something about whether $A$ occurs gives us information about whether $B$ occurs and vice versa.\n",
    "\n",
    "In the other words, if the events $A$ and $B$ are dependent the probability $P(B)$ varies depending on whether the event $A$ occurs or not.\n",
    "\n",
    "Consider a box with 4 red 8 blue balls. Totally there are 12 balls.\n",
    "\n",
    "![prob_box1.svg](fig/prob_box1.svg)\n",
    "\n",
    "The event $A$: we take blindly a red ball and do not return it back to the box. It means we *want to get a red ball*. But we do it blindly and the result can be $A$ (the ball was red) or $\\bar A$ (that was blue).\n",
    "\n",
    "The probability of $A$ is $P(A)=4/12=1/3$ (according to classical definition of probability).\n",
    "\n",
    "The event $B$: when the first ball is taken we want to take blindly another red ball. The probability of $B$ depends on the result of the first experiment: was the first ball red (event $A$ has occurred) or it was blue (event $\\bar A$ has occurred)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the event A has occurred, i.e., we indeed took a red ball.\n",
    "\n",
    "![prob_box2.svg](fig/prob_box2.svg)\n",
    "\n",
    "There are 11 balls left in the box: 3 red and 8 blue ones. \n",
    "\n",
    "Thus the probability of $B$ provided that $A$ has occurred is\n",
    "\n",
    "$$\n",
    "P(B|A) = 3/11\n",
    "$$\n",
    "\n",
    "$P(B|A)$ is called *conditional probability*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "Now assume that the event $\\bar A$ has occurred: that first ball was not red.\n",
    "\n",
    "![prob_box3.svg](fig/prob_box3.svg)\n",
    "\n",
    "Again there are 11 balls in the box, but now 4 red and 7 blue ones.\n",
    "\n",
    "The probability of $B$ provided that $\\bar A$ has occurred is\n",
    "\n",
    "$$\n",
    "P(B|\\bar A) = 4/11\n",
    "$$\n",
    "\n",
    "The joint probability of two dependent events is a product of the probability of the first event $P(A)$ and conditional probability of $B$ provided that $A$ has occurred $P(B|A)$:\n",
    "\n",
    "$$\n",
    "P(A, B) = P(B|A) P(A)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound events\n",
    "\n",
    "Assume that we are given two closed boxes. Each one can be either empty or contain a gift. The boxes and the gifts are identical. \n",
    "\n",
    "Let $A$ be an event that the box 1 contains a gift, and $B$ means that the box 2 has a gift. \n",
    "\n",
    "The presence or absence of the gift in one box is independent on the content of the other box, i.e., the events $A$ and $B$ are independent.\n",
    "\n",
    "![prob_gift1.svg](fig/prob_gift1.svg)\n",
    "\n",
    "Since the content of the boxes is uncertain as much as possible we assign both probabilities as follows:\n",
    "\n",
    "$$\n",
    "P(A)=P(B)=1/2\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we open two boxes, the following events can occur:\n",
    "- No gifts: $\\bar A$ and $\\bar B$\n",
    "- Two gifts: $A$ and $B$\n",
    "- A gift in the box 1: $A$ and $\\bar B$\n",
    "- A gift in the box 2: $\\bar A$ and $B$\n",
    "\n",
    "![prob_gift2.svg](fig/prob_gift2.svg)\n",
    "\n",
    "All these events have equal probabilities $1/4$:\n",
    "\n",
    "$$\n",
    "P(\\bar A, \\bar B) = P(A, B) = P(A, \\bar B) = P(\\bar A, B) = 1/4\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in the number of the gifts, no matter where they were found, the events and the probabilities are as follows.\n",
    "\n",
    "- No gifts: $G_0=\\bar A\\text{ and }\\bar B$\n",
    "- Two gifts: $G_2=A\\text{ and }B$\n",
    "- One gift: $G_1=(A\\text{ and }\\bar B)\\text{ or }(\\bar A\\text{ and }B)$\n",
    "\n",
    "$$\n",
    "P(G_0)=1/4, \n",
    "$$\n",
    "\n",
    "$$\n",
    "P(G_2)=1/4, \n",
    "$$\n",
    "\n",
    "$$\n",
    "P(G_1)=2/4=1/2\n",
    "$$\n",
    "\n",
    "The probability to have *at least one* gift $G_{>0}$ can found like this:\n",
    "\n",
    "$$\n",
    "P(G_{>0})=P(\\bar G_0)=1-1/4=3/4 \n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconsider probabilities using new information\n",
    "\n",
    "How the probability of $G_2$ changes if we obtain new information?\n",
    "\n",
    "What is the probability to have two gifts if the box 2 is already open and contains the gift? \n",
    "\n",
    "This is a conditional probability: we want to know the probability of $G_2$ provided that $B$ has occurred: $P(G_2|B)$.\n",
    "\n",
    "Above we have found unconditional probability of $G_2$: there are four possibilities and only one corresponds to $G_2$. Hence $P(G_2)=1/4$. \n",
    "\n",
    "But if $B$ has occurred only two possibilities remain, see the right column of the figure. Two cases shown there represent gifts in the box 2.\n",
    "\n",
    "![prob_gift3.svg](fig/prob_gift3.svg)\n",
    "\n",
    "Among them the top right one corresponds to the event $G_2$: both boxes contain the gifts. Thus one case of two fulfills the condition and hence the sought probability reads\n",
    "\n",
    "$$\n",
    "P(G_2|B) = 1/2\n",
    "$$\n",
    "\n",
    "Bearing in mind more complicated problems we consider now a  formal solution of the problem.\n",
    "\n",
    "Let us first remember: if the event $G_2$ depends on $B$ so that $P(G_2|B)$ is the conditional probability of $G_2$ provided that $B$ has occurred, then the joint probability $P(G_2,B)$ is computed as follows:\n",
    "\n",
    "$$\n",
    "P(G_2, B) = P(G_2|B) P(B)\n",
    "$$\n",
    "\n",
    "where $P(B)$ is the unconditional probability of $B$. \n",
    "\n",
    "Thus we can find the conditional probability $P(G_2|B)$ as\n",
    "\n",
    "$$\n",
    "P(G_2|B) = P(G_2,B) / P(B)\n",
    "$$\n",
    "\n",
    "The joint probability $P(G_2, B)$ means: both boxes contain the gifts ($G_2$) and the box 2 contains the gift ($B$). \n",
    "\n",
    "It is clear that the first event implies the second one. Thus\n",
    "\n",
    "$$\n",
    "P(G_2, B) = P(G_2) = 1/4\n",
    "$$\n",
    "\n",
    "Also we know that\n",
    "\n",
    "$$\n",
    "P(B)=1/2\n",
    "$$\n",
    "\n",
    "Finally we have\n",
    "\n",
    "$$\n",
    "P(G_2|B)=\\frac{1/4}{1/2}=\\frac{2}{4}=\\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider another situation. \n",
    "\n",
    "What is the probability to have two gifts if at least one of the boxes contains a gift?\n",
    "\n",
    "![prob_gift4.svg](fig/prob_gift4.svg)\n",
    "\n",
    "In the other words new information allows to exclude the case with two empty boxes.\n",
    "\n",
    "Inspecting the figure we see that there are now three possibilities and only one of them fulfills the conditions. Thus the answer is\n",
    "\n",
    "$$\n",
    "P(G_2|G_{>0}) = 1/3\n",
    "$$\n",
    "\n",
    "Now let us find the answer via a formal approach. \n",
    "\n",
    "$$\n",
    "P(G_2|G_{>0}) = P(G_2, G_{>0}) / P(G_{>0})\n",
    "$$\n",
    "\n",
    "The joint probability $P(G_2, G_{>0})$ means: two boxes contain the gifts ($G_2$) and at least on of the boxes contain the gift ($G_{>0}$). The first event implies the second one, so\n",
    "\n",
    "$$\n",
    "P(G_2, G_{>0}) = P(G_2) = 1/4\n",
    "$$\n",
    "\n",
    "Above we already have found that $P(G_{>0}) = 3/4$. \n",
    "\n",
    "Thus we have again the answer:\n",
    "\n",
    "$$\n",
    "P(G_2|G_{>0}) = \\frac{1/4}{3/4} = \\frac{4}{4\\cdot 3} = \\frac{1}{3}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Law of total probability\n",
    "\n",
    "Sometimes we know conditional probabilities and want to find marginal probability. \n",
    "\n",
    "This is computed via law of total probability.\n",
    "\n",
    "Assume we have envelopes of two colors: red and blue. \n",
    "\n",
    "If we take a red envelope this is an event $A$ and blue envelope in hands is an event $B$.\n",
    "\n",
    "Envelopes can contain gifts and the gifts are placed in a random way. The probability depends on the color. \n",
    "\n",
    "Red envelopes are filled with the probability $P(G|A)$, blue ones hold it with the probability $P(G|B)$.\n",
    "\n",
    "For concreteness, let\n",
    "\n",
    "$$\n",
    "P(G|A) = 0.2, P(G|B)=0.6\n",
    "$$\n",
    "\n",
    "![prob_totprob1.svg](fig/prob_totprob1.svg)\n",
    "\n",
    "Notice that the probabilities are conditional because they depend on the color of an envelope."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take a certain number of blue and red envelopes and put them into a box.\n",
    "\n",
    "![prob_totprob2.svg](fig/prob_totprob2.svg)\n",
    "\n",
    "The question is: What is the marginal probability $P(G)$ to have a gift if a pulling up of an envelope occurs blindly, i.e., without knowing a color an advance?\n",
    "\n",
    "The answer is given by a law of total probability.\n",
    "\n",
    "Let the probability to have a red envelope is $P(A)$ and $P(B)$ is the probability to have a blue one.\n",
    "\n",
    "In the figure there are totally 9 envelopes including 4 red ones and 5 blue ones. Thus\n",
    "\n",
    "$$\n",
    "P(A) = 4/9, P(B) = 5/9\n",
    "$$\n",
    "\n",
    "Now we are ready to compute $P(G)$.\n",
    "\n",
    "We will have a gift (event $G$) if we take a red enveloped with the gift (the joint events $G$ and $A$) or a blue envelope with the gift (the joint events $G$ and $B$).\n",
    "\n",
    "The corresponding probabilities are\n",
    "\n",
    "$$\n",
    "P(G) = P(G,A) + P(G,B)\n",
    "$$\n",
    "\n",
    "Since the event $G$ depends on $A$ and $B$ we compute the joint probabilities via conditional probabilities:\n",
    "\n",
    "$$\n",
    "P(G,A)=P(G|A) P(A), \n",
    "P(G,B)=P(G|B) P(B)\n",
    "$$\n",
    "\n",
    "Gathering all together we obtain the *law of total probability*:\n",
    "\n",
    "$$\n",
    "P(G) = P(G|A) P(A) + P(G|B) P(B)\n",
    "$$\n",
    "\n",
    "For our particular example\n",
    "\n",
    "$$\n",
    "P(G) = 0.2 \\cdot 4/9 + 0.6 \\cdot 5/9 \\approx 0.42\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' theorem\n",
    "\n",
    "Bayes' theorem is based on the fact that if a random event $A$ depends on $B$ this dependence is mutual and one can also consider how $B$ depends on $A$.\n",
    "\n",
    "This theorem allows to reverse conditional probabilities: Given $P(A|B)$ we can compute $P(B|A)$.\n",
    "\n",
    "Another description of this theorem: it allows to reconsider probabilities using new information. \n",
    "\n",
    "In fact it was done already above: we computed the probability to have two gifts from two boxes provided that some new information had appeared.\n",
    "\n",
    "First discuss this theorem formally.\n",
    "\n",
    "Consider two dependent events $A$ and $B$. They occur simultaneously with the joint probability $P(A,B)$. It can be found via conditional probabilities as follows:\n",
    "\n",
    "$$\n",
    "P(A, B) = P(B | A) P(A) = P(A | B) P(B)\n",
    "$$\n",
    "\n",
    "Assume we know $P(A | B)$ and want to find $P(B | A)$. It is found via Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(B | A) = \\frac{P(A | B) P(B)}{P(A)}\n",
    "$$\n",
    "\n",
    "If the marginal probability $P(A)$ is unknown in advance, it can be found via the law of total probability.\n",
    "\n",
    "$$\n",
    "P(A) = P(A | B) P(B) + P(A | \\bar B) P(\\bar B)\n",
    "$$\n",
    "\n",
    "Gathering these equations together we obtain a more detailed form of the Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(B | A) = \\frac{P(A | B) P(B)}{P(A | B) P(B) + P(A | \\bar B) P(\\bar B)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an example of using Bayes' theorem.\n",
    "\n",
    "Imagine a certain disease that affects 1 in every 10000 people.\n",
    "\n",
    "![prob_bayes1.svg](fig/prob_bayes1.svg)\n",
    "\n",
    "It means that the probability to encounter an ill person is \n",
    "\n",
    "$$\n",
    "P(D)=1/10000,\n",
    "$$\n",
    "\n",
    "and healthy persons have probability \n",
    "\n",
    "$$\n",
    "P(\\bar D) =  1 - P(D) = 9999 / 10000\n",
    "$$\n",
    "\n",
    "Assume that there is a test for this disease that gives the correct result for a diseased person, i.e., true positive result, with the probability\n",
    "\n",
    "$$\n",
    "P(T | D) = 0.99\n",
    "$$\n",
    "\n",
    "The false negative result is the complement (diseased person is tested as healthy):\n",
    "\n",
    "$$\n",
    "P(\\bar T | D) = 1 - P(T | D) = 0.01\n",
    "$$\n",
    "\n",
    "Also assume that when this test is applied to a healthy person it produces the true negative result with the probability\n",
    "\n",
    "$$\n",
    "P(\\bar T | \\bar D) = 0.98\n",
    "$$\n",
    "\n",
    "Its complement is the false negative result (healthy person is tested as diseased):\n",
    "\n",
    "$$\n",
    "P(T | \\bar D) = 1 - P(\\bar T | \\bar D) = 0.02\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize: probabilities of true positive $P(T | D)$ and true negative $P(\\bar T|\\bar D)$ events are characteristic of the test system obtained after its approbation. \n",
    "\n",
    "![prob_bayes2.svg](fig/prob_bayes2.svg)\n",
    "\n",
    "Two other probabilities for false positive and false negative events are computed as complements according to the general property of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start using this test system and it produces a certain amount of positive and negative results. \n",
    "\n",
    "The first reasonable question is: What is the probability that a person with a positive test (event $T$) has a disease (event $D$)?\n",
    "\n",
    "This is the conditional probability $P(D|T)$ and it can be computed via Bayes' theorem equation.\n",
    "\n",
    "$$\n",
    "P(D|T) = \\frac{P(T|D) P(D)}{P(T|D) P(D) + P(T|\\bar D) P(\\bar D)}\n",
    "$$\n",
    "\n",
    "The second question: Given a negative test (event $\\bar T$) what is the probability the person is healthy (event $\\bar D$)?\n",
    "\n",
    "$$\n",
    "P(\\bar D|\\bar T) = \\frac{P(\\bar T|\\bar D) P(\\bar D)}\n",
    "{P(\\bar T|\\bar D) P(\\bar D) + P(\\bar T| D) P(D)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute these probabilities \n",
    "P_D = 1/10000\n",
    "P_T_D = 0.99\n",
    "P_notT_notD = 0.98\n",
    "\n",
    "P_notD = 1 - P_D\n",
    "P_T_notD = 1 - P_notT_notD\n",
    "P_notT_D = 1-P_T_D\n",
    "\n",
    "P_D_T = P_T_D * P_D / (P_T_D * P_D + P_T_notD * P_notD)\n",
    "P_notD_notT = P_notT_notD * P_notD / (P_notT_notD * P_notD + P_notT_D * P_D)\n",
    "\n",
    "print(f\"Positively tested person is diseased with the probability {P_D_T*100:.2f}% and is healthy with the probability {(1-P_D_T)*100:.2f}%\")\n",
    "print(f\"Negatively tested person is healty with the probability {P_notD_notT*100}% and is diseased with the probability {100-P_notD_notT*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained results are very optimistic for patients and pessimistic for test system developers. \n",
    "\n",
    "If the test is negative the patient is most likely not diseased, and  if the test is positive the patient is also most likely not diseased.\n",
    "\n",
    "Also we observe that a value of test systems is highly overestimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical verification of the Bayes' theorem\n",
    "\n",
    "The results do not agree well with our intuition. Let us check them in a straightforward way.\n",
    "\n",
    "But first we need to discuss some computational ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genertae random floats in the range [0, 1)\n",
    "import numpy as np\n",
    "rng = np.random.default_rng() \n",
    "\n",
    "for _ in range(10):\n",
    "    print(rng.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 0 and 1 with a given probability\n",
    "import numpy as np\n",
    "rng = np.random.default_rng() \n",
    "\n",
    "prob = 0.8\n",
    "\n",
    "seq = [1 if rng.random() < prob else 0 for _ in range(100)]\n",
    "print(seq)\n",
    "\n",
    "# Estimate probability via frequency - must be close to prob\n",
    "print(\"prob=\", sum(seq) / len(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "class Test:\n",
    "    \"\"\"\n",
    "    This is the test system\n",
    "    \"\"\"\n",
    "    def __init__(self, rng, P_T_D, P_notT_notD):\n",
    "        self.P_T_D = P_T_D\n",
    "        self.P_notT_notD = P_notT_notD\n",
    "        self.rng = rng\n",
    "\n",
    "    def __call__(self, person):\n",
    "        # Generate result depending on the person status\n",
    "        assert person == 0 or person == 1\n",
    "        r = rng.random()\n",
    "        if person == 1:\n",
    "            # for diseased person returns 1 with probability P_T_D\n",
    "            return 1 if r < self.P_T_D else 0\n",
    "        else:\n",
    "            # for healthy person returns 0 with probability P_notT_notD\n",
    "            return 0 if r < self.P_notT_notD else 1\n",
    "\n",
    "P_D = 1/10000        \n",
    "P_T_D = 0.99\n",
    "P_notT_notD = 0.98\n",
    "size = 1000000\n",
    "\n",
    "# This is our population with healthy (0) and diseased (1) persons\n",
    "pop = [1 if rng.random() < P_D else 0 for _ in range(size)]\n",
    "\n",
    "# Create test system\n",
    "test = Test(rng, P_T_D, P_notT_notD)\n",
    "\n",
    "# Results of tests\n",
    "results = [test(p) for p in pop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run along population and select persons with positive tests\n",
    "pos_results = [p for r, p in zip(results, pop) if r == 1]\n",
    "\n",
    "# Estimate P_D_T - probability that persons with positive test is diseased\n",
    "est_P_D_T = sum(pos_results) / len(pos_results)\n",
    "print(f\"Estimated P_D_T={est_P_D_T*100:.2f}%, True P_D_T={P_D_T*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run along population and select persons with negative tests\n",
    "neg_results = [p for r, p in zip(results, pop) if r == 0]\n",
    "\n",
    "# Estimate P_notD_notT - probability that persons with negative test is healthy\n",
    "est_P_notD_notT = (len(neg_results) - sum(neg_results)) / len(neg_results)\n",
    "print(f\"Estimated P_notD_notT={est_P_notD_notT*100}%, True P_notD_notT={P_notD_notT*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that our computed results are very close to theoretically predicted via Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential function\n",
    "\n",
    "Before going further, we need to meet two useful functions. \n",
    "\n",
    "An exponential function reads\n",
    "\n",
    "$$\n",
    "y = b^x\n",
    "$$\n",
    "\n",
    "where b is a positive real number not equal to 1.\n",
    "\n",
    "Most often the constant called Euler number $e=2.71828\\ldots$ is used as the base for the exponential function. \n",
    "\n",
    "If not specified the exponential function with the base $e$ is assumed.\n",
    "\n",
    "$$\n",
    "y=e^x=\\mathrm{exp}(x)\n",
    "$$\n",
    "\n",
    "Below is the graph of the exponential function.\n",
    "\n",
    "Observe that the exponential function grows very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-2, 5, 100)\n",
    "y = np.exp(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$e^x$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of the exponential function:\n",
    "\n",
    "$$\n",
    "e^x > 0 \\text{ for any $x$}\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^0=1\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^x<1 \\text{ for x < 0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^x>1 \\text{ for x > 0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{a+b}=e^a\\cdot e^b\n",
    "$$\n",
    "\n",
    "$$\n",
    "(e^a)^b = e^{a\\cdot b}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithm\n",
    "\n",
    "Logarithmic function is the inverse for the exponential function. \n",
    "\n",
    "If $y$ is computed as the exponential of $x$\n",
    "\n",
    "$$\n",
    "y=b^x\n",
    "$$\n",
    "\n",
    "then we can find $x$ using the logarithm:\n",
    "\n",
    "$$\n",
    "x=\\log_b y\n",
    "$$\n",
    "\n",
    "Here $b$ is called the logarithm base.\n",
    "\n",
    "Most often three bases are used: 2, 10 and $e$:\n",
    "\n",
    "$$\n",
    "\\log_{10} x, \\;\n",
    "\\log_{2} x, \\;\n",
    "\\log_{e} x\n",
    "$$\n",
    "\n",
    "In the last case the base $e$ is usually omitted. \n",
    "\n",
    "The logarithm to the base $e$ is called natural:\n",
    "\n",
    "$$\n",
    "y = e^x\\;\\Rightarrow x=\\log y\n",
    "$$\n",
    "\n",
    "If not specified the natural logarithm is assumed. \n",
    "\n",
    "Graph of the logarithmic function.\n",
    "\n",
    "Observe that the logarithm is the slowest elementary function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(1e-3, 5, 100)\n",
    "y = np.log(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$\\log x$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of the logarithmic function:\n",
    "\n",
    "$$\n",
    "y=\\log x \\text{ only for $x>0$}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log 1 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log x < 0 \\text{ for x < 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log x > 0 \\text{ for x > 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log a\\cdot b = \\log a + \\log b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log a / b = \\log a - \\log b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log a^p = p \\log a\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random variables\n",
    "\n",
    "A variable is random when each reading its value results in different values.\n",
    "\n",
    "Given a random variable we can not predict each its particular value. But we know a range of possible values and probabilities of each value. \n",
    "\n",
    "A complete set of probabilities describing appearance of each value is called a probability distribution.\n",
    "\n",
    "Visual representation of the probability distribution is a histogram.\n",
    "\n",
    "This is just a simple illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "size = 10000\n",
    "n1 = 1000\n",
    "n2 = 500\n",
    "n3 = 1500\n",
    "n4 = 4000\n",
    "n5 = size - (n1 + n2 + n3 + n4)\n",
    "\n",
    "# Create a sequence with a given number of values\n",
    "data = [10] * n1 + [20] * n2 + [30] * n3 + [40] * n4 + [50] * n5\n",
    "\n",
    "# Set of possible values\n",
    "print(\"Set of possible values\", set(data))\n",
    "\n",
    "# Shuffle the data - obtain a random sequence\n",
    "rng = np.random.default_rng()\n",
    "rng.shuffle(data)\n",
    "print(\"Data sample\", data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Edges of bins\n",
    "bin_edges = [5,15,25,35,45,55]\n",
    "\n",
    "ax.hist(data, bins=bin_edges, rwidth=0.8)\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete and continuous random variables\n",
    "\n",
    "Data in the example above can be considered as a record of multiple readings of a random variable. \n",
    "\n",
    "Its possible vales belong to a finite set of integers. \n",
    "\n",
    "Such random variable is called discrete.\n",
    "\n",
    "Continuous random variable has a continuum of outcomes.\n",
    "\n",
    "Examples from a real world: wind speed or traffic rate during a day. These values are recorded as real numbers and can have any values within certain range.\n",
    "\n",
    "Numerical example: random number generator that produces real numbers between 0 and 1.\n",
    "\n",
    "*The important point* about random values expressed as real numbers: since there are infinitely many real numbers each particular number, say $\\pi/4$, has *a vanishing probability*. \n",
    "\n",
    "It doesn't make any sense to speak about probabilities of any particular real number. This is zero. Instead it is reasonable to consider a probability for the number to fall within some range of values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability density function\n",
    "\n",
    "Distribution of probabilities of a continuous variable is described with a probability *density* function (PDF). Usually we will denote it as $\\rho(X)$. \n",
    "\n",
    "Word *density* here means that if we want to have a probability itself we must multiply the density by the range width.\n",
    "\n",
    "The probability for $x$ to have a value between $X$ and $X+h$ is equal to\n",
    "\n",
    "$$\n",
    "P(x\\in [X, X+h])=\\rho(X) h\n",
    "$$\n",
    "\n",
    "Actually this is more or less correct when $h$ is very small. \n",
    "\n",
    "Speaking more strictly the probability of seeing a value from a certain interval equals to an area under the corresponding fragment of the PDF curve. \n",
    "\n",
    "The area in turn is computed as an integral of $\\rho(X)$:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P(x\\in [X, X+h])=\\int_{X}^{X+h} \\rho(x) dx\n",
    "$$\n",
    "\n",
    "![pdf_example.svg](fig/pdf_example.svg)\n",
    "\n",
    "The area under the whole curve is always 1. \n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} \\rho(x) dx = 1\n",
    "$$\n",
    "\n",
    "This property is called normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative distribution function\n",
    "\n",
    "In addition to PDF the cumulative distribution function\n",
    "(CDF) is considered. It gives the probability that a random variable $x$ is less than or equal to a certain value $X$. \n",
    "\n",
    "$$\n",
    "P(x\\leq X) = \\phi(X)\n",
    "$$\n",
    "\n",
    "![cdf_example1.svg](fig/cdf_example1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complement to CDF is the probability that a random variable $x$ is above $X$:\n",
    "\n",
    "$$\n",
    "P(x>X) = 1 - \\phi(X)\n",
    "$$\n",
    "\n",
    "![cdf_example2.svg](fig/cdf_example2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the probability to find $x$ between $X$ and $X+h$ equals\n",
    "\n",
    "$$\n",
    "P(x\\in [X, X+h])=\\phi(X+h) - \\phi(X)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "![cdf_example3.svg](fig/cdf_example3.svg)\n",
    "\n",
    "CDF is computed via PDF as follows\n",
    "\n",
    "$$\n",
    "\\phi(x)=\\int_{-\\infty}^{x} \\rho(\\xi) d \\xi\n",
    "$$\n",
    "\n",
    "Given CDF we compute PDF as\n",
    "\n",
    "$$\n",
    "\\rho(x) = \\frac{d \\phi(x)}{dx}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform distribution\n",
    "\n",
    "Real numbers between 0 and 1 are generated with equal probabilities.\n",
    "\n",
    "Equations for the uniform PDF:\n",
    "\n",
    "$$\n",
    "\\rho(x) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if $x<0$} \\\\\n",
    "1 & \\text{if $0\\leq x< 1$} \\\\\n",
    "0 & \\text{if $x\\geq 1$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the corresponding CDF:\n",
    "\n",
    "$$\n",
    "\\phi(x) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if $x<0$} \\\\\n",
    "x & \\text{if $0\\leq x< 1$} \\\\\n",
    "1 & \\text{if $x\\geq 1$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To show their graphs let us first define corresponding Python functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unif_pdf(x):\n",
    "    \"\"\"PDF for uniform distribution\"\"\"\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    elif x <= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def unif_cdf(x):\n",
    "    \"\"\"CDF for uniform distribution\"\"\"\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    elif x <= 1:\n",
    "        return x\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can plot the graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 4.5))\n",
    "\n",
    "x = np.linspace(-0.5, 1.5, 100)\n",
    "y_pdf = [unif_pdf(xi) for xi in x]\n",
    "y_cdf = [unif_cdf(xi) for xi in x]\n",
    "\n",
    "axs[0].plot(x, y_pdf)\n",
    "axs[1].plot(x, y_cdf)\n",
    "\n",
    "axs[0].set_title(\"PDF\")\n",
    "axs[0].set_xlabel(r'$x$')\n",
    "axs[0].set_ylabel(r'$\\rho$')\n",
    "\n",
    "axs[1].set_title(\"CDF\")\n",
    "axs[1].set_xlabel(r'$x$')\n",
    "axs[1].set_ylabel(r'$\\phi$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an experimental dataset its PDF can be estimated via histogram. \n",
    "\n",
    "Let us first recall how we can generate random numbers with uniform distribution between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This object is a random number generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method of the generator models a random variable with a uniform distribution from 0 to 1.\n",
    "print(rng.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now generate a dataset of uniformly distributed random values and visualize its PDF using a histogram.\n",
    "\n",
    "Notice an option `density=True` that triggers normalization of bin heights to obtain histogram approximation of PDF. \n",
    "\n",
    "Size of dataset and number of bins influences a quality of PDF approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "full_size = 100000\n",
    "shrt_size = full_size // 10\n",
    "nbins = [10, 100, 300]\n",
    "\n",
    "data = rng.random(size=full_size)\n",
    "\n",
    "x = np.linspace(-0.5, 1.5, 100)\n",
    "y_pdf = [unif_pdf(xi) for xi in x]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(10, 12), sharex=True)\n",
    "\n",
    "# Left column is plotted for shorter dataset\n",
    "ax_col = axs[:, 0]\n",
    "for ax, b in zip(ax_col, nbins):\n",
    "    ax.hist(data[:shrt_size], bins=b, density=True, label=\"hist\")\n",
    "    ax.plot(x, y_pdf, label=\"graph\")\n",
    "    ax.set_title(f\"bins={b}, size={shrt_size}\")\n",
    "    ax.set_xlim([-0.5, 1.5])\n",
    "    ax.set_ylim([0, 1.4])\n",
    "    ax.legend()\n",
    "    \n",
    "ax_col[-1].set_xlabel(r'$x$');\n",
    "\n",
    "# Right column is plotted for the full dataset\n",
    "ax_col = axs[:, 1]\n",
    "for ax, b in zip(ax_col, nbins):\n",
    "    ax.hist(data, bins=b, density=True, label=\"hist\")\n",
    "    ax.plot(x, y_pdf, label=\"graph\")\n",
    "    ax.set_title(f\"bins={b}, size={full_size}\")\n",
    "    ax.set_xlim([-0.5, 1.5])\n",
    "    ax.set_ylim([0, 1.4])\n",
    "    ax.legend()\n",
    "    \n",
    "ax_col[-1].set_xlabel(r'$x$');    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using larger number of bins we see finer structure of the PDF. \n",
    "\n",
    "Size of the dataset impacts upon this structure. The PDF looks more similar to the theoretical one for larger dataset.\n",
    "\n",
    "To plot the cumulative histogram that approximates CDF we use an option `cumulative=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "full_size = 100000\n",
    "data = rng.random(size=full_size)\n",
    "\n",
    "x = np.linspace(-0.5, 1.5, 100)\n",
    "y_cdf = [unif_cdf(xi) for xi in x]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(data, bins=300, density=True, cumulative=True, label=\"hist\")\n",
    "ax.plot(x, y_cdf, label=\"graph\")\n",
    "ax.set_xlim([-0.5, 1.5])\n",
    "ax.legend()\n",
    "ax.set_xlabel(r'$x$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the cumulative histogram is less affected by finite size effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The normal distribution\n",
    "\n",
    "The normal distribution is also called Gaussian distribution. It has bell or hump shape and is completely determined by two parameters: its mean $\\mu$ and its standard deviation $\\sigma$.\n",
    "\n",
    "The mean determines where the distribution is centered, and the standard deviation indicates its width.\n",
    "\n",
    "The PDF for this distribution:\n",
    "\n",
    "$$\n",
    "\\rho(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\mathrm{exp}\n",
    "\\left(\n",
    "-\\frac{(x-\\mu)^2}{2\\sigma^2}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "The corresponding CDF can not be written via elementary functions:\n",
    "\n",
    "$$\n",
    "\\phi(x)=\\frac{1}{2}\n",
    "\\left[\n",
    "1+\\mathrm{erf}\n",
    "\\left(\n",
    "\\frac{x-\\mu}{\\sigma\\sqrt{2}}\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "where $\\mathrm{erf}$ is called Gauss error function.\n",
    "\n",
    "When $\\mu=0$ and $\\sigma=1$ the distribution is called the standard normal distribution.\n",
    "\n",
    "If $S$ is a random variable with standard normal distribution, then it turns out that \n",
    "\n",
    "$$\n",
    "X = \\sigma S + \\mu\n",
    "$$\n",
    "\n",
    "is also a random variable with normal distribution but with mean $\\mu$ and standard deviation $\\sigma$.\n",
    "\n",
    "Below are examples of PDF's, CFD's for normal distribution.\n",
    "\n",
    "Observe how $\\sigma$ controls the width of the bell-shaped PDF curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def norm_pdf(x, mu, sig):\n",
    "    \"\"\"Normal probability density function\"\"\"\n",
    "    return np.exp(-(x-mu)**2 / (2*sig*sig)) / (sig * np.sqrt(2*np.pi))\n",
    "\n",
    "def norm_cdf(x, mu, sig):\n",
    "    \"\"\"Normal cumulative distribution function\"\"\"\n",
    "    return 0.5 * (1 + erf((x-mu)/(sig*np.sqrt(2))))\n",
    "\n",
    "mu1, sig1 = 0, 1\n",
    "mu2, sig2 = 1, 1.5\n",
    "\n",
    "x = np.linspace(-5, 7, 100)\n",
    "\n",
    "y1_pdf = [norm_pdf(xi, mu1, sig1) for xi in x]\n",
    "y1_cdf = [norm_cdf(xi, mu1, sig1) for xi in x]\n",
    "\n",
    "y2_pdf = [norm_pdf(xi, mu2, sig2) for xi in x]\n",
    "y2_cdf = [norm_cdf(xi, mu2, sig2) for xi in x]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 4.5))\n",
    "\n",
    "axs[0].plot(x, y1_pdf, label=f\"mu={mu1}, sig={sig1}\")\n",
    "axs[0].plot(x, y2_pdf, label=f\"mu={mu2}, sig={sig2}\")\n",
    "axs[0].set_title(\"PDF\")\n",
    "\n",
    "axs[1].plot(x, y1_cdf, label=f\"mu={mu1}, sig={sig1}\")\n",
    "axs[1].plot(x, y2_cdf, label=f\"mu={mu2}, sig={sig2}\")\n",
    "axs[1].set_title(\"CDF\")\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_xlabel(r'$x$')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a histogram obtained for numerical datasets.\n",
    "\n",
    "Notice that random numbers with normal distribution can be generated with the method `normal` of the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100000\n",
    "nbins = 300\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Random data with standard normal distribution\n",
    "data1 = rng.normal(loc=mu1, scale=sig1, size=size)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 4.5))\n",
    "\n",
    "axs[0].hist(data1, bins=nbins, density=True, label=\"hist\")\n",
    "axs[0].plot(x, y1_pdf, label=\"graph\")\n",
    "axs[0].set_title(\"PDF\")\n",
    "\n",
    "axs[1].hist(data1, bins=nbins, density=True, cumulative=True, label=\"hist\")\n",
    "axs[1].plot(x, y1_cdf, label=\"graph\")\n",
    "axs[1].set_title(\"CDF\")\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_xlabel(r'$x$')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the theoretical curve the cumulative histogram has a cutoff. This is finale size effect. Increasing the size of the dataset shifts the cutoff to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The central limit theorem\n",
    "\n",
    "The reason why the distribution is called normal is the central limit theorem.\n",
    "\n",
    "Consider $n$ random variables $x_1$, $x_2$, ... , $x_n$. Assume that all of them are independent or almost independent. \n",
    "\n",
    "Also assume that all of them have more or less similar ranges of variation. It means that no one of them dominates.\n",
    "\n",
    "Define a new random variable\n",
    "\n",
    "$$\n",
    "z_n = \\frac{1}{n}(x_1+x_2+\\cdots+x_n)\n",
    "$$\n",
    "\n",
    "The central limit theorem says that $z_n$ is approximately normally distributed. \n",
    "\n",
    "The larger number $n$ of the averaged random values the closer the distribution to the normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli random variables\n",
    "Bernoulli random variable is a discrete random variable with two outcomes: \"fail\" (0) or \"success\" (1). The probability of the success equals to $p$ and the probability of the fail is $1-p$.\n",
    "\n",
    "The fair coin, i.e., the coin that lands with heads or tails with the probability $p=0.5$ is an example of a Bernoulli variable. \n",
    "\n",
    "One can also imagine a biased coin that lands with heads with the probability $p\\in [0,1]$. It will be another, more general example of the Bernoulli variable.\n",
    "\n",
    "The mean and the standard deviation of a Bernoulli variable is, respectively,\n",
    "\n",
    "$$\n",
    "\\mu=p, \\; \\sigma=\\sqrt{p(1-p)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The binomial distribution\n",
    "\n",
    "Let us now consider a series of $n$ tossing of a biased coin and count a number of heads $B$. \n",
    "\n",
    "In the other words we have $n$ independent Bernoulli variables each having the probability $p$ and are interested in the number of successes $B$.\n",
    "\n",
    "This number $B$ is a random integer between 0 and $n$. \n",
    "\n",
    "The distribution of $B$ is called the binomial distribution:\n",
    "\n",
    "$$\n",
    "P(B=k) = \n",
    "\\binom{n}{k} p^{k}(1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "Here $k$ is an integer between 0 and $n$. The multiplier $\\binom{n}{k}$ is called binomial coefficient:\n",
    "\n",
    "$$\n",
    "\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n",
    "$$\n",
    "\n",
    "According to the central limit theorem the binomial distribution with sufficiently large $n$ will be close to the normal distribution with \n",
    "\n",
    "$$\n",
    "\\mu=np,\\; \\sigma=\\sqrt{np(1-p)}\n",
    "$$\n",
    "\n",
    "Let us check it. Observe that we compute binomial coefficients using the function `binom` from `scipy.special`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import binom\n",
    "\n",
    "def norm_pdf(x, mu, sig):\n",
    "    return np.exp(-(x-mu)**2 / (2*sig*sig)) / (sig * np.sqrt(2*np.pi))\n",
    "\n",
    "def binom_distr(n, k, p):\n",
    "    \"\"\"Binomial distribution\"\"\"\n",
    "    return binom(n, k) * p**k * (1-p)**(n-k)\n",
    "    \n",
    "p = 0.8\n",
    "ns = [10, 50, 100]    \n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(10, 12))\n",
    "\n",
    "for ax, n in zip(axs, ns):\n",
    "    k = np.arange(n+1)\n",
    "    b = [binom_distr(n, ki, p) for ki in k]\n",
    "    mu = n * p\n",
    "    sig = np.sqrt(n * p * (1 - p))\n",
    "    x = np.linspace(0, n+1, 100)\n",
    "    y = [norm_pdf(xi, mu, sig) for xi in x]\n",
    "    ax.bar(k, b, color=\"C0\", label=f\"binom, n={n}\")\n",
    "    ax.plot(x, y, color=\"C1\", label=f\"normal, mu={mu:.2}, sig={sig:.2}\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(r'$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information amount\n",
    "\n",
    "In this section for the sake of simplicity we will always consider only discrete random variables.\n",
    "\n",
    "Dealing with a random variable we never know its next value. Thus when we obtain it we obtain some new information. \n",
    "\n",
    "How to measure an amount of this information?\n",
    "\n",
    "The basic intuition here is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. \n",
    "\n",
    "A message saying \"the sun rose this morning\" is so uninformative as to be unnecessary to send, but a message saying \"there was a flash in the sun this morning\" is very informative.\n",
    "\n",
    "We would like to quantify information in a way that formalizes this intuition.\n",
    "\n",
    "- Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever.\n",
    "\n",
    "- Less likely events should have higher information content.\n",
    "\n",
    "- Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once.\n",
    "\n",
    "These three properties are satisfied if we define the particular information of an event as the logarithm of its probability $P(x)$:\n",
    "\n",
    "$$\n",
    "I(x) = - \\log P(x)\n",
    "$$\n",
    "\n",
    "Here we assume that $\\log$ is the natural logarithm, with base $e$. \n",
    "\n",
    "Below observe a graph of $I(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "P = np.linspace(1e-3, 1, 100)\n",
    "I = -np.log(P)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(P, I)\n",
    "ax.set_xlabel(r\"$P$\")\n",
    "ax.set_ylabel(r\"$I=-\\log \\, P$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us discuss how the above items are fulfilled with $I(x)$.\n",
    "\n",
    "- Likely events have probability close to 1. We observe that when $P\\to 1$ the graph approaches zero, $I\\to 0$.\n",
    "\n",
    "- Less likely events have vanishing probability. We observe that when $P\\to 0$ the information grows, $I\\to\\infty$.\n",
    "\n",
    "- Probability of two independent events is the product of their marginal probabilities: $P(x,y) = P(x) P(y)$. Thus \n",
    "\n",
    "$$\n",
    "I(x,y)=-\\log P(x,y) = -\\log P(x) - \\log P(y) = I(x) + I(y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Shanon entropy\n",
    "\n",
    "Assume that we have a random variable $x$ with $n$ possible outcomes \n",
    "$x_i$ ($i=1,2,\\ldots, n$), whose probabilities are $P(x_i)$.\n",
    "\n",
    "Assume now that we record multiple successive outcomes, i.e., the variable is an information source. \n",
    "\n",
    "We already know that each particular outcome conveys an information $I(x_i)$. \n",
    "\n",
    "To characterize the information source as a whole we can average particular informations over the probability distribution:\n",
    "\n",
    "$$\n",
    "H(P) = - \\sum_{i=1}^{n} P(x_i) \\log P(x_i)\n",
    "$$\n",
    "\n",
    "here $H$ is called the Shannon entropy or the information entropy.\n",
    "\n",
    "The Shannon entropy of a distribution is the expected average amount of information in an event drawn from that distribution. \n",
    "\n",
    "If $P(x_1)=1$ and $P(x_{2,3,\\ldots,n})=0$ the variable is in fact deterministic: only the first outcome $x_1$ occurs all time. \n",
    "\n",
    "Notice that as follows from the properties of logarithms $(1\\cdot \\log 1) = 0$. Also can it be computed that $(0\\cdot \\log 0)=0$.\n",
    "\n",
    "Thus in this case in accordance with our intuition \n",
    "\n",
    "$$\n",
    "H(P)=0\n",
    "$$\n",
    "\n",
    "i.e., the information is not generated.\n",
    "\n",
    "If on contrary all probabilities are identical, $P(x_i) = 1/n$, the uncertainty is the largest. \n",
    "\n",
    "We expect the largest information in this case and indeed the entropy is the highest:\n",
    "\n",
    "$$\n",
    "H(P) = \\log n\n",
    "$$\n",
    "\n",
    "To provide more detailed illustration consider an entropy of a Bernoulli variable (recall that is has only two outcomes):\n",
    "\n",
    "$$\n",
    "H(P) = -P\\log P - (1-P) \\log (1-P)\n",
    "$$\n",
    "\n",
    "Its smallest value is zero when $P = 0$ or $P = 1$ and its largest value is $H=\\log 2$ when $P=1/2$.\n",
    "\n",
    "Below is the graph of this entropy as a function of $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def entrop(P):\n",
    "    if P == 0.0 or P == 1.0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return -P * np.log(P) - (1-P) * np.log(1-P)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "P = np.linspace(0, 1, 100)\n",
    "H = [entrop(pi) for pi in P]\n",
    "\n",
    "ax.plot(P, H)\n",
    "ax.set_xlabel(r'$P$')\n",
    "ax.set_ylabel(r'$H(P)$')\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kullback-Leibler divergence\n",
    "\n",
    "Assume that we have a discrete random variable with $n$ outcomes $x_1$, $x_2$, ... , $x_n$. \n",
    "\n",
    "The true probability distribution of these outcomes is known in advance and is $P(x_i)$. \n",
    "\n",
    "Assume now that we have built a model of this stochastic process that predicts this distribution as $Q(x_i)$. \n",
    "\n",
    "We want to estimate the performance of our model comparing true distribution $P(x_i)$ and the approximated one $Q(x_i)$.\n",
    "\n",
    "For this purpose the Kullback-Leibler (KL) divergence can be  used. This is also called the relative entropy because  it gives the loss of information if $P$ is modeled\n",
    "in terms of $Q$: \n",
    "\n",
    "$$\n",
    "D_{KL}(P,Q) = \\sum_{i=1}^n P(x_i) \n",
    "\\log \\frac{P(x_i)}{Q(x_i)}\n",
    "$$\n",
    "\n",
    "Note that $D_{KL}(P, Q) \\geq 0$ with equality if and only if $P=Q$. \n",
    "\n",
    "Sometimes Kullbackâ€“Leibler divergence is called the Kullbackâ€“Leibler distance, but it is not\n",
    "formally a distance metric because it is asymmetrical in P and Q. \n",
    "\n",
    "Here is a simple illustration how this equation works.\n",
    "\n",
    "Assume that the random variable produces three outcomes with the probabilities \n",
    "\n",
    "$$\n",
    "P_1 = 0.5, P_2 = 0.3, P_3 = 0.2\n",
    "$$\n",
    "\n",
    "The first model predicts them as\n",
    "\n",
    "$$\n",
    "Q_1 = 0.4, Q_2 = 0.35, Q_3 = 0.25\n",
    "$$\n",
    "\n",
    "and the second one produces the distribution\n",
    "\n",
    "$$\n",
    "Q_2 = 0.4, Q_2 = 0.4, Q_3 = 0.2\n",
    "$$\n",
    "\n",
    "Which one is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kl_div(P, Q):\n",
    "    \"\"\"Kullbackâ€“Leibler divergence\n",
    "    For simplicity we do not check if Pi or Qi are zeros and hope thay are not\n",
    "    \"\"\"\n",
    "    assert len(P) == len(Q)\n",
    "    assert abs(sum(P)-1) < 1e-10 and abs(sum(Q)-1) < 1e-10\n",
    "    return sum([p * np.log(p / q) for p, q in zip(P, Q)])\n",
    "    \n",
    "P = [0.5, 0.3, 0.2]\n",
    "Q1 = [0.4, 0.35, 0.25]\n",
    "Q2 = [0.4, 0.4, 0.2]\n",
    "\n",
    "print(f\"KL for P and Q1 {kl_div(P, Q1):.5f}\")\n",
    "print(f\"KL for P and Q2 {kl_div(P, Q2):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first model is preferable since KL divergence for $Q_1$ and P is less then that for $Q_2$ and $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy\n",
    "\n",
    "Equation for the Kullbackâ€“Leibler divergence can be rewritten as follows:\n",
    "\n",
    "$$\n",
    "D_{KL}(P,Q) = \\sum_{i=1}^n P(x_i) \n",
    "(\\log P(x_i) - \\log Q(x_i))=\n",
    "\\sum_{i=1}^n P(x_i) \\log P(x_i) -\n",
    "\\sum_{i=1}^n P(x_i) \\log Q(x_i)\n",
    "$$\n",
    "\n",
    "Here we recognize the information entropy\n",
    "\n",
    "$$\n",
    "H(P) = -\\sum_{i=1}^n P(x_i) \\log P(x_i)\n",
    "$$\n",
    "\n",
    "The second item of the formula for the KL divergence is\n",
    "called *the cross entropy*:\n",
    "\n",
    "$$\n",
    "H(P,Q) = - \\sum_{i=1}^n P(x_i) \\log Q(x_i)\n",
    "$$\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "$$\n",
    "H(P,Q) = D_{KL}(P, Q) + H(P)\n",
    "$$\n",
    "\n",
    "Cross entropy of two distributions $P$ and $Q$ differs from KL divergence by an offset $H(P)$.\n",
    "\n",
    "Let us remember that in these equations we consider $P$ as a given true probability distribution and $Q$ is its approximation. \n",
    "\n",
    "Tuning a model we search for the better $Q$ while $P$ remains unchanged. \n",
    "\n",
    "Minimizing the cross-entropy with respect to $Q$ is equivalent to minimizing the KL divergence, because $Q$\n",
    "does not participate in the omitted term $H(P)$.\n",
    "\n",
    "Reconsider the above example computing now the cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_ent(P, Q):\n",
    "    \"\"\"Cross-entropy\"\"\"\n",
    "    assert len(P) == len(Q)\n",
    "    assert abs(sum(P)-1) < 1e-10 and abs(sum(Q)-1) < 1e-10\n",
    "    return sum([-p * np.log(q) for p, q in zip(P, Q)])\n",
    "    \n",
    "P = [0.5, 0.3, 0.2]\n",
    "Q1 = [0.4, 0.35, 0.25]\n",
    "Q2 = [0.4, 0.4, 0.2]\n",
    "\n",
    "print(f\"Cross-entropy for P and Q1 {cross_ent(P, Q1):.5f}\")\n",
    "print(f\"Cross-entropy for P and Q2 {cross_ent(P, Q2):.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "ru",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "113px",
    "width": "228px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
