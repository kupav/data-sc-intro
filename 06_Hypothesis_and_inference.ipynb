{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 Hypothesis and inference\n",
    "\n",
    "Part of [\"Introduction to Data Science\" course](https://github.com/kupav/data-sc-intro) by Pavel Kuptsov, [kupav@mail.ru](mailto:kupav@mail.ru)\n",
    "\n",
    "Recommended reading for this section:\n",
    "\n",
    "1. Grus, J. (2019). Data Science From Scratch: First Principles with Python (Vol. Second edition). Sebastopol, CA: O’Reilly Media\n",
    "\n",
    "The following Python modules will be required. Make sure that you have them installed.\n",
    "- `matplotlib`\n",
    "- `numpy`\n",
    "- `scipy`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering properties of a deterministic system\n",
    "\n",
    "When we explore a deterministic system usually we can easily establish that two its characteristics depends on each other: \n",
    "\n",
    "We need to measure and record one and another and thus recover their dependence.\n",
    "\n",
    "Consider a system that includes a heated pot of water, a bunch of fresh eggs and a timer. \n",
    "\n",
    "We want to know how to boil the eggs. \n",
    "\n",
    "![boil.svg](fig/boil.svg)\n",
    "\n",
    "Conducting the experiment we boil the water, put their an egg, boil for some time, pull the egg out and check it.\n",
    "\n",
    "In this experiment we have two characteristics: time of boiling and the resulting state of eggs. \n",
    "\n",
    "Since the system is deterministic we can repeat the experiment many times varying time of boiling and finally discover that after 3 minutes of boiling we obtain a soft-boiled eggs and 10 minutes is enough for hard-boiled eggs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering for stochastic systems\n",
    "\n",
    "But for a stochastic system  we cannot have solid answers and predictions.\n",
    "\n",
    "Instead we can form hypothesis and test their likelihood.\n",
    "\n",
    "Assume that we want to predict a weather using a barometer.\n",
    "\n",
    "![barometer.svg](fig/barometer.svg)\n",
    "\n",
    "Our system includes the barometer and surrounding conditions. \n",
    "\n",
    "The experiment includes recording the barometer readings and subsequent weather observation during several next ours. \n",
    "\n",
    "Repeating the experiment many times we will discover that often a low atmospheric pressure results in a bad weather and a good weather follows a high atmospheric pressure. \n",
    "\n",
    "But we also will notice that this is not the strict dependence: in some cases high pressure can be followed by a bad weather and vice versa.\n",
    "\n",
    "In the other words the weather probably depends on the atmospheric pressure but this is not one hundred percent correct.\n",
    "\n",
    "We have an uncertainty and the only thing that can be done is to use statistical methods to estimate its degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "\n",
    "Dealing with stochastic experiments and trying to extract as much information from them as possible we form statistical hypothesis about the presence of some properties or dependencies and test them. \n",
    "\n",
    "This part of statistics is called hypothesis testing.\n",
    "\n",
    "It includes the following steps.\n",
    "\n",
    "- Form a null hypothesis. This is the statement that the property we are interested for is absent, one characteristic is not influenced by another and so on. This hypothesis is usually denoted as $H_0$.\n",
    "  \n",
    "- Form an alternative hypothesis $H_1$ that states the presence of the property. $H_0$ and $H_1$ must be mutually exclusive (no result could satisfy both conditions) and exhaustive (all possible results will satisfy one of the two conditions). \n",
    "\n",
    "- The purpose of the hypothesis testing is to conclude weather the null hypothesis can be rejected or not. Rejecting $H_0$ often means that we have discovered some new feature and this probably not due to chance.\n",
    "  \n",
    "- Make a decision about the significance $\\alpha$. Errors are unavoidable and $\\alpha$ is the accepted probability to make a type 1 error. \n",
    "\n",
    "- This error means the rejecting $H_0$ though it is true (\"Yay, we've got it\" by mistake). \n",
    "\n",
    "- Usual value for $\\alpha$ is $0.05$, however there are   studies with $\\alpha=0.01$ or even smaller.\n",
    "\n",
    "- Compute the range of the experiment outcomes such that when  the observations fall within we have to accept $H_0$ hypothesis.\n",
    "\n",
    "- For the computed range estimate the type 2 error probability   $\\beta$. \n",
    "\n",
    "- This error means that we fail to reject $H_0$ even when it is false. (The discovery was right in front of us but we've missed it.)\n",
    "\n",
    "- Usually taking very small $\\alpha$ results in increasing $\\beta$. The overall setup is correct if $\\beta$ is not much higher then $\\alpha$. For $\\alpha=0.05$ the appropriate is $\\beta\\approx 0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation of an example\n",
    "\n",
    "Below we will consider an example: there is a coin and we want to check if it is unfair. \n",
    "\n",
    "It means that the probability of showing up its heads or tails is not $0.5$.\n",
    "\n",
    "The only think we can do is to toss it many times and count the appearances of the heads.\n",
    "\n",
    "To draw a conclusion we need to know what ranges of values confirm the fairness or unfairness of the coin. \n",
    "\n",
    "Before proceeding with the hypothesis testing we need to model our experiment and discuss the required computer tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Bernoulli variable and binomial distribution\n",
    "\n",
    "Let us first remember from the previous lecture that  mathematically our coin can be represented as a Bernoulli random variable: a discrete random variable with two outcomes: \"tails\" (0) or \"heads\" (1). \n",
    "\n",
    "The probability of heads is $P$ and tails are showed up with the probability $1-P$.\n",
    "\n",
    "To reveal the unfairness we will repeat the experiment $N$ times and count heads outcomes. Let it be $x$. \n",
    "\n",
    "For infinitely large $N\\to\\infty$ the count of heads tends to $x\\to PN$. For example for a fair coin this is $x\\to N/2$. \n",
    "\n",
    "But if $N$ is not infinite this $x$ is itself a random variable. \n",
    "\n",
    "It means that if we will repeat a series of $N$ tossing again and again we will obtain different $x$.\n",
    "\n",
    "Let us also remember: The discrete random variable $x$ is governed by the binomial distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Central limit theorem and normal distributions\n",
    "\n",
    "Due to the central limit theorem if $N$ is sufficiently large the binomial distribution can be approximately described by a normal distribution. \n",
    "\n",
    "The PDF (probability density function) for the normal distribution is\n",
    "\n",
    "$$\n",
    "\\rho(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\mathrm{exp}\n",
    "\\left(\n",
    "-\\frac{(x-\\mu)^2}{2\\sigma^2}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "and the corresponding CDF (cumulative distribution function) reads\n",
    "\n",
    "$$\n",
    "\\phi(x)=\\frac{1}{2}\n",
    "\\left[\n",
    "1+\\mathrm{erf}\n",
    "\\left(\n",
    "\\frac{x-\\mu}{\\sigma\\sqrt{2}}\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "where the mean $\\mu$ and the standard deviation $\\sigma$ are \n",
    "\n",
    "$$\n",
    "\\mu=NP,\\; \\sigma=\\sqrt{N P (1-P)}\n",
    "$$\n",
    "\n",
    "These formulas can be implemented as Python functions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "\n",
    "def get_mu_sig(N, P):\n",
    "    \"\"\"\n",
    "    mu and sig corresponding to a binomial distribution\"\"\"\n",
    "    mu = P * N\n",
    "    sig = np.sqrt(N * P * (1 - P))\n",
    "    return mu, sig\n",
    "\n",
    "def norm_pdf(x, mu, sig):\n",
    "    \"\"\"Normal probability density function\"\"\"\n",
    "    return np.exp(-(x-mu)**2 / (2*sig*sig)) / (sig * np.sqrt(2*np.pi))\n",
    "\n",
    "def norm_cdf(x, mu, sig):\n",
    "    \"\"\"Normal cumulative distribution function\"\"\"\n",
    "    return 0.5 * (1 + erf((x-mu)/(sig*np.sqrt(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the plots of PDF and CDF approximating the binomial distribution at $N=1000$ and $P=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "N = 1000\n",
    "P = 0.5\n",
    "\n",
    "mu, sig = get_mu_sig(N, P)\n",
    "\n",
    "x = np.linspace(mu - 100, mu + 100, 100)\n",
    "\n",
    "y1_pdf = [norm_pdf(xi, mu, sig) for xi in x]\n",
    "y1_cdf = [norm_cdf(xi, mu, sig) for xi in x]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 4.5))\n",
    "\n",
    "axs[0].plot(x, y1_pdf, label=f\"mu={mu:.2f}, sig={sig:.2f}\")\n",
    "axs[0].set_title(\"PDF\")\n",
    "axs[0].set_xlabel(r\"$x$\")\n",
    "axs[0].set_ylabel(r\"$\\rho(x)$\")\n",
    "\n",
    "axs[1].plot(x, y1_cdf, label=f\"mu={mu:.2f}, sig={sig:.2f}\")\n",
    "axs[1].set_title(\"CDF\")\n",
    "axs[1].set_xlabel(r\"$X$\")\n",
    "axs[1].set_ylabel(r\"$P(x<X)$\")\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.set_xlabel(r'$x$')\n",
    "    ax.legend()\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Computing probabilities of a random normal variable with CDF\n",
    "\n",
    "Let us remember: CDF gives the probability that a random variable $x$ is less than or equal to a certain value $X$. \n",
    "\n",
    "$$\n",
    "P(x\\leq X) = \\phi(X)\n",
    "$$\n",
    "\n",
    "![cdf_example1.svg](fig/cdf_example1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complement to CDF is the probability that a random variable $x$ is above $X$:\n",
    "\n",
    "$$\n",
    "P(x>X) = 1 - \\phi(X)\n",
    "$$\n",
    "\n",
    "![cdf_example2.svg](fig/cdf_example2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability to find $x$ between $X$ and $X+h$ equals\n",
    "\n",
    "$$\n",
    "P(x\\in [X, X+h])=\\phi(X+h) - \\phi(X)\n",
    "$$\n",
    "\n",
    "![cdf_example3.svg](fig/cdf_example3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical note: Function inverse\n",
    "\n",
    "Knowing the CDF we can compute probabilities for a given value of a random variable. \n",
    "\n",
    "But to perform the hypothesis test we take a probability of making an error and compute the corresponding values of a random variable. \n",
    "\n",
    "It means that we need to find the inverse of the CDF.\n",
    "\n",
    "Consider a simple example: Given the exponential function\n",
    "\n",
    "$$\n",
    "y = e^x\n",
    "$$\n",
    "find its inverse.\n",
    "\n",
    "In this particular case it is easy since we know the inverse for the exponential function:\n",
    "\n",
    "$$\n",
    "x = \\log y\n",
    "$$\n",
    "\n",
    "Below are the graphs of the original and the inverted functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "xx = np.linspace(-2, 5, 100)\n",
    "yy = [np.exp(xi) for xi in xx]\n",
    "xinv = [np.log(yi) for yi in yy] \n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(xx, yy)\n",
    "axs[0].set_title(r\"$y=e^x$\")\n",
    "axs[0].set_xlabel(r\"$x$\")\n",
    "axs[0].set_ylabel(r\"$y$\")\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(yy, xinv)\n",
    "axs[1].set_title(r\"$x=\\log y$\")\n",
    "axs[1].set_xlabel(r\"$y$\")\n",
    "axs[1].set_ylabel(r\"$x$\")\n",
    "axs[1].grid()\n",
    "\n",
    "# Put markers to improve clarity\n",
    "mx = [-1, 4.5]\n",
    "my = [np.exp(-1), np.exp(4.5)]\n",
    "axs[0].scatter(mx, my, c=['r', 'g'], s=[100,100])\n",
    "axs[1].scatter(my, mx, c=['r', 'g'], s=[100,100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General case: Given a function\n",
    "\n",
    "$$\n",
    "y = f(x)\n",
    "$$\n",
    "\n",
    "find the inverse\n",
    "\n",
    "$$\n",
    "x = f^{-1}(y)\n",
    "$$\n",
    "\n",
    "In general case the inverse is unknown and it can be found only numerically using computer algorithms for root finding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical note: Root finding\n",
    "\n",
    "Root finding: Given $y$ and a function $f$ find such $x$ that\n",
    "\n",
    "$$\n",
    "f(x) = y\n",
    "$$\n",
    "\n",
    "The value of $x$ that fulfills the equation is called root of this equation.\n",
    "\n",
    "Root finding routines usually seek for zeros of some function $F(x)$. It varies $x$ until finds such one that \n",
    "\n",
    "$$\n",
    "F(x) = 0\n",
    "$$\n",
    "\n",
    "To find the function inverse we take $y$, feed the root finding routine with \n",
    "\n",
    "$$\n",
    "F(x) = f(x) - y\n",
    "$$\n",
    "\n",
    "and compute the corresponding $x$.\n",
    "\n",
    "There are a lot of root finding routines. We will use `root_scalar` form the module `scipy.optimize`.\n",
    "\n",
    "To see how it works let us find such $x$ that $e^x=50$. \n",
    "\n",
    "We will use the simplest methods named \"bisection\". \n",
    "\n",
    "It requires preliminary root bracketing, i.e. finding an approximate range where the root is located.\n",
    "\n",
    "As one can see in the graph below the root is near $x=4$. The values 2 and 5 are good as the root brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "xx = np.linspace(1, 5, 100)\n",
    "yy = [np.exp(xi) for xi in xx]\n",
    "\n",
    "y_find = 50\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(xx, yy)\n",
    "ax.plot([1, 5], [y_find, y_find], 'k--')\n",
    "ax.set_title(r\"$y=e^x$\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can compute the root as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import root_scalar\n",
    "import numpy as np\n",
    "\n",
    "y = 50\n",
    "\n",
    "sol = root_scalar(lambda x: np.exp(x) - y, method='bisect', bracket=(2, 5))\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routine `root_scalar` returns an object that contains a lot of service information. \n",
    "\n",
    "The root is stored in its attribute `.root`. \n",
    "\n",
    "Let us test that the exponential of `sol.root` is indeed `y_find`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the root, must be equal to y_find\n",
    "print(y_find, np.exp(sol.root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical note: Lambda-functions\n",
    "\n",
    "Observe that above we pass an equation $(e^x-y)$ to the root finder as an anonymous $\\lambda$-function.\n",
    "\n",
    "$\\lambda$-function is a fast way to build a function right in place. Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plus_one(fun, x):\n",
    "    \"\"\"Takes a function fun and a variable x and returns fun(x) + 1\n",
    "    \"\"\"\n",
    "    return fun(x) + 1\n",
    "\n",
    "# Here lambda-function computes square of its argument\n",
    "y = plus_one(lambda x: x**2, 10)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a lambda function we write a keyword `lambda` followed by a comma separated list of its parameters. Then goes a colon  and a function body after it.\n",
    "\n",
    "Lambda-function can be assigned to a variable and then it is used as an ordinary function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes a sum of squares of its parameters\n",
    "newfun = lambda x, y: x**2 + y**2\n",
    "\n",
    "print(newfun(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse CDF for normal distribution\n",
    "\n",
    "Now we are ready to compute the inverse of CDF of a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import root_scalar\n",
    "\n",
    "def inverse_norm_cdf(p, mu, sig):\n",
    "    \"\"\"\n",
    "    Given the probability P find the corresponding upper threshold X below which \n",
    "    the normally distributed variable x falls with the probability P.\n",
    "    \n",
    "    If mu != 0 or sig != 1 we compute a solution for the standard CDF and then rescale it. \n",
    "    Otherwise we would need to rescale the bracketing values (-10, 10).\n",
    "    \"\"\"\n",
    "    if mu != 0 or sig != 1:\n",
    "        return mu + sig * inverse_norm_cdf(p, mu=0, sig=1)\n",
    "    \n",
    "    fun = lambda x: norm_cdf(x, mu=0, sig=1) - p\n",
    "    sol = root_scalar(fun, method='bisect', bracket=(-10, 10))\n",
    "    return sol.root\n",
    "\n",
    "# Simple test. We know that CDF(mu) = 0.5. Thus the inversed CDF of 0.5 must return mu\n",
    "mu_test, sig_test = 100, 5\n",
    "print(inverse_norm_cdf(0.5, mu=mu_test, sig=sig_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the graph of CDF of a normal distribution and its inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mu, sig = 0, 1\n",
    "\n",
    "xx = np.linspace(-10, 10, 100)\n",
    "cdf = [norm_cdf(xi, mu=mu, sig=sig) for xi in xx]\n",
    "inv_cdf = [inverse_norm_cdf(p, mu=mu, sig=sig) for p in cdf]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(xx, cdf)\n",
    "axs[0].set_title(\"CDF\")\n",
    "axs[0].set_xlabel(r\"$X$\")\n",
    "axs[0].set_ylabel(r\"$P(x<X)$\")\n",
    "\n",
    "axs[1].plot(cdf, inv_cdf)\n",
    "axs[1].set_title(\"inv CDF\")\n",
    "axs[1].set_xlabel(r\"$P(x<X)$\")\n",
    "axs[1].set_ylabel(r\"$X$\")\n",
    "\n",
    "for ax in axs.reshape(-1):\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability to fall in range via CDF\n",
    "\n",
    "Finally, we will need to compute a probability of a normal random variable to fall between the given bounds. \n",
    "\n",
    "Here is the illustration to remind how it is computed.\n",
    "\n",
    "![cdf_example3.svg](fig/cdf_example3.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_btw(lo, hi, mu, sig):\n",
    "    \"\"\"Probability for a normal random variable to fall between lo and hi.\n",
    "    \"\"\"\n",
    "    assert lo < hi\n",
    "    \n",
    "    # Probability for a normal random variable to fall below lo\n",
    "    p1 = norm_cdf(lo, mu, sig)\n",
    "    \n",
    "    # Probability for a normal random variable to fall below hi\n",
    "    p2 = norm_cdf(hi, mu, sig)\n",
    "    \n",
    "    return p2 - p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the coin tossing example: null hypothesis, alternative and significance\n",
    "\n",
    "Now we have done all preparation and are ready to perform the hypothesis testing.\n",
    "\n",
    "Let us recall that we want to check if a coin is unfair, $P\\neq 0.5$, by tossing it many times and counting a number of heads.\n",
    "\n",
    "We perform $N=1000$ tossing and $x$ is the number of heads appeared.\n",
    "\n",
    "In this case a null hypothesis $H_0$ is that $P=0.5$ (fair coin) and the alternative $H_1$ is that $P\\neq 0.5$ (unfair coin).\n",
    "\n",
    "We will accept the standard value of significance $\\alpha=0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most probable range for a fair coin\n",
    "\n",
    "First, we compute a range for $x$ where it falls with the probability $P_\\alpha=1-\\alpha=0.95$ if the coin is fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_fair = 0.5\n",
    "N = 1000\n",
    "\n",
    "# Admitted probability of a type 1 error\n",
    "alpha = 0.05\n",
    "\n",
    "# Probability for a range of values that indicates that H0 is true\n",
    "p_a = 1 - alpha\n",
    "\n",
    "# Tails - probabilty for ranges outside the area, corresponding p_a\n",
    "p_tail = (1 -  p_a) / 2\n",
    "\n",
    "mu_0, sig_0 = get_mu_sig(N=N, P=p_fair)\n",
    "\n",
    "# Random normal variable with (mu, sig) falls below X_lo with the probability p_tail\n",
    "X_lo = inverse_norm_cdf(p_tail, mu=mu_0, sig=sig_0)\n",
    "\n",
    "# Random normal variable with (mu, sig) is above X_hi with probability p_tail\n",
    "X_hi = inverse_norm_cdf(1-p_tail, mu=mu_0, sig=sig_0)\n",
    "print(f\"Test bounds to accept H0 ({round(X_lo)}, {round(X_hi)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below shows the PDF for a random number of heads $x$ appearing after $N=1000$ coin tossing. The filling in the middle indicates a range where $x$ falls with the probability $P_\\alpha=0.95$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "N = 1000\n",
    "P = 0.5\n",
    "\n",
    "mu, sig = get_mu_sig(N, P)\n",
    "\n",
    "x = np.linspace(mu - 100, mu + 100, 100)\n",
    "pdf = [norm_pdf(xi, mu, sig) for xi in x]\n",
    "\n",
    "x_a = np.linspace(X_lo, X_hi, 50)\n",
    "p_a = [norm_pdf(xi, mu, sig) for xi in x_a]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.fill_between(x_a, p_a, 0, color=\"C1\", label=f\"X_lo={round(X_lo)}, X_hi={round(X_hi)}\")\n",
    "ax.plot(x, pdf, \"C0\", label=f\"mu={mu:.2f}, sig={sig:.2f}\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$\\rho(x)$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The routine above allows to compute the range for any given significance $\\alpha$ and the corresponding $P_\\alpha=1-\\alpha$. \n",
    "\n",
    "For particular probability $P_\\alpha=0.95$ the range can be found in a simpler way. \n",
    "\n",
    "It is known that the 95% range is \n",
    "\n",
    "$$\n",
    "[\\mu-1.96\\sigma, \\mu+1.96\\sigma]\n",
    "$$\n",
    "\n",
    "It means that\n",
    "\n",
    "$$ \n",
    "P(x\\in[\\mu-1.96\\sigma, \\mu+1.96\\sigma]) = 0.95\n",
    "$$\n",
    "\n",
    "Let us check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lo1 = mu - 1.96 * sig\n",
    "X_hi1 = mu + 1.96 * sig\n",
    "\n",
    "print(f\"Computed  95% bounds ({round(X_lo)}, {round(X_hi)})\")\n",
    "print(f\"Estimated 95% bounds ({round(X_lo1)}, {round(X_hi1)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, given these bounds we can conduct our experiment with tossing a coin, and if the number of heads \n",
    "$x$ is outside the range we reject the null hypothesis concluding that the coin is unfair. \n",
    "\n",
    "The probability that actually this is an error (type 1 error, a fake discovery) is $\\alpha=0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type 2 error probability\n",
    "\n",
    "If $x$ is between `X_lo` and `X_hi` we do not reject the null hypothesis and accept that the coin is fair. What is the probability $\\beta$ that this is an error (type 2 error, a missed discovery)?\n",
    "\n",
    "To be honest, the developed approach is not very good for revealing this type of errors. \n",
    "\n",
    "This is because technically the coin becomes unfair even if $P$ deviates from $0.5$ very slightly, say at $P=0.500000001$.\n",
    "\n",
    "But we probably will not notice this small deviation performing a statistical experiment.\n",
    "\n",
    "Let us assume that the coin is considered unfair only if $P\\geq 0.55$ or $P\\leq 0.45$.\n",
    "\n",
    "The probability $\\beta$ of the type 2 error is the probability that the unfair coin will produce $x$ (number of heads after $N$ tossing) within the found range `X_lo`, `X_hi`.\n",
    "\n",
    "Let us compute this probability for $P=0.55$ and $0.45$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_unfair1 = 0.55\n",
    "mu_1, sig_1 = get_mu_sig(N=N, P=p_unfair1)\n",
    "\n",
    "p_unfair2 = 0.45\n",
    "mu_2, sig_2 = get_mu_sig(N=N, P=p_unfair2)\n",
    "\n",
    "# Probability that a normal random variable fall within the range X_lo, X_hi\n",
    "beta1 = norm_btw(X_lo, X_hi, mu_1, sig_1)\n",
    "beta2 = norm_btw(X_lo, X_hi, mu_2, sig_2)\n",
    "\n",
    "print(f\"Type 2 error probaility beta({p_unfair1})={beta1:.2f}, beta({p_unfair2})={beta2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that due to the symmetry both probabilities are identical. If they were different we would take the largest one.\n",
    "\n",
    "Let us remember that above we discussed that for $\\alpha=0.05$ the obtained $\\beta=0.1$ is an acceptable value.\n",
    "\n",
    "Below there is a graph of PDF for $x$ (number of heads) in a series of $N=1000$ tossing of an unfair coin with $P=0.55$. \n",
    "\n",
    "The filled region shows the range \\[`X_lo`, `X_hi`\\]. Its area  is $\\beta=0.11$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "N = 1000\n",
    "\n",
    "x = np.linspace(mu_1 - 100, mu_1 + 100, 100)\n",
    "pdf = [norm_pdf(xi, mu_1, sig_1) for xi in x]\n",
    "\n",
    "x_a = np.linspace(X_lo, X_hi, 50)\n",
    "p_a = [norm_pdf(xi, mu_1, sig_1) for xi in x_a]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.fill_between(x_a, p_a, 0, color=\"C1\", label=f\"X_lo={round(X_lo)}, X_hi={round(X_hi)}\")\n",
    "ax.plot(x, pdf, \"C0\", label=f\"mu={mu_1:.2f}, sig={sig_1:.2f}\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$\\rho(x)$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude: Type 2 error will occur if the unfair coin will produce $x$ inside the range \\[`X_lo`, `X_hi`\\]. \n",
    "\n",
    "In this case the unfair coin will be considered as a fair one.\n",
    "\n",
    "This probability is reasonably small, $\\beta=0.11$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The power\n",
    "\n",
    "Usually one more characteristic is used, a power of a test $1-\\beta$. \n",
    "\n",
    "The meaning of power is the probability to reject a null hypothesis, i.e., the probability to make a discovery.\n",
    "\n",
    "In our case we will reveal the unfair coin with the probability \n",
    "\n",
    "$$\n",
    "\\mathrm{Pw} = 0.89\n",
    "$$\n",
    "\n",
    "and it will be an error with the probability\n",
    "\n",
    "$$\n",
    "\\alpha=0.05\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning significance\n",
    "\n",
    "Notice that beta and power depends on the significance. \n",
    "\n",
    "When increasing by lowering $\\alpha$ (because we do not want to make a fake discovery) we increase $\\beta$ (i.e., magnify a chance to miss the discovery).\n",
    "\n",
    "The analysis is correct if $\\alpha$ and $\\beta$ do not differ much.\n",
    "\n",
    "Here is the computation for $\\alpha=0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha = 0.01\n",
    "N = 1000\n",
    "\n",
    "p_fair = 0.5\n",
    "p_unfair = 0.55\n",
    "\n",
    "mu_0, sig_0 = get_mu_sig(N=N, P=p_fair)\n",
    "mu_1, sig_1 = get_mu_sig(N=N, P=p_unfair)\n",
    "\n",
    "p_a = 1 - alpha\n",
    "p_tail = (1 -  p_a) / 2\n",
    "\n",
    "X_lo = inverse_norm_cdf(p_tail, mu=mu_0, sig=sig_0)\n",
    "X_hi = inverse_norm_cdf(1-p_tail, mu=mu_0, sig=sig_0)\n",
    "beta = norm_btw(X_lo, X_hi, mu_1, sig_1)\n",
    "Pw = 1 - beta\n",
    "\n",
    "print(f\"N={N}, X_lohi=({round(X_lo)}, {round(X_hi)}), alpha={alpha}, beta={beta}, Pw={Pw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that chasing for a higher significance results in unreasonably small power. \n",
    "\n",
    "Analysis with such parameters will be incorrect.\n",
    "\n",
    "Higher significance requires longer experimental series: The appropriate $\\beta$ and power for $\\alpha=0.01$ is obtained at $N=2000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha = 0.01\n",
    "N = 2000\n",
    "\n",
    "p_fair = 0.5\n",
    "p_unfair = 0.55\n",
    "\n",
    "mu_0, sig_0 = get_mu_sig(N=N, P=p_fair)\n",
    "mu_1, sig_1 = get_mu_sig(N=N, P=p_unfair)\n",
    "\n",
    "p_a = 1 - alpha\n",
    "p_tail = (1 -  p_a) / 2\n",
    "\n",
    "X_lo = inverse_norm_cdf(p_tail, mu=mu_0, sig=sig_0)\n",
    "X_hi = inverse_norm_cdf(1-p_tail, mu=mu_0, sig=sig_0)\n",
    "beta = norm_btw(X_lo, X_hi, mu_1, sig_1)\n",
    "Pw = 1 - beta\n",
    "\n",
    "print(f\"N={N}, X_lohi=({round(X_lo)}, {round(X_hi)}), alpha={alpha}, beta={beta}, Pw={Pw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The experiment itself with fair and unfair coins\n",
    "\n",
    "Let us now model the discussed experiment explicitly and see how often errors will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Coin:\n",
    "    \"\"\" Model of a coin tossing. P is the probability of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, P):\n",
    "        self.P = P\n",
    "        self.rng = np.random.default_rng()\n",
    "        \n",
    "    def toss(self):\n",
    "        return 1 if self.rng.random() < self.P else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An experiment with a fair coin\n",
    "\n",
    "N = 1000\n",
    "X_lo, X_hi = 469, 531 # The H0 range for alpha=0.05, N=1000\n",
    "\n",
    "# The coin is fair\n",
    "coin = Coin(0.5)\n",
    "seq = [coin.toss() for _ in range(N)]\n",
    "x = sum(seq)\n",
    "\n",
    "if X_lo <= x <= X_hi:\n",
    "    print(f\"x={x} within the range ({X_lo}, {X_hi}), accept that the coin is fair, no error\")\n",
    "else:\n",
    "    print(f\"x={x} outside the range ({X_lo}, {X_hi}), accept that coin is unfair, type 1 error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An experiment with an unfair coin\n",
    "\n",
    "N = 1000\n",
    "X_lo, X_hi = 469, 531 # The H0 range for alpha=0.05, N=1000\n",
    "\n",
    "# The coin is unfair\n",
    "coin = Coin(0.55)\n",
    "seq = [coin.toss() for _ in range(N)]\n",
    "x = sum(seq)\n",
    "\n",
    "if X_lo <= x <= X_hi:\n",
    "    print(f\"x={x} within the range ({X_lo}, {X_hi}), accept that the coin is fair, type 2 error\")\n",
    "else:\n",
    "    print(f\"x={x} outside the range ({X_lo}, {X_hi}), accept that coin is unfair, no error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $p$-values\n",
    "\n",
    "An alternative way of doing the analysis above is to compute a $p$-value for an obtained experimental result.\n",
    "\n",
    "- Assuming that $H_0$ is true and the corresponding distribution of experimental results is known we accept some significance level $\\alpha$. Usually $\\alpha=0.05$.\n",
    "\n",
    "- Given an experimental result $x$ we compute the probability $P$ to see a value at least as extreme as $x$. \n",
    "\n",
    "- The extreme means that it deviates from the center of the distribution more then $x$. \n",
    "\n",
    "- For a one-sided test (e.g., weather a coin is head biased) this probability is $p$-value, $p=P$.\n",
    "\n",
    "- For two sides test (weather a coin is unfair) $p$-value is doubled this probability, $p = 2 P$.\n",
    "\n",
    "- If $p<\\alpha$ the $H_0$ is rejected.\n",
    "\n",
    "To illustrate it, let us consider again a test if a coin is unfair. \n",
    "\n",
    "The null hypothesis $H_0$ is that the coin is fair. \n",
    "\n",
    "The experiment is tossing the coin $N$ times and the result is  a number of heads $x$.\n",
    "\n",
    "For this experiment the distribution of results $x$ is known: For large $N$ this is normal distribution.\n",
    "\n",
    "Let the significance level be $\\alpha=0.05$.\n",
    "\n",
    "We conduct the experiment, compute $p$-value and reject $H_0$ if $p<\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "\n",
    "def get_mu_sig(N, P):\n",
    "    \"\"\"\n",
    "    mu and sig corresponding to a binomial distribution\"\"\"\n",
    "    mu = P * N\n",
    "    sig = np.sqrt(N * P * (1 - P))\n",
    "    return mu, sig\n",
    "\n",
    "def norm_cdf(x, mu, sig):\n",
    "    \"\"\"Normal cumulative distribution function\"\"\"\n",
    "    return 0.5 * (1 + erf((x-mu)/(sig*np.sqrt(2))))\n",
    "\n",
    "def get_pvalue_twosided(x, mu0, sig0):\n",
    "    if x <= mu0:\n",
    "        return 2 * norm_cdf(x, mu0, sig0)\n",
    "    else:\n",
    "        return 2 * (1 - norm_cdf(x, mu0, sig0))\n",
    "\n",
    "N = 1000\n",
    "P = 0.5\n",
    "alpha = 0.05\n",
    "\n",
    "# mean and standard deviation for a fair coin\n",
    "mu0, sig0 = get_mu_sig(N, P)\n",
    "\n",
    "# Let the result of the experiment be\n",
    "x = 530\n",
    "p = get_pvalue_twosided(x, mu0, sig0)\n",
    "print(f'x={x}, p-value={p}, alpha={alpha}, {\"reject H0\" if p < alpha else \"H0 is true\"}')\n",
    "\n",
    "# Let the result of the experiment be\n",
    "x = 531\n",
    "p = get_pvalue_twosided(x, mu0, sig0)\n",
    "print(f'x={x}, p-value={p}, alpha={alpha}, {\"reject H0\" if p < alpha else \"H0 is true\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact we do the same think as above. \n",
    "\n",
    "But now we do not compute the bounds `X_lo` and `X_hi` in advance.\n",
    "\n",
    "Instead we compute $p$-value for the experiment result $x$ and test if $H_0$ can be rejected comparing the $p$-value with the significance $\\alpha$.\n",
    "\n",
    "Since the acceptable level of the significance is commonly known often just a $p$-value of the discovered feature is reported. \n",
    "\n",
    "Once again: $p$-value is the probability to see $x$ and more extreme values provided that $H_0$ is true. \n",
    "\n",
    "If this probability is too small we can conclude that this could not happen at all so that $H_0$ is not true and have to be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval\n",
    "\n",
    "Another way of treating the result of the stochastic experiment is computing a confidence interval. \n",
    "\n",
    "Assume again that we have a coin and want to know if it is unfair. \n",
    "\n",
    "We have performed a series of $N=1000$ tossing and get $x$ heads. \n",
    "\n",
    "From this result we can estimate the probability of this coin as \n",
    "\n",
    "$$\n",
    "\\hat P=x/N\n",
    "$$\n",
    "\n",
    "If $N\\to \\infty$ our estimate $\\hat P$ approaches true probability of the coin $P$.\n",
    "\n",
    "But how $\\hat P$ deviates from $P$ for a finite $N$? \n",
    "\n",
    "The answer is given by a confidential interval.\n",
    "\n",
    "The confidence interval is the interval around the experimental value $\\hat P$ such that the true value $P$ belongs to it with  the probability $1-\\alpha$, where $\\alpha$ is as usual the significance. Often $1-\\alpha=0.95$.\n",
    "\n",
    "We need to estimate the distance between true value $P$ and its  approximation $\\hat P$. \n",
    "\n",
    "When the series length $N$ is large, say $N>100$, it can be as follows: \n",
    "\n",
    "First we need to know the width of the distribution of $\\hat P$. \n",
    "\n",
    "Let us remember that if $z$ is the standard normal distribution ($\\mu=0$, $\\sigma=1$) then $x$ is distributed with $\\mu$ and $\\sigma$ if\n",
    "\n",
    "$$\n",
    "x = \\sigma z + \\mu\n",
    "$$\n",
    "\n",
    "Divide it by $N$:\n",
    "\n",
    "$$\n",
    "x/N = (\\sigma z + \\mu) / N\n",
    "$$\n",
    "\n",
    "Since $\\hat P = x/N$ we have:\n",
    "\n",
    "$$\n",
    "\\hat P = \\left(\\frac{\\sigma}{N}\\right) z + \n",
    "\\left(\\frac{\\mu}{N}\\right)\n",
    "$$\n",
    "\n",
    "So, if the standard deviation of $x$ is $\\sigma$ then the standard deviation for $\\hat P$ is $\\sigma/N$.\n",
    "\n",
    "Also if $x$ has a mean $\\mu$ then the mean for $\\hat P$ is $\\mu/N$.\n",
    "\n",
    "Let us now remember that if $x$ is the number of heads of a coin with the probability $P$ the standard deviation is\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{N P (1-P)}, \\mu=P N\n",
    "$$\n",
    "\n",
    "But we do not know $P$ and use $\\hat P$ instead (do not forget that we know $\\hat P$ since this is the value that we have obtained in the experiment).\n",
    "\n",
    "So instead we take \n",
    "\n",
    "$$\n",
    "\\sigma' = \\sqrt{N \\hat P (1- \\hat P)}, \\mu=\\hat P N\n",
    "$$ \n",
    "\n",
    "Finally we obtain an estimate for the unknown standard deviation for $\\hat P$:\n",
    "\n",
    "$$\n",
    "\\hat \\sigma = \\frac{\\sigma'}{N}\n",
    "=\\sqrt{\\hat P (1- \\hat P)/N}, \\hat \\mu = \\hat P\n",
    "$$\n",
    "\n",
    "This is where we employ the assumption that $N$ is large. For small $N$ we cannot take $\\hat \\sigma$ as an approximate estimate of an known distribution width and must use another approach.\n",
    "\n",
    "Now the confidential interval is found as the $(1-\\alpha)$ probability range, e.g., 95% range, for the normal distribution with $\\mu=\\hat \\mu$ and $\\sigma = \\hat\\sigma$.\n",
    "\n",
    "For $\\alpha=0.05$ this is 95% range that can be found as\n",
    "\n",
    "$$\n",
    "[\\hat \\mu - 1.96 \\hat \\sigma, \\hat \\mu + 1.96 \\hat \\sigma]\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\left[\\hat P - 1.96 \\sqrt{\\hat P (1- \\hat P)/N},\n",
    "\\hat P + 1.96 \\sqrt{\\hat P (1- \\hat P)/N}\\right]\n",
    "$$\n",
    "\n",
    "Often the deviations $\\pm 1.96 \\hat \\sigma$ are treated as an error range of the experimental result. We can say that true probability $P$ is obtained as\n",
    "\n",
    "$$\n",
    "P = \\hat P \\pm 1.96 \\hat \\sigma\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us do some checks. \n",
    "\n",
    "Below is the function that computes the confidential interval according to the formula above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conf_interv_95(N, P):\n",
    "    \"\"\"Compute confidential interval.\n",
    "    \"\"\"\n",
    "    return 1.96 * np.sqrt((P * (1 - P)) / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume some results and compute the confidential interval for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our result: number of heads after N tossing\n",
    "x = 525\n",
    "\n",
    "N = 1000\n",
    "\n",
    "P_hat = x / N\n",
    "ci = conf_interv_95(N, P_hat)\n",
    "print(f'Estimated probability is P={P_hat}±{ci:6.4f}')\n",
    "print(f'Confidential interval is [{(P_hat - ci):6.4f} - {(P_hat + ci):6.4f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of all this computations is as follows:\n",
    "\n",
    "We have conduct an experiment that includes $N=1000$ tossing of a coin whose probability $P$ is unknown. \n",
    "\n",
    "The result is $x=525$ of heads and thus the probability is estimated as $\\hat P = 0.525$.\n",
    "\n",
    "The error of this measurements is $\\pm 0.0310$.\n",
    "\n",
    "The confidential interval, i.e., a range where the true value $P$ falls with the 95% probability is $[0.4940, 0.5560]$.\n",
    "\n",
    "In particular it means that the coin still can be fair, because $P=0.5$ belongs to this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coin stil can be fair\n",
    "x = 530\n",
    "\n",
    "N = 1000\n",
    "\n",
    "P_hat = x / N\n",
    "ci = conf_interv_95(N, P_hat)\n",
    "print(f'Estimated probability is P={P_hat}±{ci:6.4f}')\n",
    "print(f'Confidential interval is [{(P_hat - ci):6.4f} - {(P_hat + ci):6.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we conclude that the coin is unfair\n",
    "x = 531\n",
    "\n",
    "N = 1000\n",
    "\n",
    "P_hat = x / N\n",
    "ci = conf_interv_95(N, P_hat)\n",
    "print(f'Estimated probability is P={P_hat}±{ci:6.4f}')\n",
    "print(f'Confidential interval is [{(P_hat - ci):6.4f} - {(P_hat + ci):6.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this value is obtained, we again conclude that the coin is unfair\n",
    "x = 469\n",
    "\n",
    "N = 1000\n",
    "\n",
    "P_hat = x / N\n",
    "ci = conf_interv_95(N, P_hat)\n",
    "print(f'Estimated probability is P={P_hat}±{ci:6.4f}')\n",
    "print(f'Confidential interval is [{(P_hat - ci):6.4f} - {(P_hat + ci):6.4f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two value above for which the coin can be treated as unfair are 469 and 531. \n",
    "\n",
    "Observe that this is exactly the values that we have obtained above performing the hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/B test\n",
    "\n",
    "In electronic commerce a conversion rate is a portion of site visitors who did some activity. \n",
    "\n",
    "For example, during a week there was $N$ visitors and $n$ of them have bought something. \n",
    "\n",
    "Or among $N$ visitors $n$ have clicked an advertisement.\n",
    "\n",
    "Thus the conversion rate is\n",
    "\n",
    "$$\n",
    "c = n / N\n",
    "$$\n",
    "\n",
    "Assume that we have a web site and its design has changed. \n",
    "\n",
    "The question is if the new design brings more customers.\n",
    "\n",
    "Let us denote the old design as A and the new one as B. \n",
    "\n",
    "The A/B test is about testing of the user preferences. \n",
    "\n",
    "To perform this test we start showing two versions of the website at random: \n",
    "some sees A and others obtain B.\n",
    "\n",
    "After a while we count $N_A$ visits of the website A and $N_B$ visitors of B.\n",
    "\n",
    "Among those who get A there are $n_A$ visitors who did something useful, and for B-visitors \n",
    "this number is $n_B$.\n",
    "\n",
    "The conversion rates for two version of the website are\n",
    "\n",
    "$$\n",
    "c_A = n_A / N_A, \n",
    "c_B = n_B / N_B\n",
    "$$\n",
    "\n",
    "These conversion rates are random variables. In fact we have only single values for them, but if \n",
    "we repeat the measurements many times we will obtain different $c_A$ and $c_B$. \n",
    "\n",
    "If $N_A$ and $N_B$ are large it is reasonable to treat the conversion rates as having normal distributions.\n",
    "\n",
    "Also we assume that $c_A$ and $c_B$ are independent. This is indeed the case if each visitor has \n",
    "access to the only one version of the website.\n",
    "\n",
    "As we already discussed above mean and standard deviations for $c_A$ and $c_B$ can be estimated as\n",
    "\n",
    "$$\n",
    "\\mu_A = c_A, \\sigma_A = \\sqrt{c_A(1-c_A)/N_A}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_B = c_B, \\sigma_B = \\sqrt{c_B(1-c_B)/N_B}\n",
    "$$\n",
    "\n",
    "The null hypothesis for our analysis is that $\\mu_A=\\mu_B$, i.e., for infinitely many visitors $c_B = c_A$ and there is no difference between two versions of the website.\n",
    "\n",
    "Consider a variable \n",
    "\n",
    "$$\n",
    "c_{AB} = c_B - c_A\n",
    "$$\n",
    "\n",
    "Again, we have only one value of it. But repeating the measurements many times we will obtain different \n",
    "values. \n",
    "\n",
    "It can be derived that $c_{AB}$ is also normally distributed with the mean and the standard deviation\n",
    "\n",
    "$$\n",
    "\\mu_{AB} = \\mu_B - \\mu_A, \\sigma_{AB} = \\sqrt{\\sigma_A^2 + \\sigma_B^2}\n",
    "$$\n",
    "\n",
    "According to our null hypothesis, $\\mu_{AB}=0$.\n",
    "\n",
    "To test it we compute $p$-value for $c_{AB}$.\n",
    "\n",
    "Let us remember that it means that provided that the null hypothesis is true we need to compute\n",
    "the probability to see a value at least as extreme as $c_{AB}$.\n",
    "\n",
    "If $c_{AB}>0$ we compute it as follows:\n",
    "\n",
    "$$\n",
    "p = 2(1 - \\phi(c_{AB}, \\mu_{AB}=0, \\sigma_{AB}))\n",
    "$$\n",
    "\n",
    "and for $c_{AB}<0$ we do\n",
    "\n",
    "$$\n",
    "p = 2\\phi(c_{AB}, \\mu_{AB}=0, \\sigma_{AB})\n",
    "$$\n",
    "\n",
    "where $\\phi(\\cdot)$ is CDF (cumulative distribution function) for a normal distribution that corresponds to our null \n",
    "hypothesis.\n",
    "\n",
    "Notice that we multiply the probability by 2. This is because we perform a two sided test: Deviation to both sided are meaningful. \n",
    "\n",
    "If the $p$-value is below some accepted significance $\\alpha$ level we conclude that the new website results in \n",
    "the change of the conversion ratio, i.e., $H_0$ is rejected.\n",
    "\n",
    "There are two cases: If $c_{AB}>0$ the new website works better then the old one, and for $c_{AB}<0$ we conclude that the new website make things worse.\n",
    "\n",
    "Let us consider a particular example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "\n",
    "# The website A\n",
    "Na = 1000\n",
    "na = 200\n",
    "\n",
    "# The website B\n",
    "Nb = 1200\n",
    "nb = 290\n",
    "\n",
    "# Convertion ratios\n",
    "ca = na / Na\n",
    "cb = nb / Nb\n",
    "\n",
    "cab = cb - ca\n",
    "\n",
    "def sigma_test(N, c):\n",
    "    \"\"\"Standard deviation for c\"\"\"\n",
    "    return np.sqrt((c * (1 - c)) / N)\n",
    "\n",
    "def norm_cdf(x, mu, sig):\n",
    "    \"\"\"Normal cumulative distribution function\"\"\"\n",
    "    return 0.5 * (1 + erf((x-mu)/(sig*np.sqrt(2))))\n",
    "\n",
    "sig_a = sigma_test(Na, ca)\n",
    "sig_b = sigma_test(Nb, cb)\n",
    "sig_ab = np.sqrt(sig_a**2 + sig_b**2)\n",
    "mu_ab = 0  # this accirding to our null hypothesis\n",
    "\n",
    "p = 2*norm_cdf(cab, mu=mu_ab, sig=sig_ab) if cab < 0 else 2*(1 - norm_cdf(cab, mu=mu_ab, sig=sig_ab))\n",
    "\n",
    "print(f\"ca={ca:6.4f}, cb={cb:6.4f}, cab={cab:6.4f}, p-value={p:6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example $c_b$ is higher then $c_a$, but the difference is not very large.\n",
    "\n",
    "Do not forget that the values are volatile, so that we can not trust just to a mere positive difference.\n",
    "\n",
    "But the $p$-value is $\\approx 0.02$. \n",
    "\n",
    "It means that we have to accept the new design since the significance $(1-p)$ of the hypothesis that it is better is very high. Even higher than usual level $0.05$.\n",
    "\n",
    "Let us remember: this $p$ value means that the probability that the new design has produced higher conversion ration by chance is $0.02$, that is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference\n",
    "\n",
    "Let us remember Bayes' theorem.\n",
    "\n",
    "First we know that the probability to meet a person with a certain disease $D$ is $P(D)$. This is the prior probability.\n",
    "\n",
    "Then we get some test system that mines new data. It can be said that it provides an evidence for $D$.\n",
    "\n",
    "The test system operates as follows: It gives a positive result $T$ for a really diseased person with the probability\n",
    "$P(T|D)$ and erroneously detects the disease for a healthy person with the probability $P(T|\\bar D)$.\n",
    "\n",
    "Using the new data produced by this test system we can refine the prior probability using Bayes' theorem:\n",
    "\n",
    "The probability to meet a diseased person provided that he or she already have a positive test is\n",
    "\n",
    "$$\n",
    "P(D|T) = \\frac{P(T|D) P(D)}{P(T|D) P(D) + P(T|\\bar D) P(\\bar D)}\n",
    "$$\n",
    "\n",
    "This probability is the posterior probability after taking the evidence $T$ into account\n",
    "\n",
    "$P(T|D)$ is called a likelihood function. This is the probability of the evidence $T$ provided that $D$ is true.\n",
    "\n",
    "Thus the Bayes' theorem refines somehow known prior probability using a new evidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' theorem can be used in statistical inference exactly as shown above.\n",
    "\n",
    "First we have some prior estimates of a probability distribution. This can be even intuitive guesses or expert assessments.\n",
    "\n",
    "Then we find a new information and refine the prior probabilities via Bayes' theorem.\n",
    "\n",
    "Using Bayes' theorem for continuous variables is rather complicated mathematical problem: \n",
    "\n",
    "Given a prior probability distribution function we have to find a posterior distribution. \n",
    "\n",
    "But if a prior distribution function is prior conjugate to a likelihood distribution the posterior distribution function is \n",
    "the same as the prior one but with another parameters.\n",
    "\n",
    "Many prior conjugate pairs of distributions are known.\n",
    "\n",
    "The experimental results, i.e., the evidences considered in the examples above (number of heads in tossing of a coin and number of consumers among all website visitors) have binomial distribution. The conjugate pair for it is so called beta distribution.\n",
    "\n",
    "Before proceeding we will consider what is it.\n",
    "\n",
    "Beta distribution is\n",
    "\n",
    "$$\n",
    "\\rho(x) = \\frac{ x^{a-1} (1-x)^{b-1} }{B(a,b)}\n",
    "$$\n",
    "\n",
    "where $B(a,b)$ is a beta function:\n",
    "\n",
    "$$\n",
    "B(a,b)=\\int_0^1 x^{a-1}(1-x)^{b-1} dx\n",
    "$$\n",
    "\n",
    "In fact we will not use the former formula to compute $B(a,b)$. The module `scipy.special` provides a function `beta` for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import beta\n",
    "import numpy as np\n",
    "\n",
    "def beta_pdf(x, a, b):\n",
    "    if x <= 0 or x >= 1:\n",
    "        return 0\n",
    "    return x**(a - 1) * (1 - x)**(b - 1) / beta(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all notice that this distribution is located in the unit interval and is zero everywhere outside it.\n",
    "\n",
    "There are two parameters, $a$ and $b$. \n",
    "\n",
    "For $a=b=1$ it is just the uniform distribution. \n",
    "\n",
    "When $a=b>1$ the curve is symmetric with respect to $1/2$ and has a hump-like shape. \n",
    "\n",
    "The higher values the more pointy and narrow is the curve. \n",
    "\n",
    "At $a<b$ the curve is shifted toward 0 and for $a>b$ all probabilities are near 1.\n",
    "\n",
    "The mean value for this distribution is\n",
    "\n",
    "$$\n",
    "\\mu=\\frac{a}{a+b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = np.linspace(0, 1, 100)\n",
    "y_1_1 = [beta_pdf(x, 1, 1) for x in xx]\n",
    "y_10_10 = [beta_pdf(x, 10, 10) for x in xx]\n",
    "y_30_30 = [beta_pdf(x, 30, 30) for x in xx]\n",
    "y_5_10 = [beta_pdf(x, 5, 10) for x in xx]\n",
    "y_20_5 = [beta_pdf(x, 20, 5) for x in xx]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xx, y_1_1, label='a=1, b=1')\n",
    "ax.plot(xx, y_10_10, label='a=10, b=10')\n",
    "ax.plot(xx, y_30_30, label='a=30, b=30')\n",
    "ax.plot(xx, y_5_10, label='a=5, b=10')\n",
    "ax.plot(xx, y_20_5, label='a=20, b=5')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned above the beta distribution is conjugate prior to the binomial distribution. \n",
    "\n",
    "It means that if a priory distribution is beta we can refine it using an experiment describing by the binomial distribution.\n",
    "\n",
    "It works as follows. \n",
    "\n",
    "First we choose $a$ and $b$ to fit the priory probability $P$ of a considered Bernoulli variable, e.g., a coin.\n",
    "\n",
    "$$\n",
    "P = \\frac{a}{a+b}\n",
    "$$\n",
    "\n",
    "If there are no guess about $P$ at all we set $a=b=1$ that corresponds to the uniform distribution. \n",
    "\n",
    "If there are reasons to expect that $P\\approx 0.5$ we set $a=b>1$. \n",
    "\n",
    "Larger values of $a$ and $b$ correspond to higher confidence in this guess since for larger $a$ and $b$ the curve becomes more concentrated near $0.5$.\n",
    "\n",
    "For other guesses of $P$ we can also find $a$ and $b$ in a similar way: the mean value of the distribution corresponds to the guess and its width reflects our confidence in it.\n",
    "\n",
    "Then we perform an experiment, say toss a coin. Let as usual there $N$ tossing and $x$ heads. \n",
    "\n",
    "According to the Bayes' theorem the posterior distribution, i.e., the one refined after new obtaining the new information will also be beta with \n",
    "\n",
    "$$\n",
    "a'=a+x, b'=b+(N-x)\n",
    "$$\n",
    "\n",
    "The mathematical details are too complicated to be mentioned here.\n",
    "\n",
    "Thus a refined estimate of $P$ is\n",
    "\n",
    "$$\n",
    "P' = \\frac{a+x}{a+b+N}\n",
    "$$\n",
    "\n",
    "Below is an example of computations using the above formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_infer(a, b, N, x):\n",
    "    \"\"\"Takes initial a and be and computes the refined a1 and b1.\n",
    "    Returns priori and posterir P.\n",
    "    \"\"\"\n",
    "    P = a / (a + b)\n",
    "    a1 = a + x\n",
    "    b1 = b + (N - x)\n",
    "    P1 = a1 / (a1 + b1)\n",
    "    return P, P1\n",
    "\n",
    "a = 1\n",
    "b = 1\n",
    "N = 1000\n",
    "x = 530\n",
    "\n",
    "P, P1 = bayes_infer(a, b, N, x)\n",
    "print(f\"prior P={P:6.4f}, posterior P1={P1:6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of using of Bayesian inference\n",
    "\n",
    "Consider a more realistic example. \n",
    "\n",
    "Assume that there is a website and previously we have computed its conversion ratio as\n",
    "\n",
    "$$\n",
    "c = 0.11\n",
    "$$\n",
    "\n",
    "Now we have changed its design and want to refine the conversion ratio. \n",
    "\n",
    "The initial guess is obviously our previous conversion rate. \n",
    "\n",
    "We need to find such $a$ and $b$ that\n",
    "\n",
    "$$\n",
    "c = \\frac{a}{a+b}\n",
    "$$\n",
    "\n",
    "There is one equation for two unknown variables, $a$ and $b$. \n",
    "\n",
    "Solve it for $b$:\n",
    "\n",
    "$$\n",
    "b = a \\frac{1-c}{c}\n",
    "$$\n",
    "\n",
    "We have one free variable $a$: choosing it we compute the corresponding $b$.\n",
    "\n",
    "Let us see how the distribution depends on $a$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "c = 0.11\n",
    "\n",
    "xx = np.linspace(0, 0.3, 100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for a in [1,2,5]:  # possible values of a\n",
    "    b = a * (1-c)/c  # corresponding b\n",
    "    yy = [beta_pdf(x, a, b) for x in xx]\n",
    "    ax.plot(xx, yy, label=f'a={a:6.4f}, b={b:6.4f}')\n",
    "    \n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The width of the curve reflects to our expectations.\n",
    "\n",
    "The higher $a$ results the narrower curve. It corresponds to the lower uncertainty in our expectations: we expect that the new design will have rather similar performance as the old one and $c$ will be changed slightly.\n",
    "\n",
    "But if we expect that the new design will result in the essential corrections, we have to keep $a$ as low as possible. \n",
    "\n",
    "Let us be optimistic  and expect a serious correction. \n",
    "\n",
    "At $a=1$ the curve looks inappropriately, so we set\n",
    "\n",
    "$$\n",
    "a = 2, b = 16.1818\n",
    "$$\n",
    "\n",
    "Now we start the test: count the total number of visitors $N$ and those who did something useful, e.g., bought something. The former will be denoted as $x$.\n",
    "\n",
    "Let us assume that after some time we have\n",
    "$$\n",
    "N = 100, x = 17\n",
    "$$\n",
    "\n",
    "Using the Python function defined above we refine the conversion ratio as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.11\n",
    "a = 2\n",
    "b = a * (1 - c) / c\n",
    "\n",
    "N = 100\n",
    "x = 17\n",
    "\n",
    "_, c1 = bayes_infer(a, b, N, x)\n",
    "\n",
    "print(f\"prior c={c:6.4f}, posterior c1={c1:6.4f}, plain mean x/N={x/N:6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated conversion ratio is\n",
    "\n",
    "$$\n",
    "c'=0.1608\n",
    "$$\n",
    "\n",
    "It indicates that the improvements have occurred indeed.\n",
    "\n",
    "Notice that the new value $c'$ differs from the experimental mean value \n",
    "\n",
    "$$\n",
    "x/N = 0.17\n",
    "$$\n",
    "\n",
    "Here is the plots of the prior and posterior distributions. \n",
    "\n",
    "Observe that the posterior one is narrower: It reflects that fact that after obtaining a new information the uncertainty is decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "c = 0.11\n",
    "a = 2\n",
    "b = a * (1 - c) / c\n",
    "\n",
    "N = 100\n",
    "x = 17\n",
    "\n",
    "a1 = a + x\n",
    "b1 = b + (N - x)\n",
    "\n",
    "xx = np.linspace(0, 1, 500)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y0 = [beta_pdf(x, a, b) for x in xx]\n",
    "y1 = [beta_pdf(x, a1, b1) for x in xx]\n",
    "    \n",
    "ax.plot(xx, y0, label=f'a={a:6.4f}, b={b:6.4f}')\n",
    "ax.plot(xx, y1, label=f'a1={a1:6.4f}, b1={b1:6.4f}');\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis makes sense only if we have a small amount of data.\n",
    "\n",
    "Above there were $N=100$ visitors.\n",
    "\n",
    "Let us compute $c'$ if $N=1000$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.11\n",
    "a = 2\n",
    "b = a * (1 - c) / c\n",
    "\n",
    "# incresed by a factor of 10\n",
    "N = 1000\n",
    "x = 170\n",
    "\n",
    "_, c1 = bayes_infer(a, b, N, x)\n",
    "\n",
    "print(f\"prior c={c:6.4f}, posterior c1={c1:6.4f}, plain mean x/N={x/N:6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that now $c'$ is very close to the experimental mean $x/N$.\n",
    "\n",
    "Below are the corresponding plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "c = 0.11\n",
    "a = 2\n",
    "b = a * (1 - c) / c\n",
    "\n",
    "# incresed by a factor of 10\n",
    "N = 1000\n",
    "x = 170\n",
    "\n",
    "a1 = a + x\n",
    "b1 = b + (N - x)\n",
    "\n",
    "xx = np.linspace(0, 1, 500)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y0 = [beta_pdf(x, a, b) for x in xx]\n",
    "y1 = [beta_pdf(x, a1, b1) for x in xx]\n",
    "    \n",
    "ax.plot(xx, y0, label=f'a={a:6.4f}, b={b:6.4f}')\n",
    "ax.plot(xx, y1, label=f'a1={a1:6.4f}, b1={b1:6.4f}');\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is very pointy. It tells us that we must be pretty sure that the new conversion ration is $x/N=0.17$.\n",
    "\n",
    "Thus we conclude that when a lot of experimental data is available the Bayesian inference is not so useful since its result mere coincides with the experimental mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square distribution\n",
    "\n",
    "Let us remember that many natural processes can be modeled as stochastic ones with a normal distribution. \n",
    "\n",
    "This due to the central limit theorem that says that if many independent factors influence an observable \n",
    "then their impacts add up so that the observable demonstrates a random behavior with a normal distribution.\n",
    "\n",
    "The PDF (probability density function) for the normal distribution is as follows \n",
    "\n",
    "$$\n",
    "\\rho(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\mathrm{exp}\n",
    "\\left(\n",
    "-\\frac{(x-\\mu)^2}{2\\sigma^2}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "When $\\mu=0$ and $\\sigma=1$ the distribution is called the standard normal distribution.\n",
    "\n",
    "The plot of this distribution is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def norm_pdf(x, mu, sig):\n",
    "    \"\"\"Normal probability density function\"\"\"\n",
    "    return np.exp(-(x-mu)**2 / (2*sig*sig)) / (sig * np.sqrt(2*np.pi))\n",
    "\n",
    "mu, sig = 0, 1\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = [norm_pdf(xi, mu, sig) for xi in x]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y, label=f\"mu={mu}, sig={sig}\")\n",
    "\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$\\rho$')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum and difference of two random normal variables have also the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us remember what is the distance. Assume that we have two vectors. \n",
    "\n",
    "$$\n",
    "\\vec x = (x_1, x_2, x_3, x_4, x_5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec y = (y_1, y_2, y_3, y_4, y_5)\n",
    "$$\n",
    "\n",
    "In fact these are just two sets of numbers.\n",
    "\n",
    "If we want to compare how different they are we need a distance between them. \n",
    "\n",
    "Most often the Euclidean distance is used:\n",
    "\n",
    "$$\n",
    "d = \\sqrt{\\sum_{i=1}^{k} (x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "where $k$ is the number of vector components. For two vectors above $k=5$.\n",
    "\n",
    "Often comparing two vectors the square root is omitted. The meaning of the result is not changed, but computations are simplified.\n",
    "\n",
    "$$\n",
    "d^2 = \\sum_{i=1}^{k} (x_i - y_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assume that we have two random vectors with $k$ elements and the elements are random normal variables.\n",
    "\n",
    "$$\n",
    "\\vec x = (x_1, x_2, x_3, x_4, x_5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec y = (y_1, y_2, y_3, y_4, y_5)\n",
    "$$\n",
    "\n",
    "Often we need to answer the question: Do elements of these two vectors belong to the same probability distribution or not?\n",
    "\n",
    "The answer is given with the help of $\\chi^2$-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $k$ independent random numbers that have standard normal distribution, i.e., $\\mu=0$, $\\sigma=1$:\n",
    "\n",
    "$$\n",
    "z_1, z_2, z_3, z_4, z_5\n",
    "$$\n",
    "\n",
    "The distribution of the sum of their squares\n",
    "\n",
    "$$\n",
    "d^2 = \\sum_{i=1}^k z_i^2\n",
    "$$\n",
    "\n",
    "is called $\\chi^2$-distribution. This distribution has one parameter $k$ that is a number of $z$'s. This $k$ is called a number of degrees of freedom.\n",
    "\n",
    "PDF for $\\chi^2$-distributions is given by a formula:\n",
    "\n",
    "$$\n",
    "\\rho(x) = \\frac{(1/2)^{\\frac{k}{2}}}{\\Gamma\\left(\\frac{k}{2}\\right)} x^{\\frac{k}{2} - 1} e^{-\\frac{x}{2}}\n",
    "$$\n",
    "\n",
    "where $\\Gamma(\\cdot)$ is gamma function. We will compute if using function `gamma` from `scipy.special`.\n",
    "\n",
    "Examples of the $\\chi^2$ PDF for different $k$ are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "import numpy as np\n",
    "\n",
    "def chi2_pdf(x, k):\n",
    "    \"\"\"Chi-square PDF\"\"\"\n",
    "    return (1/2)**(k/2) * x**(k/2-1) * np.exp(-x/2) / gamma(k/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = np.linspace(1e-5, 10, 100)\n",
    "y_1 = [chi2_pdf(x, 1) for x in xx]\n",
    "y_2 = [chi2_pdf(x, 2) for x in xx]\n",
    "y_3 = [chi2_pdf(x, 3) for x in xx]\n",
    "y_4 = [chi2_pdf(x, 4) for x in xx]\n",
    "y_5 = [chi2_pdf(x, 5) for x in xx]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xx, y_1, label='k=1')\n",
    "ax.plot(xx, y_2, label='k=2')\n",
    "ax.plot(xx, y_3, label='k=3')\n",
    "ax.plot(xx, y_4, label='k=4')\n",
    "ax.plot(xx, y_5, label='k=5')\n",
    "ax.set_ylim([0, 0.6])\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we want to compare two random vectors with $k$ elements and the elements are random normal variables. \n",
    "\n",
    "$$\n",
    "\\vec x = (x_1, x_2, x_3, x_4, x_5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec y = (y_1, y_2, y_3, y_4, y_5)\n",
    "$$\n",
    "\n",
    "Though this test can be performed for an arbitrary sets of number, typically the compared vectors are frequencies of certain values of some stochastic variable. Thus the vector elements are assumed to be positive.\n",
    "\n",
    "In principle to compare $\\vec x$ and $\\vec y$ we could compute their plan distance and analyse its distribution. But it would be rather complicated problems since the original data can have an arbitrary magnitudes. \n",
    "\n",
    "Instead we compute the rescaled squared distance as\n",
    "\n",
    "$$\n",
    "d^2 = \\sum_{i=1}^{k} \\frac{(x_i - y_i)^2}{x_i}\n",
    "$$\n",
    "\n",
    "Then we select a significance $\\alpha$, say $\\alpha=0.05$, and check if the computed $d^2$ falls within $(1-\\alpha)$ probability area of $\\chi^2$-distribution. \n",
    "\n",
    "If yes the considered vectors are sampled from the same distributions.\n",
    "\n",
    "Typically this test is performed to compare the theoretically expected frequencies with the observed ones.\n",
    "\n",
    "Notice that previously we defined a frequency as number of a particular observed values divided by the total number of observations. They were number within the range between 0 and 1. \n",
    "\n",
    "More precisely it can be called a relative frequency.\n",
    "\n",
    "Performing $\\chi^2$ test we call the number of observed values as the frequency. The frequency is an integer number. The sum of all frequencies equals to the total number of observations. \n",
    "\n",
    "Thus to perform $\\chi^2$-test we compute\n",
    "\n",
    "$$\n",
    "d^2 = \\sum_{i=1}^{k} \\frac{(E_i - O_i)^2}{E_i}\n",
    "$$\n",
    "\n",
    "where $E_i$ are the expected frequencies and $O_i$ are the observed frequencies.\n",
    "\n",
    "Then this value is compared with the $\\chi^2$-distribution as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square test of an unfair coin\n",
    "\n",
    "Let us again perform a test of a coin. \n",
    "\n",
    "The idea is that we conduct a series of tossing many times. Now $N$ is small, say $N=5$ and we repeat it $M=250$ times.\n",
    "\n",
    "The result of each series is a binomial variable $x$ within the range from 0 to $N$. \n",
    "\n",
    "Thus we have a series of $M$ generation of a binomial variable $x$. \n",
    "\n",
    "We count the frequencies of each value of $x$.\n",
    "\n",
    "Also we take the binomial distribution and compute theoretically expected frequencies.\n",
    "\n",
    "Let us first find the theoretical frequencies corresponding to the $H_0$ hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import binom\n",
    "\n",
    "def binom_distr(n, k, p):\n",
    "    \"\"\"Binomial distribution\"\"\"\n",
    "    return float(binom(n, k) * p**k * (1-p)**(n-k))\n",
    "\n",
    "N = 5\n",
    "M = 250\n",
    "\n",
    "P_h0 = 0.5  # This P corresponds to the H0 hypothesis\n",
    "\n",
    "freq_expected = [M * binom_distr(N, k, P_h0) for k in range(N+1)]\n",
    "\n",
    "# expected frequencies\n",
    "print(freq_expected)\n",
    "\n",
    "# their sum equals to the total number of experiments M\n",
    "print(sum(freq_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we model an experiment: repeat a series of $N$ tossing $M$ times and count observed frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "P_actual = 0.55 # Actual probability of the coin\n",
    "\n",
    "freq_observed = [0] * (N + 1) \n",
    "rng = np.random.default_rng()\n",
    "for _ in range(M):  # repeat series of tossing M times\n",
    "    # one series: toss a coin N times and freq_observed heads\n",
    "    x  = sum([1 if x < P_actual else 0 for x in rng.random(size=N)]) \n",
    "    # freq_observed[0] - how many times zero heads appearer in a series of N tossing\n",
    "    # freq_observed[1] - how many times one head appeared after N tossing \n",
    "    # freq_observed[2] - two head in N tossing \n",
    "    # and so on\n",
    "    freq_observed[x] += 1  # freq_observed appeard number of heads\n",
    "\n",
    "# computed freq_observeders\n",
    "print(freq_observed)\n",
    "\n",
    "# sum of all freq_observed must be M\n",
    "print(sum(freq_observed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the $\\chi^2$-test. \n",
    "\n",
    "While the common idea of this test is not so complicated, the actual computations includes some subtle steps and we do not write the routine ourself. (For example frequencies with less then 5 observations must be merged with other frequencies.)\n",
    "\n",
    "The function `chisquare` from `scipy.stats` is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "\n",
    "chi = chisquare(f_obs=freq_observed, f_exp=freq_expected)\n",
    "\n",
    "# Print the result return by chisquare\n",
    "print(chi)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "if chi.pvalue < alpha:\n",
    "    print(\"H0 is rejected, the coin is unfair\")\n",
    "else:\n",
    "    print(\"H0 is not rejected, the coin is fair\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recomputing the test with new random series of tossing at $P_\\text{actual}=0.55$ we observe that the unfair coin is basically detected. \n",
    "\n",
    "But sometimes errors also occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "ru",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "113px",
    "width": "228px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
