{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Dimension reduction\n",
    "\n",
    "Part of [\"Introduction to Data Science\" course](https://github.com/kupav/data-sc-intro) by Pavel Kuptsov, [kupav@mail.ru](mailto:kupav@mail.ru)\n",
    "\n",
    "Recommended reading for this section:\n",
    "\n",
    "1. Grus, J. (2019). Data Science From Scratch: First Principles with Python (Vol. Second edition). Sebastopol, CA: O’Reilly Media\n",
    "1. Muller, A and Guido, S (2017). Introduction to Machine Learning with Python. O'Reilly\n",
    "1. A beginner’s guide to dimensionality reduction in Machine Learning. https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e\n",
    "\n",
    "The following Python modules will be required. Make sure that you have them installed.\n",
    "- `matplotlib`\n",
    "- `requests`\n",
    "- `numpy`\n",
    "- `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal components\n",
    "\n",
    "Before start we define the function the downloads CSV file from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "def load_csv_dataset(file_name, dtype=float):\n",
    "    \"\"\"Downloads csv numeric dataset from repo to numpy array.\"\"\"\n",
    "    base_url = \"https://raw.githubusercontent.com/kupav/data-sc-intro/main/data/\"\n",
    "    web_data = requests.get(base_url + file_name)\n",
    "    assert web_data.status_code == 200\n",
    "    \n",
    "    reader = csv.reader(web_data.text.splitlines(), delimiter=',')\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        try:\n",
    "            # Try to parse as a row of floats\n",
    "            float_row = [dtype(x) for x in row]\n",
    "            data.append(float_row)\n",
    "        except ValueError:\n",
    "            # If parsing as floats failed - this is header\n",
    "            print(row)\n",
    "            \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes multidimensional data contain redundant information. \n",
    "\n",
    "Consider a two dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_csv_dataset('pca1.csv')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains two columns. Let us plot their separated histograms first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "axs[0].hist(data[:, 0], bins=50)\n",
    "axs[1].hist(data[:, 1], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see random normally distributed data.\n",
    "\n",
    "But their scatter plot looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[:, 0], data[:, 1])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means that the two columns depends on each other: It is easy to notice that they obey the equation\n",
    "$$\n",
    "y = -2x\n",
    "$$\n",
    "We can check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[:, 0], data[:, 1])\n",
    "\n",
    "vx = np.linspace(-4, 4, 100)\n",
    "vy = -2 * vx\n",
    "ax.plot(vx, vy, color='red')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset like this we do not need to analyze both of its columns. \n",
    "\n",
    "Only one contains the essential information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More often such strong dependence is absent. \n",
    "\n",
    "But we still can notice that the data columns are not totally independent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_csv_dataset('pca2.csv')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[:, 0], data[:, 1])\n",
    "ax.grid()"
   ]
  },
  {
   "attachments": {
    "pca_idea.svg": {
     "image/svg+xml": [
      "<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg
   xmlns:osb="http://www.openswatchbook.org/uri/2009/osb"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="81.122673mm"
   height="50.090759mm"
   viewBox="0 0 81.122672 50.090759"
   version="1.1"
   id="svg8"
   inkscape:version="1.0.1 (3bc2e813f5, 2020-09-07, custom)"
   sodipodi:docname="pca_idea.svg">
  <defs
     id="defs2">
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2274"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2255"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2236"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2217"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2192"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2086"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1963"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1890"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1825"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <linearGradient
       id="linearGradient3347"
       osb:paint="solid">
      <stop
         style="stop-color:#000000;stop-opacity:1;"
         offset="0"
         id="stop3345" />
    </linearGradient>
    <linearGradient
       id="linearGradient3341"
       osb:paint="solid">
      <stop
         style="stop-color:#cccccc;stop-opacity:1;"
         offset="0"
         id="stop3339" />
    </linearGradient>
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1241"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1237"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1233"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1229"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1225"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1219"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1211"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1147"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1143"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <marker
       style="overflow:visible"
       id="marker1121"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="Arrow1Lstart"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,10,0)"
         style="fill:#000000;fill-opacity:1;fill-rule:evenodd;stroke:#000000;stroke-width:1pt;stroke-opacity:1"
         d="M 0,0 5,-5 -12.5,0 5,5 Z"
         id="path1119" />
    </marker>
    <marker
       style="overflow:visible"
       id="Arrow1Lstart"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="Arrow1Lstart"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,10,0)"
         style="fill:#000000;fill-opacity:1;fill-rule:evenodd;stroke:#000000;stroke-width:1pt;stroke-opacity:1"
         d="M 0,0 5,-5 -12.5,0 5,5 Z"
         id="path843" />
    </marker>
    <inkscape:path-effect
       effect="bspline"
       id="path-effect841"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect837"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <pattern
       patternUnits="userSpaceOnUse"
       width="15.345923"
       height="10.518884"
       patternTransform="translate(154.82704,139.74056)"
       id="pattern3334">
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.623622;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         d="M 0.17296163,10.259441 15.172962,0.25944247"
         id="path3321" />
    </pattern>
    <marker
       style="overflow:visible"
       id="marker3345"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343" />
    </marker>
    <inkscape:path-effect
       effect="bspline"
       id="path-effect3341"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect3331"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <marker
       style="overflow:visible"
       id="marker3345-3"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-6" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker3345-5"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-3" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker3345-6"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-2" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker3345-6-1"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-2-2" />
    </marker>
  </defs>
  <sodipodi:namedview
     id="base"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageopacity="0.0"
     inkscape:pageshadow="2"
     inkscape:zoom="2.8"
     inkscape:cx="194.34543"
     inkscape:cy="174.13897"
     inkscape:document-units="mm"
     inkscape:current-layer="layer1"
     inkscape:document-rotation="0"
     showgrid="true"
     inkscape:window-width="1770"
     inkscape:window-height="1326"
     inkscape:window-x="786"
     inkscape:window-y="29"
     inkscape:window-maximized="0"
     inkscape:snap-global="true">
    <inkscape:grid
       type="xygrid"
       id="grid833"
       originx="6.3636268"
       originy="-24.99051" />
  </sodipodi:namedview>
  <metadata
     id="metadata5">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title></dc:title>
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     inkscape:label="Слой 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(6.3636273,-24.990511)">
    <path
       style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.293112px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;marker-start:url(#marker1121)"
       d="m -4.875133,25.559548 c 0,15.875215 0,31.750216 0,47.624998"
       id="path835"
       inkscape:path-effect="#path-effect837"
       inkscape:original-d="m -4.875133,25.559548 c 3.969e-4,15.875215 3.969e-4,31.750216 0,47.624998" />
    <path
       style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.3735;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#Arrow1Lstart)"
       d="m 72.515492,73.184548 c -25.797225,0 -51.594103,0 -77.3906241,0"
       id="path839"
       inkscape:path-effect="#path-effect841"
       inkscape:original-d="m 72.515492,73.184548 c -25.797225,3.96e-4 -51.594103,3.96e-4 -77.3906241,0" />
    <text
       xml:space="preserve"
       style="font-size:4.93889px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:none;stroke-width:0.3975;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
       x="70.531113"
       y="69.215797"
       id="text1185"><tspan
         sodipodi:role="line"
         id="tspan1183"
         x="70.531113"
         y="69.215797"
         style="font-style:italic;font-size:4.93889px;fill:#000000;stroke:none;stroke-width:0.3975;stroke-miterlimit:4;stroke-dasharray:none">x<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:65%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub"
   id="tspan911">1</tspan></tspan></text>
    <text
       xml:space="preserve"
       style="font-size:4.93889px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396874px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       x="-1.0325345"
       y="28.138975"
       id="text1189"><tspan
         sodipodi:role="line"
         id="tspan1187"
         x="-1.0325345"
         y="28.138975"
         style="font-style:italic;font-size:4.93889px;fill:#000000;stroke:none;stroke-width:0.396874px">x<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:65%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub"
   id="tspan913">2</tspan></tspan></text>
    <g
       id="g2521"
       transform="translate(-2.9409167,1.7501433)">
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.265;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
         d="m 5.2916663,54.239583 c 0.8091454,1.725928 1.6181412,3.451537 2.4269884,5.176829"
         id="path2190"
         inkscape:path-effect="#path-effect2192"
         inkscape:original-d="m 5.2916663,54.239583 c 0.8092605,1.725874 1.6182566,3.451483 2.4269884,5.176829"
         sodipodi:nodetypes="cc" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
         d="m 27.269659,51.616216 c 1.052627,2.197691 2.105101,4.395062 3.157424,6.592117"
         id="path2190-0"
         inkscape:path-effect="#path-effect2217"
         inkscape:original-d="m 27.269659,51.616216 c 1.052739,2.197637 2.105213,4.395008 3.157424,6.592117"
         sodipodi:nodetypes="cc" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
         d="m 39.230561,46.768788 c 1.034418,2.04961 2.068675,4.098902 3.102773,6.147879"
         id="path2190-0-9"
         inkscape:path-effect="#path-effect2236"
         inkscape:original-d="m 39.230561,46.768788 c 1.034522,2.049557 2.068779,4.09885 3.102773,6.147879"
         sodipodi:nodetypes="cc" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
         d="m 43.656249,35.71875 c 1.17413,2.47883 2.34811,4.957342 3.521939,7.435537"
         id="path2190-0-9-3"
         inkscape:path-effect="#path-effect2255"
         inkscape:original-d="m 43.656249,35.71875 c 1.174244,2.478776 2.348224,4.957288 3.521939,7.435537"
         sodipodi:nodetypes="cc" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
         d="m 63.795083,36.61444 c 0.783715,1.906617 1.567298,3.812915 2.350751,5.718894"
         id="path2190-0-9-3-6"
         inkscape:path-effect="#path-effect2274"
         inkscape:original-d="m 63.795083,36.61444 c 0.783848,1.906562 1.567432,3.81286 2.350751,5.718894"
         sodipodi:nodetypes="cc" />
      <text
         xml:space="preserve"
         style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
         x="7.7171559"
         y="54.701439"
         id="text1189-0"><tspan
           sodipodi:role="line"
           id="tspan1187-6"
           x="7.7171559"
           y="54.701439"
           style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2">1</tspan></tspan></text>
      <text
         xml:space="preserve"
         style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
         x="22.866385"
         y="58.445599"
         id="text1189-0-6"><tspan
           sodipodi:role="line"
           id="tspan1187-6-1"
           x="22.866385"
           y="58.445599"
           style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8">2</tspan></tspan></text>
      <text
         xml:space="preserve"
         style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
         x="43.640316"
         y="50.856781"
         id="text1189-0-6-7"><tspan
           sodipodi:role="line"
           id="tspan1187-6-1-9"
           x="43.640316"
           y="50.856781"
           style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8-2">3</tspan></tspan></text>
      <text
         xml:space="preserve"
         style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
         x="47.071804"
         y="38.854546"
         id="text1189-0-6-7-0"><tspan
           sodipodi:role="line"
           id="tspan1187-6-1-9-2"
           x="47.071804"
           y="38.854546"
           style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8-2-3">4</tspan></tspan></text>
      <text
         xml:space="preserve"
         style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
         x="67.452507"
         y="40.015141"
         id="text1189-0-6-7-0-7"><tspan
           sodipodi:role="line"
           id="tspan1187-6-1-9-2-5"
           x="67.452507"
           y="40.015141"
           style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8-2-3-9">5</tspan></tspan></text>
      <path
         style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345)"
         d="m 5.2916663,54.239583 c 0.097002,0.175847 0.2337022,0.423661 0.341749,0.619531"
         id="path3339"
         inkscape:path-effect="#path-effect3341"
         inkscape:original-d="m 5.2916663,54.239583 c 0.09709,0.175798 0.233793,0.423611 0.341749,0.619531"
         sodipodi:nodetypes="cc" />
      <path
         style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-5)"
         d="m 30.427083,58.208333 c 0.097,0.175847 0.233702,0.423661 0.341749,0.619531"
         id="path3339-5"
         inkscape:path-effect="#path-effect1890"
         inkscape:original-d="m 30.427083,58.208333 c 0.09709,0.175798 0.233793,0.423611 0.341749,0.619531"
         sodipodi:nodetypes="cc" />
      <path
         style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-3)"
         d="m 42.333333,52.916667 c 0.097,0.175847 0.233702,0.423661 0.341749,0.619531"
         id="path3339-7"
         inkscape:path-effect="#path-effect1825"
         inkscape:original-d="m 42.333333,52.916667 c 0.09709,0.175798 0.233793,0.423611 0.341749,0.619531"
         sodipodi:nodetypes="cc" />
      <path
         style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-6-1)"
         d="m 43.65625,35.71875 c 0.097,0.175847 0.233702,0.423661 0.341749,0.619531"
         id="path3339-9-7"
         inkscape:path-effect="#path-effect2086"
         inkscape:original-d="m 43.65625,35.71875 c 0.09709,0.175798 0.233793,0.423611 0.341749,0.619531"
         sodipodi:nodetypes="cc" />
      <path
         style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-6)"
         d="m 66.145833,42.333333 c 0.097,0.175847 0.233702,0.423661 0.341749,0.619531"
         id="path3339-9"
         inkscape:path-effect="#path-effect1963"
         inkscape:original-d="m 66.145833,42.333333 c 0.09709,0.175798 0.233793,0.423611 0.341749,0.619531"
         sodipodi:nodetypes="cc" />
      <path
         style="fill:#0000ff;fill-rule:evenodd;stroke:#0000ff;stroke-width:0.524999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"
         d="M 68.791666,34.395833 C 47.179465,43.217139 25.571825,52.036584 3.9687492,60.854166"
         id="path3329"
         inkscape:path-effect="#path-effect3331"
         inkscape:original-d="M 68.791666,34.395833 C 47.179595,43.217458 25.571955,52.036904 3.9687492,60.854166"
         sodipodi:nodetypes="cc" />
    </g>
  </g>
</svg>
"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a cloud of points highly stretched along a certain direction. \n",
    "\n",
    "Obviously that variations along this direction are more essential than the perpendicular ones.\n",
    "\n",
    "Probably the true underlying process that generated these data had strong dependence between \n",
    "columns\n",
    "$$\n",
    "y=kx\n",
    "$$\n",
    "and deviations from it appeared due to noise.\n",
    "\n",
    "Direction of the most intensive variations is called principal component. \n",
    "\n",
    "The idea of its finding is as follows.\n",
    "\n",
    "![pca_idea.svg](fig/pca_idea.svg)\n",
    "\n",
    "We imagine that we have a scatter plot of data and draw a line between them. Then we compute distances $d_i$ between the line and the data points as shown in the figure. \n",
    "\n",
    "We need to rotate the line to make the sum of the distances as small as possible.\n",
    "\n",
    "This line or a unit vector along it is called the first principal component. \n",
    "\n",
    "The vector perpendicular to the first  principal component is called the second  principal component. \n",
    "\n",
    "In the examples above we considered two dimensional data so that the cloud of points could \n",
    "be stretched along a single direction.\n",
    " \n",
    "If data are multidimensional, i.e., there are more then two columns, the number of principal \n",
    "components equals to the number of columns.\n",
    "\n",
    "The first one is the directions where the cloud is the most stretched.\n",
    "\n",
    "The second one shows the most stretched direction among all perpendicular to the first one.\n",
    "\n",
    "The third is the most stretched direction that is perpendicular to the first two. And so on.\n",
    "\n",
    "Data analysis via the principal components is called PCA, Principal Components Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the principal components\n",
    "\n",
    "Mathematically computation of principal components involves the following steps.\n",
    "\n",
    "First we must compute mean values along each data column and subtract them so that \n",
    "the data cloud becomes centered near the origin.\n",
    "$$\n",
    "\\tilde x_i = x_i - \\mu\n",
    "$$\n",
    "\n",
    "This is absolutely important step. Omitting it results in incorrect results. \n",
    "\n",
    "Also it is often recommended to rescale the data columns by dividing by the standard deviation. Together with the shifting means to the origin this is called data standardizing.\n",
    "$$\n",
    "z_i = \\frac{x_i - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "As we already discussed preliminary standardizing is very important when we have different \n",
    "data units in different columns.\n",
    "\n",
    "If the units are the same (e.g., all columns are in meters) it may be reasonable to left the data not rescaled. \n",
    "\n",
    "However the shift to the origin must be done in any case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is computing variances and covariances.\n",
    "\n",
    "Let us remember that the variance is the mean squared deviation from the mean value:\n",
    "$$\n",
    "\\overline x = \\frac{1}{N}\\sum_{i=1}^N x_i.\n",
    "$$\n",
    "$$\n",
    "\\text{Var}=\\frac{1}{N-1}\\sum_{i=1}^N (x_i - \\overline x)^2.\n",
    "$$\n",
    "\n",
    "Since we have shifted the data to the origin the mean is zero, so that\n",
    "$$\n",
    "\\text{Var}=\\frac{1}{N-1}\\sum_{i=1}^N z_i^2.\n",
    "$$\n",
    "\n",
    "Let us denote our multidimensional standardized data as $z_{i,j}$. \n",
    "\n",
    "Here $i$ is a number of a row. Usually we have many rows. The number of rows is the size of the dataset. \n",
    "\n",
    "Index $j$ is a number of a column. The number of columns means the dimension of the dataset. Given two columns we have two dimensional dataset.\n",
    "\n",
    "Covariance is computed like this (provided that the data have zero mean values along $i$):\n",
    "$$\n",
    "\\text{Cov}(j,k)=\\frac{1}{N-1}\\sum_{i=1}^N z_{i,j} z_{i,k}.\n",
    "$$\n",
    "Here the summation runs along rows for two columns $j$ and $k$.\n",
    "\n",
    "If $j=k$ we merely have the variance of the column $j$. And $\\text{Cov}(j,k)=\\text{Cov}(k,j)$\n",
    "\n",
    "The matrix collecting covariances for all pairs of the columns is called covariance matrix:\n",
    "$$\n",
    "C = \n",
    "\\begin{pmatrix}\n",
    "\\text{Cov}(0,0)  & \\text{Cov}(0,1) & \\text{Cov}(0,2) & \\ldots \\\\\n",
    "\\text{Cov}(1,0)  & \\text{Cov}(1,1) & \\text{Cov}(1,2) & \\ldots \\\\\n",
    "\\ldots & \\ldots & \\ldots & \\ldots \n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to compute eigenvalues and eigenvectors of the covariance matrix $C$.\n",
    "\n",
    "Let us remember: we can multiply a matrix by a vector to obtain a new vector:\n",
    "$$\n",
    "A v_1 = v_2\n",
    "$$\n",
    "In general case $v_1$ and $v_2$ are different. They have different lengths and directions.\n",
    "\n",
    "Each square matrix $N\\times N$ has $N$ special vectors such that \n",
    "$$\n",
    "A u = \\lambda u\n",
    "$$\n",
    "Here $\\lambda$ is scalar (i.e., just a number). It means that when we multiply the matrix $A$ by the vector $u$ \n",
    "we obtain a vector that points the same direction that $u$ but stretched or shrined by $\\lambda$.\n",
    "\n",
    "Scalars $\\lambda$ are called eigenvalues of the matrix $A$ and $u$ are its eigenvectors.\n",
    "\n",
    "Eigenvectors of the covariance matrix $C$ are principal components of our dataset and the corresponding eigenvalues $\\lambda$ indicate the range of variations along this components.\n",
    "\n",
    "For the covariance matrix $C$ the eigenvalues are always real positive numbers (this is because $C$ is symmetric).\n",
    "\n",
    "The first principal component corresponds to the largest eigenvalue. The second one corresponds to the second largest $\\lambda$ and so on.\n",
    "\n",
    "The eigenvalues indicate how the cloud of points is stretched along the corresponding principal component. \n",
    "\n",
    "For example if the cloud almost exactly fits a line the first $\\lambda_1$ is the largest and all others are close to zero.\n",
    "\n",
    "If the cloud of multidimensional data is spread along a plane two first eigenvalues $\\lambda_1$ and $\\lambda_2$ will be large, while all others small. And so on.\n",
    "\n",
    "Here the function that implements the steps above. We compute covariances and eigenvalues using functions from `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    \"\"\"Standartize data\"\"\"\n",
    "    return (data - np.mean(data, axis=0)) / np.std(data, axis=0)    \n",
    "\n",
    "def prin_comp(data):\n",
    "    \"\"\"Computes principal components for a multidimensional data\n",
    "    Returns a list of eigenvalues and eigenvectors of the covariance matrix.\n",
    "    Columns of the matrix vec are the principal components. Corresponding\n",
    "    lam indicate their importance. Lamdas are always returned in \n",
    "    the ascending order.\n",
    "    \"\"\"\n",
    "    # Covariance matrix\n",
    "    cov = np.cov(data, rowvar=False)\n",
    "    # Eigenvalues and eigenvectors of a symmetric matrix\n",
    "    lam, vec = np.linalg.eigh(cov)\n",
    "    return lam, vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the dataset and standardize it at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = standardize(load_csv_dataset(\"pca3.csv\"))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing the principal components let us visualize the data using histograms and pairwise scatter \n",
    "plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = data.shape[1]\n",
    "fig, axs = plt.subplots(nrows=N, ncols=N, figsize=(10, 10))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        if i == j:\n",
    "            axs[i, i].hist(data[:, i], bins=300, color='C1')\n",
    "        else:\n",
    "            axs[i, j].scatter(data[:, i], data[:, j], s=1)\n",
    "\n",
    "# Requred to avoid overlapping of the subplots            \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual inspection revels that columns 1, 2, and 4 are correlated. (Observe stretched clouds in panels 1-2, and 1-4).\n",
    "\n",
    "Also the correlated columns are 3 and 5. (Stretched clouds in the panels 3-5. \n",
    "\n",
    "And these two sets of columns are not correlated at all. (Circular clouds, e.g. in panels 1-3 and 1-5)\n",
    "\n",
    "These two groups of the correlated columns indicate that in the full 5th dimensional space there are tho \n",
    "main independent directions, i.e., the cloud of points is highly stretched along a plane while variations along \n",
    "the three other dimensions are small.\n",
    "\n",
    "Now we compute the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=2):\n",
    "    pc_lam, pc_vec = prin_comp(data)\n",
    "    print(\"pc_lam=\\n\", pc_lam)\n",
    "    print(\"pc_vec=\\n\", pc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential and non-essential principal components\n",
    "\n",
    "We indeed observe that two principal components are the most essential. \n",
    "\n",
    "It means altogether that our 5th dimensional dataset is essentially 2 dimensional and \n",
    "three dimensions can be dropped out.\n",
    "\n",
    "This is called dimension reduction. \n",
    "\n",
    "Since PCA can see only a linear dependencies this is also called a linear dimension reduction.\n",
    "\n",
    "All criteria for choosing the essential and non essential principal components are based on values of the eigenvalues \n",
    "$\\lambda_i$.\n",
    "\n",
    "Sometimes it is obvious, like in our example, which components can be removed.\n",
    "\n",
    "But if $\\lambda_i$ are not so different, various approaches are used. All of them are heuristic, i.e, are based on some intuition. \n",
    "\n",
    "- Keep components whose eigenvalues are greater than 1.\n",
    "- Plot the scatter plot of $i$ against $\\lambda_i$ and see if the points can be visually \n",
    "  separated into two clusters of high and small values.\n",
    "- Compute the explained variances: $\\tilde\\lambda_i = \\lambda_i / \\sum_{i=1}^N \\lambda_i$ (each eigenvalue is divided by the sum of all of them). Then keep those components that explain 95\\% of variance.\n",
    "\n",
    "Let us apply these approaches to our data. \n",
    "\n",
    "Two the most essential components are indeed grater then 1 while others are less.\n",
    "\n",
    "The visualization confirms that we have to keep only two components: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "nums = list(range(len(pc_lam)))\n",
    "ax.scatter(nums, pc_lam);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variances are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each eigenvalue is divided by their sum and np.flip reverses the order\n",
    "expl_var = np.flip(pc_lam / np.sum(pc_lam))\n",
    "print(expl_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply the cumulative sum to see what components explain 95% of variance\n",
    "np.cumsum(expl_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this criterion of 95% explained variance is not fulfilled exactly. \n",
    "\n",
    "The explained variance above 95% includes three components. \n",
    "\n",
    "But since the sum of the first two gives 94% while the next one bring only 1% it looks reasonable again \n",
    "to keep only two components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear dimension reduction using principal components\n",
    "\n",
    "When we have a matrix whose columns are the principal components and have made a decision which will be kept we compose \n",
    "a matrix of this components.\n",
    "\n",
    "Let us denote the result as $W$. This matrix is also called a projection matrix because we will find projection \n",
    "of the original data onto its columns. \n",
    "\n",
    "The Columns of this matrix are the essential principal components, i.e., the eigenvectors of\n",
    "the covariance matrix that correspond to the essential eigenvalues. \n",
    "\n",
    "The number of the essential components will be the reduced dimension of our data set. \n",
    "\n",
    "In our example it will be 2, since we have decided to keep only two components. \n",
    "\n",
    "To perform the reduction we have to take rows of the initial dataset (of course, the one that we obtained after the standardizing)\n",
    "and multiply them by the matrix $W$: \n",
    "$$\n",
    "Z_{\\text{red}} = Z w\n",
    "$$\n",
    "Here $Z$ is standardized dataset whose rows are one by one multiplied by $W$.\n",
    "\n",
    "The result is $Z_{\\text{red}}$, the reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pc_vec)\n",
    "print()\n",
    "proj_w = pc_vec[:,-2:]\n",
    "print(proj_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduced dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_data = data @ proj_w\n",
    "print(red_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see its scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(red_data[:, 0], red_data[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The circular cloud indicates that there are no more dependencies in our reduced data.\n",
    "\n",
    "It means that each column is contains an essential information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization\n",
    "\n",
    "Dimension reduction is required to extract essential features from the dataset. \n",
    "\n",
    "The problem with PCA is that the extracted features can not be treated qualitatively. \n",
    "\n",
    "We can not say what exactly features are extracted by PCA, what they tell about the original data.\n",
    "\n",
    "Another approach widely used for feature extraction is named Non-Negative Matrix Factorization (NMF). \n",
    "\n",
    "NMF can extract easily interpretable features.\n",
    "\n",
    "For example, in the case of facial images, the features such as eyes, noses, moustaches, and lips. \n",
    "\n",
    "The important requirement is that the processed data must be non-negative.\n",
    "\n",
    "Assume there is a data matrix $X$ whose entries are non negative. \n",
    "\n",
    "Its columns are features and the number of columns $N$.\n",
    "\n",
    "NMF is representation of this matrix as a product of two non-negative matrices\n",
    "$$\n",
    "X \\approx W H\n",
    "$$\n",
    "\n",
    "The reduced data are in $W$. Its columns are new features. Their number is $R\\leq N$ and the number of rows in $W$ is the same as in $X$. \n",
    "\n",
    "The matrix $H$ gives weights of the reduced features in the original features. The most essential reduced features have the largest weights.\n",
    "\n",
    "This decomposition unlike PCA is approximate and not unique.\n",
    "\n",
    "The algorithm of NMF is rather complicated and we will use its implementation from the library `sklearn`.\n",
    "\n",
    "Consider how NMF works: we take two signals and mix them together and add noise. Then apply NMF to extract the original signals.\n",
    "\n",
    "First we create the signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Dataset size\n",
    "size = 1000\n",
    "\n",
    "# Two original signals\n",
    "tt = np.linspace(0, 9*np.pi, size)\n",
    "s0 = 2 - np.fmod(-tt, np.pi) + np.fmod(2*tt, 3*np.pi)\n",
    "s1 = np.abs(np.cos(tt*2)) + np.sin(0.5*tt)**2\n",
    "\n",
    "s0 /= s0.max()\n",
    "s1 /= s1.max()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(10, 4), sharex=True)\n",
    "\n",
    "axs[0].plot(tt, s0)\n",
    "axs[1].plot(tt, s1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dataset: we mix two signals with random weights and with noise. Repeat it $N$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features in the dataset that will be processed\n",
    "N = 100\n",
    "\n",
    "# Empty array\n",
    "data = np.zeros((size, N))\n",
    "\n",
    "# Weights for the signals in the range [1, 4]\n",
    "wts0 = 1 + 3 * rng.uniform(size=(size,))\n",
    "wts1 = 1 + 3 * rng.uniform(size=(size,))\n",
    "\n",
    "# Mix signals\n",
    "for i in range(N):\n",
    "    data[:, i] = wts0[i] * s0 + wts1[i] * s1 + 2 * rng.uniform(size=(size,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(nrows=5, ncols=1, figsize=(10, 10), sharex=True)\n",
    "for i in range(5):\n",
    "    axs[i].plot(tt, data[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply NMF. \n",
    "\n",
    "We export the class `NMF` from `sklearn` and specify the we want to get 2 components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=2, init='random', random_state=0, max_iter=10000)\n",
    "W = nmf.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the plot of the extracted features. \n",
    "\n",
    "Observe that we rescale them by maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 4), sharex=True)\n",
    "axs[0].plot(tt, W[:, 0] / np.max(W[:, 0]))\n",
    "axs[0].plot(tt, s1)  # try s1 here to adjust order\n",
    "axs[1].plot(tt, W[:, 1] / np.max(W[:, 1]))\n",
    "axs[1].plot(tt, s0); # try s0 here to adjust order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some times the order of the recovered signals can be reverted. It can be adjusted manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the dataset of 100 columns where only two features were essential. \n",
    "\n",
    "NMF has successfully extracted them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear dimension reduction\n",
    "\n",
    "PCA and NMF can reveal linear dependencies between data columns, i.e.,\n",
    "$$\n",
    "y = k x\n",
    "$$\n",
    "\n",
    "If columns depend on each other nonlinearly, for example like this\n",
    "$$\n",
    "y = x^2\n",
    "$$\n",
    "these methods fail to extract lower dimensional set of features.\n",
    "\n",
    "To extract nonlinear dependencies one can use various special methods.\n",
    "\n",
    "The algorithms are rather complicated and we will use their implementation from `sklearn` library.\n",
    "\n",
    "To test these methods we create a three dimensional dataset whose points lays on a curved smooth surface. \n",
    "\n",
    "This surface is called manifold.\n",
    "\n",
    "This dataset will be processed using different methods that tries to find lower dimensional manifolds in high dimensional data.\n",
    "\n",
    "Descriptions of the methods are taken from the review paper \"A beginner’s guide to dimensionality reduction in Machine Learning\" by \n",
    "Judy T Raj https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e\n",
    "\n",
    "First we define several useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    \"\"\"Standartize data\"\"\"\n",
    "    return (data - np.mean(data, axis=0)) / np.std(data, axis=0)    \n",
    "\n",
    "def cos_sin(deg):\n",
    "    \"\"\"Given an anagles in degrees computes cos and sin\"\"\"\n",
    "    rad = deg * np.pi / 180\n",
    "    return np.cos(rad), np.sin(rad)\n",
    "\n",
    "def rotate(v, ax, ay, az):\n",
    "    \"\"\"Rotate a vector v around axis x, y, and z\"\"\"\n",
    "    cx, sx = cos_sin(ax)\n",
    "    cy, sy = cos_sin(ay)\n",
    "    cz, sz = cos_sin(az)\n",
    "    Mx = np.array([[1,0,0], [0, cx, -sx], [0, sx, cx]])\n",
    "    My = np.array([[cy, 0, sy], [0, 1, 0], [-sy, 0, cy]])\n",
    "    Mz = np.array([[cz, -sz, 0], [sz, cz, 0], [0, 0, 1]])\n",
    "    return Mz @ (My @ (Mx @ v))\n",
    "\n",
    "def colorize(row):\n",
    "    \"\"\"Given the data row takes its first and second elemends and assingn color codes\"\"\"\n",
    "    x, y = row[:2]\n",
    "    if x > 0 and y > 0:\n",
    "        return 0\n",
    "    if x > 0 and y < 0:\n",
    "        return 1\n",
    "    if x < 0 and y > 0:\n",
    "        return 2\n",
    "    if x < 0 and y < 0:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create three dimensional dataset.\n",
    "\n",
    "It will be 3D function  \n",
    "$$\n",
    "z=x^3 - y^2\n",
    "$$\n",
    "We add a noise to it and rotate to complicate the task.\n",
    "\n",
    "After that we have to standardize the data. \n",
    "\n",
    "Manifold searching methods are based on a nearest-neighbor search. \n",
    "\n",
    "It means that data from different columns are compared. \n",
    "\n",
    "For the proper work the data must have the same units and moreover they must have same scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Size of the dataset\n",
    "size = 1000\n",
    "\n",
    "# There will be three columns\n",
    "N = 3\n",
    "\n",
    "# Empty storage\n",
    "data = np.zeros((size, N))\n",
    "\n",
    "# Uniform random numbers between -1 and 1\n",
    "data[:, 0] = 2*rng.uniform(size=(size,))-1\n",
    "data[:, 1] = 2*rng.uniform(size=(size,))-1\n",
    "\n",
    "# Function x^3 - x^2 plus noise\n",
    "data[:, 2] = data[:, 0]**3 - data[:,1]**2 + 0.25 * (2*rng.uniform(size=(size,))-1)\n",
    "\n",
    "# Different colors in different \n",
    "clrs = [colorize(row) for row in data]\n",
    "\n",
    "# Angles of rotation around axes\n",
    "ax, ay, az = 10, 15, 34\n",
    "\n",
    "# Perform rotation\n",
    "for i in range(size):\n",
    "    data[i] = rotate(data[i], ax, ay, az)\n",
    "\n",
    "data = standardize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line to plot figure in a separate interactive window\n",
    "#%matplotlib qt \n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(8,8))\n",
    "im = ax.scatter(data[:, 0], data[:, 1], data[:, 2], c=clrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply different nonlinear methods to represent our three dimensional data on the plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"t-distributed Stochastic Neighbor Embedding (t-SNE): Computes the probability that pairs of data points in the high-dimensional space are related and then chooses a low-dimensional embedding which produce a similar distribution.\" https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e\n",
    "\n",
    "This method is probabilistic and different runs result in different results. \n",
    "\n",
    "To freeze a curtain plot we can specify parameters `random_state` that seed the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE()  # add here random_state=0 to keep the plot unchanged \n",
    "W = proj.fit_transform(data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(W[:, 0], W[:, 1], c=clrs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Multi-dimensional scaling (MDS): A technique used for analyzing similarity or dissimilarity of data as distances in a geometric spaces. Projects data to a lower dimension such that data points that are close to each other (in terms if Euclidean distance) in the higher dimension are close in the lower dimension as well.\" https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "proj = MDS()\n",
    "W = proj.fit_transform(data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(W[:, 0], W[:, 1], c=clrs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Isometric Feature Mapping (Isomap): Projects data to a lower dimension while preserving the geodesic distance (rather than Euclidean distance as in MDS). Geodesic distance is the shortest distance between two points on a curve.\" https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "proj = Isomap()\n",
    "W = proj.fit_transform(data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(W[:, 0], W[:, 1], c=clrs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Locally Linear Embedding (LLE): Recovers global non-linear structure from linear fits. Each local patch of the manifold can be written as a linear, weighted sum of its neighbours given enough data.\" https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "proj = LocallyLinearEmbedding()\n",
    "W = proj.fit_transform(data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(W[:, 0], W[:, 1], c=clrs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hessian Eigenmapping (HLLE): Projects data to a lower dimension while preserving the local neighbourhood like LLE but uses the Hessian operator to better achieve this result and hence the name.\" https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "proj = LocallyLinearEmbedding(method='hessian', n_neighbors=11, eigen_solver='dense')\n",
    "W = proj.fit_transform(data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(W[:, 0], W[:, 1], c=clrs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another improved version of the Locally Linear Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "proj = LocallyLinearEmbedding(method='modified')\n",
    "W = proj.fit_transform(data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(W[:, 0], W[:, 1], c=clrs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Spectral Embedding (Laplacian Eigenmaps): Uses spectral techniques to perform dimensionality reduction by mapping nearby inputs to nearby outputs. It preserves locality rather than local linearity\" https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import SpectralEmbedding\n",
    "proj = SpectralEmbedding()\n",
    "W = proj.fit_transform(data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(W[:, 0], W[:, 1], c=clrs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "ru",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "113px",
    "width": "228px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
