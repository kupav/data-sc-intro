{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 Basic concepts of machine learning\n",
    "\n",
    "Part of [\"Introduction to Data Science\" course](https://github.com/kupav/data-sc-intro) by Pavel Kuptsov, [kupav@mail.ru](mailto:kupav@mail.ru)\n",
    "\n",
    "Recommended reading for this section:\n",
    "\n",
    "1. Grus, J. (2019). Data Science From Scratch: First Principles with Python (Vol. Second edition). Sebastopol, CA: O’Reilly Media\n",
    "1. Muller, A and Guido, S (2017). Introduction to Machine Learning with Python. O'Reilly\n",
    "1. B. Shmueli. Matthews Correlation Coefficient is The Best Classification Metric You’ve Never Heard Of. https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a\n",
    "\n",
    "The following Python modules will be required. Make sure that you have them installed.\n",
    "- `matplotlib`\n",
    "- `numpy`\n",
    "- `collections`\n",
    "- `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical science vs machine learning\n",
    "\n",
    "All science is about prediction of a future. \n",
    "\n",
    "First of all one collects data from a domain of interest: records planets locations vs time of year, or air temperature vs wind \n",
    "speed, or canon ball distance vs amount of gun powder and so on. \n",
    "\n",
    "Then one tries to extract dependencies: does one value really depends on another? What can be a mathematical formula for this \n",
    "dependence? \n",
    "\n",
    "Next step is to check the revealed mathematical formulas on new observations. Do they rely work? After this step experimental laws appear: Newton's second law and Coulomb's law in physics, or Mendelian inheritance in genetics and so on.\n",
    "\n",
    "When enough number of the laws are discovered a mathematical theory appears that integrates them and provides the tools for making predictions. \n",
    "\n",
    "Examples of predictions done in this way are: marine navigation (one predicts ship location observing stars), safe bridge load (one can compute it knowing properties of materials), probability of genetic diseases (using genetics laws).\n",
    "\n",
    "The main problem in creation of science in this way is to find the most simple, fundamental governing laws that lay in \n",
    "the basis of all other laws.\n",
    "\n",
    "In physics such fundamental law is for example Newton's laws of motions, in genetics this is a knowledge about DNA structure and so on.\n",
    "\n",
    "And the law means the law: the things alway occur as it states. No exclusions.\n",
    "\n",
    "The success of the whole filed depends on the success in finding such basic laws.\n",
    "\n",
    "All of this can be called classical approach in knowledge creation.\n",
    "\n",
    "In the second part of the XX century when computers allowed to deal with large amount of data, scientists started trying to apply the classical ideas to a more complicated areas, like for example social relations or medicine.\n",
    "\n",
    "But in such areas no fundamental law can be found. Only some probabilistic dependencies.\n",
    "\n",
    "If you have a lot of practical experience in a certain area, say medicine, you are an expert. \n",
    "\n",
    "May be you do not know the exact fundamental laws (because nobody knows) but you are sure that an event A most probably will be followed by an event B.\n",
    "\n",
    "For example a medical doctor observing certain symptoms can be sure that a patient has a certain disease.\n",
    "\n",
    "How can we formalize this experience, how can we take the knowledge from an expert and put it to a computer?\n",
    "\n",
    "Computer expert systems were created. Mostly they were lists of manually coded rules \"if then else\". \n",
    "\n",
    "The rule were created after questioning experts in the field.\n",
    "\n",
    "In fact this was just a try to apply classical methods to fields where they did not worked properly: \n",
    "recall that the classical science approach requires to move from raw observations to extracting laws. But the computer expert systems were stuck on the first step.\n",
    "\n",
    "Two main problems of the computer systems purely based on expert rules are:\n",
    "\n",
    "- No generalization. We can only answer the questions that the expert were asked. No laws derived so that previously unseen cases can not be properly proceed.\n",
    "\n",
    "- No universality. Solves only specific problems and cannot solve any others. Expert opinion based computer systems could not recognize faces on photos. Because no one human expert can firmly formulate strict rules that allows him to do it.\n",
    "\n",
    "That is why machine learning concept has appeared.\n",
    "\n",
    "**Machine learning is a collection of mathematical methods and computer algorithms that automatically\n",
    "extract knowledge from data.** \n",
    "\n",
    "The generalization and universality is the key requirement. \n",
    "\n",
    "The extracted knowledge must be generic. \n",
    "\n",
    "It means that the machine learning method can not just remember the answers. \n",
    "\n",
    "It must derive common dependencies in data and must be able correctly process new previously unseen cases.\n",
    "\n",
    "This is what classical science does extracting fundamental laws from experimental data. \n",
    "\n",
    "There are two differences.\n",
    "\n",
    "- Classical science laws are discovered by humans. The generalizing of the data is performed automatically \n",
    "by a computer program in the course of learning process.\n",
    "\n",
    "- Since the classical laws are discovered by humans they have a form understandable for humans. These laws are represented \n",
    "as mathematical equations or text in natural language. Machine learning knowledge obtained after generalization of data \n",
    "consist of numbers. Typically this are huge arrays of numbers. In the most cases humans can not understand this knowledge. We can only \n",
    "use it without knowing why it works.\n",
    "\n",
    "Machine learning methods are universal. They are applicable in every filed that can be described by a dataset. \n",
    "\n",
    "Of course some methods work better for face recognition while others are preferred for time series predictions. \n",
    "\n",
    "But the key ideas are common and all domain of interest can be analyzed using machine methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning\n",
    "\n",
    "The following paragraphs are taken form book [2].\n",
    "\n",
    "---\n",
    "\n",
    "Machine learning is about extracting knowledge from data. \n",
    "\n",
    "The application of machine learning methods has in recent years become ubiquitous in everyday life. \n",
    "\n",
    "From automatic recommendations of which movies to watch, to what food to order or which\n",
    "products to buy, to personalized online radio and recognizing your friends in your\n",
    "photos, many modern websites and devices have machine learning algorithms at their\n",
    "core. \n",
    "\n",
    "When you look at a complex website like Facebook, Amazon, or Netflix, it is\n",
    "very likely that every part of the site contains multiple machine learning models.\n",
    "\n",
    "Outside of commercial applications, machine learning has had a tremendous \n",
    "influence on the way data-driven research is done today. \n",
    "\n",
    "The \\[machine learning\\] tools ... have been applied to diverse scientific problems \n",
    "such as understanding stars, finding distant planets, discovering new particles, \n",
    "analyzing DNA sequences, and providing personalized cancer treatments.\n",
    "\n",
    "---\n",
    "\n",
    "Quite possibly the most important part in the machine learning process is understanding \n",
    "the data you are working with and how it relates to the task you want to\n",
    "solve. \n",
    "\n",
    "It will not be effective to randomly choose an algorithm and throw your data at\n",
    "it. \n",
    "\n",
    "It is necessary to understand what is going on in your dataset before you begin\n",
    "building a model. \n",
    "\n",
    "Each algorithm is different in terms of what kind of data and what\n",
    "problem setting it works best for. \n",
    "\n",
    "While you are building a machine learning solution,\n",
    "you should answer, or at least keep in mind, the following questions:\n",
    "\n",
    "- What question(s) am I trying to answer? Do I think the data collected can answer that question?\n",
    "\n",
    "- What is the best way to phrase my question(s) as a machine learning problem?\n",
    "\n",
    "- Have I collected enough data to represent the problem I want to solve?\n",
    "\n",
    "- What features of the data did I extract, and will these enable the right predictions?\n",
    "\n",
    "- How will I measure success in my application?\n",
    "\n",
    "- How will the machine learning solution interact with other parts of my research or business product?\n",
    "\n",
    "In a larger context, the algorithms and methods in machine learning are only one\n",
    "part of a greater process to solve a particular problem, and it is good to keep the big\n",
    "picture in mind at all times. \n",
    "\n",
    "Many people spend a lot of time building complex machine learning solutions, only to find \n",
    "out they don’t solve the right problem.\n",
    "\n",
    "When going deep into the technical aspects of machine learning ..., it is easy to lose sight \n",
    "of the ultimate goals. \n",
    "\n",
    "---\n",
    "\n",
    "End of citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "The key role in the machine learning belong to models. \n",
    "\n",
    "Models extraction knowledges and do predictions.\n",
    "\n",
    "The following paragraphs are taken from book [1]\n",
    "\n",
    "---\n",
    "\n",
    "What is a model? \n",
    "\n",
    "It’s simply a specification of a mathematical (or probabilistic) relationship \n",
    "that exists between different variables.\n",
    "\n",
    "For instance, if you're trying to raise money for your social networking site,\n",
    "you might build a business model (likely in a spreadsheet) that takes inputs\n",
    "like \"number of users,\" \"ad revenue per user,\" and \"number of employees\"\n",
    "and outputs your annual profit for the next several years. \n",
    "\n",
    "A cookbook recipe entails a model that relates inputs like \"number of eaters\" \n",
    "and \"hungriness\" to quantities of ingredients needed. \n",
    "\n",
    "And if you've ever watched poker on television, you know that each player's \n",
    "\"win probability\" is estimated in real time based on a model that takes into \n",
    "account the cards that have been revealed so far and the distribution of cards in the deck.\n",
    "\n",
    "The business model is probably based on simple mathematical\n",
    "relationships: profit is revenue minus expenses, revenue is units sold times\n",
    "average price, and so on. \n",
    "\n",
    "The recipe model is probably based on trial and\n",
    "error - someone went in a kitchen and tried different combinations of\n",
    "ingredients until they found one they liked. \n",
    "\n",
    "And the poker model is based on probability theory, the rules of poker, \n",
    "and some reasonably innocuous\n",
    "assumptions about the random process by which cards are dealt.\n",
    "\n",
    "---\n",
    "\n",
    "End of citation\n",
    "\n",
    "More rigorously, model is a simplified system that possess the most important properties and relations of another system. \n",
    "\n",
    "Models are created using a formal language, such that mathematic notation, programming language. \n",
    "\n",
    "Human language is basically inappropriate for creation models due to its vagueness. However sometimes it is also used for example in psychology.\n",
    "\n",
    "Models are created to obtain knowledges about the modeled system.\n",
    "\n",
    "Typically natural system (e.g, social ones) are too complicated for a straightforward study.\n",
    "\n",
    "In these case a model is created that keeps only the most essential feature of the original system. \n",
    "\n",
    "Studying the model gives knowledges about the original system.\n",
    "\n",
    "Since models are always truncated in features in comparison with the original system errors are unavoidable.\n",
    "\n",
    "Building a model is always a trade-off between simplicity (and thus possibility of its study) and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning models\n",
    "\n",
    "Machine learning model is an aggregate of computer program and array of parameters (are mere computer variables). \n",
    "\n",
    "The model has inputs where the information from the domain of interest is fed and outputs where the prediction \n",
    "appears.\n",
    "\n",
    "The predictions are done after processing inputs by a model computer program using parameters. \n",
    "\n",
    "While the computer program is often standard, the values of parameters are specially tuned in \n",
    "the course of training.\n",
    "\n",
    "The parameters are the most precious part of the model. Their number is usually large and they are kept in \n",
    "files in external memory.\n",
    "\n",
    "While predictions are done by the model itself, its training requires additional software. \n",
    "\n",
    "It can be standard or written by a data scientist for the particular task.\n",
    "\n",
    "The training software \n",
    "\n",
    "- performs data gathering from different sources (data mining, big data),\n",
    "- provides data access (reading from files, downloading from web, etc), \n",
    "- feeding the data to the model,\n",
    "- receiving the prediction, \n",
    "- estimation its quality,\n",
    "- correction of the model parameters to improve it if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised models\n",
    "\n",
    "These models are trained on a labeled dataset.\n",
    "\n",
    "It means each input, i.e., each data record, is accompanied by a desired output. \n",
    "\n",
    "In the course of training the input is passed to the model. \n",
    "\n",
    "The model produces the output that is compared  with the desired one. \n",
    "\n",
    "If they are different the model parameters are corrected to decrease the difference.\n",
    "\n",
    "Labeled datasets are created by humans. Often this is done by volunteers via crowd-findings platforms.\n",
    "\n",
    "The famous example is ImageNet. \n",
    "\n",
    "The ImageNet project is a large visual database designed for use in visual object recognition software research.\n",
    "\n",
    "More than 14 million images have been hand-annotated by the project to indicate what objects are pictured. In at least one million of the images bounding boxes are also provided.\n",
    "\n",
    "The purpose of the supervised learning is the extraction and storing the essential features of data as model parameters.\n",
    "\n",
    "The desired result of the training: the algorithm is able to create a correct output for an input\n",
    "it has never seen before without any help from a human.\n",
    "\n",
    "For example we can create a spam filter that analyses the content of emails and decides if this is spam or not. \n",
    "\n",
    "To train it we have to create a dataset the includes various emails that are labeled as spam or not spam.\n",
    "\n",
    "After training, given a new email, the algorithm will then produce a prediction as to whether the new email is\n",
    "spam.\n",
    "\n",
    "More examples of supervised machine learning tasks\n",
    "\n",
    "- Identifying user by their faces.\n",
    "- Identifying the zip code from handwritten digits on an envelope.\n",
    "- Detecting fraudulent activity in credit card transactions.\n",
    "- Identifying license plate number on road cameras images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised models\n",
    "\n",
    "In unsupervised learning, only the input data are known, and no known output data is given to the model. \n",
    "\n",
    "The purpose of the unsupervised learning is to detect patterns from data.\n",
    "\n",
    "One type of the unsupervised learning tasks is to discover groups of similar examples within the data. This is called __data clustering__. \n",
    "\n",
    "Also this class of tasks is called __non-parametric unsupervised Learning__ (in comparison with the next one).\n",
    "\n",
    "The second type is known as __density estimation__ or __parametric learning__. \n",
    "\n",
    "In this case, we expect that the data are not grouped into clusters but distributed according to a certain law, e.g., normal distribution. \n",
    "\n",
    "We assume a certain distribution function followed by the data and the goal is to compute its parameters. \n",
    "\n",
    "Examples of unsupervised learning tasks:\n",
    "\n",
    "- Identifying topics in a set of blog posts\n",
    "- Segmenting customers into groups with similar preferences\n",
    "- Detecting abnormal access patterns to a website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement models\n",
    "\n",
    "These models are little bit similar to the supervised ones. But nevertheless there is a serious difference. \n",
    "\n",
    "If the learning is supervised we know a global loss function. \n",
    "\n",
    "It means that we always know exact desired output for each training input. \n",
    "\n",
    "Thus we can compare the desired and the obtained outputs and compute the distance between them. \n",
    "\n",
    "Given the distance we use an optimization\n",
    "algorithm (e.g., gradient decent) and compute parameters updates directly towards to the optimum. \n",
    "\n",
    "In the course of the reinforcement learning a model interacts with a dynamic environment that sends it inputs and the models replies with outputs (examples are driving a vehicle or playing a game against an opponent). \n",
    "\n",
    "The difference with the supervised learning is that the exact desired output is unknown. \n",
    "\n",
    "The environment is dynamic, i.e., the situation changes in time. \n",
    "\n",
    "The model trainer can not estimate its output globally (if this move definitely leads to a win or to a loss), but it can say is it good or not locally, for this particular situation.\n",
    "\n",
    "Thus each model output evaluated in terms of rewards and punishments and parameters updates are computed to minimize the punishments and to increase rewards in future situations.\n",
    "\n",
    "Reinforcement learning is used to teach computer plays chess or Go game.\n",
    "\n",
    "In fact computer learns itself. \n",
    "\n",
    "The learning process starts and the model make moves for one and another side.\n",
    "\n",
    "The training software estimates its moves using known functions characterizing quality of a game configuration.\n",
    "\n",
    "Using this estimates the model parameters are updated.\n",
    "\n",
    "After sufficiently many games the model becomes a grandmaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1\\. Answer in writing what is a model and what is a machine learning model\n",
    "\n",
    "2\\. Write brief descriptions of three types of model learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and underfitting\n",
    "\n",
    "A standard problem in machine learning is overfitting. \n",
    "\n",
    "The model with overfitting performs very well on the train data but fails on any new data.\n",
    "\n",
    "Overfitting means that instead of finding generalizing of the train data it merely has \n",
    "remembered all of them. Also it can involve learning noise in the data.\n",
    "\n",
    "The opposite problem is underfitting.\n",
    "\n",
    "A model with underfitting performs bad even on training data. \n",
    "\n",
    "Overfitting usually appears when the model is too complex\n",
    "for a given dataset, i.e., has too many parameters. \n",
    "\n",
    "Underfitting on the contrary means that either the model is too simple. Number of \n",
    "its degrees of freedom (parameters) is not enough to store the extracted information. \n",
    "\n",
    "Another reason is that the dataset have not enough features (columns) and does not describe the solved problem well.\n",
    "\n",
    "Fighting underfitting and overfitting is the central problem of creation of an appropriate machine learning model.\n",
    "\n",
    "When the model is underfitted we need to add more capacity to it. \n",
    "\n",
    "It means that its structure must be changed to add more parameters. \n",
    "\n",
    "We can add more structure elements to the model (e.g., more layers to a neural network), or we can collect \n",
    "more features from the domain of interest. \n",
    "\n",
    "A simple trick is to add new data columns as powers of the old ones: squares, cubes and so on. \n",
    "\n",
    "And if the overfitting occurs we can either simplify the mode by decreasing the number of its parameter, or we can try to \n",
    "find more data records (often this is impossible) to enlarge the training dataset.\n",
    "\n",
    "One more way to fight the overfitting is called regularization.\n",
    "\n",
    "Regularization means limiting parameter variation. \n",
    "\n",
    "Overfitted models often have very large by magnitude parameters.\n",
    "\n",
    "When we apply the regularization we add a penalty to the parameter magnitude: the smaller the better.\n",
    "\n",
    "And one more way do defeat the overfitting is called dropout.\n",
    "\n",
    "In the course of training we switch off at random some model parameters, i.e., exclude them from the training routine.\n",
    "\n",
    "As we can see all ways of fighting the overfitting are directed to prohibiting of the mere remembering the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Split dataset to train, validation and test parts\n",
    "\n",
    "The underfitting is clearly seen: models just works bad.\n",
    "\n",
    "And the overfitting can not be detected considering only the training dataset. \n",
    "\n",
    "To revel the overfitting we need to split the dataset to training and testing parts.\n",
    "\n",
    "But here we have another problem.\n",
    "\n",
    "Imagine that we have done several training steps and want to see how it is going.\n",
    "\n",
    "We take the test part of the data and estimate the model performance. \n",
    "\n",
    "If it is as good as for the training data everything is good, there is no overfitting.\n",
    "\n",
    "But if the models performs bad on the testing data we detect the overfitting.\n",
    "\n",
    "In this case we change something in the model.\n",
    "\n",
    "It means that the test data also take part in training. \n",
    "\n",
    "They influences although indirectly on the modification to the models.\n",
    "\n",
    "But the final score of the model can be estimated only for the data than are never seen by the model and that\n",
    "never influenced the model training.\n",
    "\n",
    "It means that the splitting the initial dataset into training and testing parts is not enough. \n",
    "\n",
    "We must split the dataset into three parts, training, validation and test:\n",
    "- training data are immediately used to update model parameters \n",
    "- validation data are used after several steps of training to estimate the performance and overfitting of the model; on the basis of this estimation corrections to training can be done\n",
    "- test data are used just once to get the final score of the model; no model parameter updates cab be done after that.\n",
    "\n",
    "Usually the dataset is split as 70\\%, 20\\%, 10\\% or 80\\%, 10\\%, 10\\%. \n",
    "\n",
    "The training part is usually larger than all others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "If the size of the whole dataset is not too large, we can improve the training by using cross-validation.\n",
    "\n",
    "First we take off the test data to find the final model score.\n",
    "\n",
    "The rest of the data is not split to the training and validation data permanently. \n",
    "\n",
    "Instead we split all remaining data into $k$ parts. First $k-1$ parts are used for the model training and then\n",
    "the performance is checked using the last one.\n",
    "\n",
    "Then we rotate the data so that the next part becomes the testing set. \n",
    "\n",
    "The procedure is repeated $k$ times and the result of tests are averaged.\n",
    "\n",
    "Such rolling estimate allows more uniform use of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient decent, epoch\n",
    "\n",
    "Often the dataset is very large and computer has not enough memory to keep it all at once.\n",
    "\n",
    "In this case the dataset is divided by pieces called minibatches or just batches.\n",
    "\n",
    "Instead of applying the training algorithm to the whole dataset it is applied at minibatches and model parameters are updated after each minibatch. \n",
    "\n",
    "Usually this is used when the parameters are optimized using gradient decent optimization method. \n",
    "\n",
    "Gradient decent applied at minibatches is called stochastic gradient decent.\n",
    "\n",
    "It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). \n",
    "\n",
    "Especially in high-dimensional optimization problems this reduces the computational burden, achieving faster iterations in trade for a lower convergence rate.\n",
    "\n",
    "When the dataset is split to mimibatches and we have fed all of them one by one to the training routine this is called epoch.\n",
    "\n",
    "In the other words epoch passes when we show all minibatches, i.e., the whole training dataset, to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression and classification\n",
    "\n",
    "Supervised machine learning usually solves two main problem:\n",
    "\n",
    "- regression,\n",
    "- classification.\n",
    "\n",
    "Classification is the task of assigning labels to data samples belonging to different classes. For example training a model to distinguish between cats and dogs is a classification problem with cats and dogs being the two classes.\n",
    "\n",
    "Regression, on the other hand, is the task of predicting continuous values by learning from various independent features. For example predicting the price of a house based on features like the number of bedrooms, locality etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for regression\n",
    "\n",
    "Performance of machine learning models are estimated with metrics.\n",
    "\n",
    "In regression tasks we have a true vector $\\hat y$ and a vector predicted by a model $y$.\n",
    "\n",
    "Metrics tell how different they are.\n",
    "\n",
    "Usually some versions of the distance is used.\n",
    "\n",
    "Assume the vectors $\\hat y$ and $y$ has $n$ elements.\n",
    "\n",
    "For example the following vectors have $n=5$:\n",
    "$$\n",
    "\\hat y = (2.1, 3.4, -5.1, 1.6, -3.3)\n",
    "$$\n",
    "$$\n",
    "y = (2.2, 3.45, -5.0, 1.7, -3.2)\n",
    "$$\n",
    "\n",
    "Let us denote vector elements as $\\hat y_i$ and $y_i$, where $i=1,2,\\ldots, n$.\n",
    "\n",
    "For the example above $y_1=2.2$, $y_2=3.45$.\n",
    "\n",
    "__MSE, Mean Squared Error__, the most often used metric:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n (\\hat y_i - y_i)^2\n",
    "$$\n",
    "This is the squared Euclidean distance between two vectors divided by $n$.\n",
    "\n",
    "\n",
    "__RMSE, Root Mean Squared Error__. This is the square root of MSE, the Euclidean distance divided by $\\sqrt{n}$.\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (\\hat y_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "__MAE, Mean Absolute Error__.\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^n |\\hat y_i - y_i|\n",
    "$$\n",
    "This is the taxi-cab distance divided by $n$.\n",
    "\n",
    "The difference between these metrics in different scale for small and large errors.\n",
    "\n",
    "For example MSE squares the error so that it assigns more penalty to large errors in comparison with RMSE.\n",
    "\n",
    "And when errors are small RMSE put more penalty then MSE.\n",
    "\n",
    "MAE unlike those two gives uniform penalties to small and large errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for classification\n",
    "\n",
    "When a model performs classification must return a class label.\n",
    "\n",
    "But often the actual output of the model is a vector $y$ of $n$ elements where $n$ is \n",
    "a number of classes.\n",
    "\n",
    "To obtain a class label this vector is normalized. Often so called softmax function is used:\n",
    "$$\n",
    "q_i = \\frac{e^{y_i}}{\\sum_{i=1}^n e^{y_i}}\n",
    "$$\n",
    "\n",
    "After the normalization all vector elements are positive and the their sum equals to 1. If one of the elements approaches 1 all others go to zeros.\n",
    "\n",
    "The predicted class label corresponds to the vector entry with the larges value after the normalization.\n",
    "\n",
    "To estimate how good is the prediction this vector is compared with the desired prediction.\n",
    "\n",
    "For this purpose the desired class labels are represented in one-hot form:\n",
    "$$\n",
    "p=(1,0,0,0,0) \\text{ : class 1}\n",
    "$$\n",
    "$$\n",
    "p=(0,1,0,0,0) \\text{ : class 2}\n",
    "$$\n",
    "$$\n",
    "p=(0,0,1,0,0) \\text{ : class 3}\n",
    "$$\n",
    "and so on. \n",
    "\n",
    "Then the model prediction $q$ is compared with the desired one-hot output via \n",
    "__cross entropy__:\n",
    "$$\n",
    "H(p,q) = - \\sum_{i=1}^n p_i \\log q_i\n",
    "$$\n",
    "\n",
    "This cross entropy will be very high if the model predicts wrong class (wrong vector entry is the largest) and becomes very small when the correct entry of the predicted vector is much higher then the others. \n",
    "\n",
    "In case of binary classification the model returns only one real value $y$. It\n",
    "must be first of all fitted into the range $[0,1]$.\n",
    "\n",
    "Usually the sigmoid function $\\sigma(y)$ is used:\n",
    "$$\n",
    "q = \\sigma(y) = \\frac{1}{1+e^{-y}}\n",
    "$$\n",
    "The output of this function belongs to the range $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "xs = np.linspace(-5, 5, 100)\n",
    "ys = 1 / (1 + np.exp(-xs))  # The sigmoid function is computed here\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xs, ys)\n",
    "ax.grid()\n",
    "ax.set_title(\"Sigmod function\")\n",
    "ax.set_xlabel(\"y\")\n",
    "ax.set_ylabel(\"s\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels, i.e., the desired outputs $p$ in the binary classification \n",
    "are either 0 or 1.\n",
    "\n",
    "Thus, given the desired label $p$ and the predicted $q$ \n",
    "a __binary cross entropy__ is computed:\n",
    "$$\n",
    "H(p,q) = -p \\log q - (1-p) \\log (1-q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for binary classification: accuracy, precision and recall. Matthews Correlation Coefficient\n",
    "\n",
    "Dealing with a classification problem it seems at first glance obvious\n",
    "to estimate the performance via accuracy, a fraction of percentage of correct predictions.\n",
    "\n",
    "But in fact it this not a good idea when the data is not balanced, i.e., \n",
    "when categories appear in the data with different probabilities.\n",
    "\n",
    "Let us for the sake of simplicity consider binary classification: \n",
    "is this email spam or not, is this person diseased or not, will be a rain tomorrow or not and so on.\n",
    "\n",
    "There are two types of predictions: positive and negative. And each one can be true or false \n",
    "We know it because we consider a supervised learning and have ground true labels.\n",
    "\n",
    "- True positive: model says that this message is spam and this indeed spam.\n",
    "- False positive: model says that this is spam, but this is actually not spam. Type I error, wrong discovery.\n",
    "- False negative: model says that the message is not spam, but this is incorrect. Type II error, missed discovery.\n",
    "- True negative: model says that the message is not spam and the message is indeed not spam.\n",
    "\n",
    "All these cases can be collect into a table that is called confusion matrix.\n",
    "\n",
    "|                    | Actually spam                           | Actually not spam                      |\n",
    "|--------------------|-----------------------------------------|----------------------------------------|\n",
    "| Predicted spam     | TP (number of <br>true positive cases)  | FP (number of<br>false positive cases) |\n",
    "| Predicted not spam | FN (number of <br>false negative cases) | TN (number of <br>true negative cases) |\n",
    "\n",
    "Let us now create a spam filter that will have 90% accuracy. It means its 90% predictions will be correct.\n",
    "\n",
    "The filter will produce its predictions at random. \n",
    "\n",
    "In most cases it will mark messages as non-spam and\n",
    "only with the probability $p=0.001$ it will report spam. \n",
    "\n",
    "Let us model its operation. First let us create messages. \n",
    "\n",
    "Assume one spam message appear at each 10 messages. Our dataset will contain $10^6$ messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# One spam message (marked as 1) for each 10 messages (marked as 0), 1000000 totqly\n",
    "messages = ([1] + [0] * 9) * 100000\n",
    "\n",
    "# Shuffle spam and non-spam messages\n",
    "rng.shuffle(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the filter. It just generates 1 (means spam is predicted) with small probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_spam_filter = 0.001\n",
    "\n",
    "# Do predicitons without even looking at messages\n",
    "predicts = [1 if rng.random() < p_spam_filter else 0 for _ in range(len(messages))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of our filter: run along the messages and predictions in parallel and put 1 if they coincide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put 1 when precition is correct\n",
    "success = [1 if m == p else 0 for m, p in zip(messages, predicts)]\n",
    "\n",
    "accuracy = sum(success) / len(messages)\n",
    "print(f\"accuracy={accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random filter predicts correct results in 90% cases.\n",
    "\n",
    "Why? \n",
    "\n",
    "The message list contains many zeros (non-spam messages). And the spam filter basically reports \n",
    "zeros (prefer to predict non-spam).\n",
    "\n",
    "These zeros coincide very often.\n",
    "\n",
    "To have more appropriate metric, let us compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def report_result(mes, pred):\n",
    "    if mes == 1:\n",
    "        if pred == 1:\n",
    "            return 'tp'  # True positive prediction\n",
    "        else:\n",
    "            return 'fn'  # False negative\n",
    "    else:\n",
    "        if pred == 1:\n",
    "            return 'fp'  # False positive\n",
    "        else:\n",
    "            return 'tn'  # True negative\n",
    "\n",
    "check = [report_result(m, p) for m, p in zip(messages, predicts)]\n",
    "\n",
    "# Count results of predictions\n",
    "cm = Counter(check)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(f\"tp={cm['tp']:10}\", f\"  fp={cm['fp']:10}\")\n",
    "print(f\"fn={cm['fn']:10}\", f\"  tn={cm['tn']:10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are extremely many true negative predictions - as we mentioned above both messages \n",
    "and predictions have many zeros and they coincide very often.\n",
    "\n",
    "It means the value TN must be excluded since it is high just by chance, due to highly unbalanced data.\n",
    "\n",
    "Instead of accuracy precision and recall as well as their combination is considered.\n",
    "\n",
    "Precision is a fraction of true positive predictions in the total number of positive predictions.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "Recall is a fraction of true positive predictions in the total number of positive cases\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "Given these two values their harmonic mean is computed that is called F1-score:\n",
    "\n",
    "$$\n",
    "\\text{F1-score} = 2\\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Let us compute these metrics for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = cm['tp'] / (cm['tp'] + cm['fp'])\n",
    "recall = cm['tp'] / (cm['tp'] + cm['fn'])\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"precision={precision:.3f}\")\n",
    "print(f\"recall={recall:.3f}\")\n",
    "print(f\"f1_score={f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all three values are very small that corresponds to our intuition that our spam filter is not so good as indicates the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases precision or recall can be more important than the other one.\n",
    "\n",
    "Imagine, for example, that a classifier needs to detect some disease in human patients. \n",
    "\n",
    "Positive means that the patient has the disease, and negative means the patient is healthy. \n",
    "\n",
    "In this case, the recall is more important because we need that the classifier revel as many truly diseased patients as possible.\n",
    "\n",
    "Another example is a recommendation system. The classifier prediction is considered positive when the recommendation is relevant and negative for non-relevant recommendations. \n",
    "\n",
    "In this case we need high precision: most of positive recommendations are relevant.\n",
    "\n",
    "To summarize, the relative importance assigned to precision and recall should be an aspect of the problem. \n",
    "\n",
    "Classifying a sick person as healthy has a different cost from classifying a healthy person as sick.\n",
    "\n",
    "That is why F1-score as a single characteristic of a classifier should be used with care: it assigns identical weights both to recall and precision.\n",
    "\n",
    "One of the possible solution is to use the generalized F-score (also called Fbeta-score) that has an additional parameter that changes the relative weights of the precision and recall\n",
    "\n",
    "$$\n",
    "\\text{F-score} = (1+\\beta^2)\\frac{\\text{Precision} \\times \\text{Recall}}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "At $\\beta=1$ the general F-score becomes F1-score.\n",
    "\n",
    "A smaller beta value, such as 0.5, gives more weight to precision and less to recall, whereas a larger beta value, such as 2.0, gives less weight to precision and more weight to recall in the calculation of the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three considered metrics have some issues. Accuracy works bad for imbalanced data. Precision and recall are asymmetric, i.e., ignores some information. F1-score combines them but it is unclear what weights to assign to each one of them.\n",
    "\n",
    "In some cases as mentioned above precision or recall are enough. \n",
    "\n",
    "But what if both classes are of interest and true predictions for both are very important? There is one way to combine results of binary classification.\n",
    "\n",
    "This is based on the idea that the true class and the predicted class can be treated as two (binary) variables. \n",
    "\n",
    "The measure of quality of prediction is their correlation coefficient. \n",
    "\n",
    "Previously we considered Pearson correlation coefficient. But we analyzed time series. Now we have two random binary variables and TP, TN, FP, FN are their probabilities. \n",
    "\n",
    "The higher the correlation between true and predicted values, the better the prediction. \n",
    "\n",
    "This is called phi-coefficient or Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MCC=\\frac{TP\\times TN-FP\\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the classifier is perfect (FP = FN = 0) the value of MCC is 1, indicating perfect positive correlation. \n",
    "\n",
    "Conversely, when the classifier always misclassifies (TP = TN = 0), we get a value of -1, representing perfect negative correlation. \n",
    "\n",
    "In this case, you we simply reverse the classifier’s outcome to get the ideal classifier. \n",
    "\n",
    "In fact, MCC value is always between -1 and 1, with 0 meaning that the classifier is no better than a random flip of a fair coin. \n",
    "\n",
    "MCC is also perfectly symmetric, so no class is more important than the other; if we switch the positive and negative, we will still get the same value.\n",
    "\n",
    "MCC takes into account all four values in the confusion matrix, and a high value (close to 1) means that both classes are predicted well, even if one class is disproportionately under- (or over-) represented.\n",
    "\n",
    "Although the equation for MCC is rather simple its straightforward computation will probably fail.\n",
    "\n",
    "Observe a lot of multiplications in denominator and in numerator. They will be really huge after that! \n",
    "\n",
    "But after their division we will have a reasonable value. \n",
    "\n",
    "Thus to compute this value first we need to rescale properly the denominator and numerator. Due to this reason we will use out-of-box routine from sklearn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "MCC = matthews_corrcoef(messages, predicts)\n",
    "print(f\"MCC={MCC:10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that MCC estimates our stupid spam classifier as a very very bad. \n",
    "\n",
    "As mentioned above the very close to zero value indicates that the classifiers is just a tossing a coin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of model training, underfitting and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before considering the example let us discuss some mathematical concepts.\n",
    "\n",
    "The polynomial is a construct that looks as follows:\n",
    "\n",
    "$$\n",
    "y = a_0 + a_1 x^1 + a_2 x^2 + a_3 x^3 + \\ldots\n",
    "$$\n",
    "\n",
    "The largest exponents is called a degree of the polynomial:\n",
    "\n",
    "$$\n",
    "y = 2 \\text{ :  degree 0}\n",
    "$$\n",
    "$$\n",
    "y = 3-4x \\text{ :  degree 1}\n",
    "$$\n",
    "$$\n",
    "y = 6+2x+3x^2 \\text{ :  degree 2}\n",
    "$$\n",
    "\n",
    "A polynomial can be considered as a function. One can substitute a value of $x$ there and obtain the corresponding $y$.\n",
    "\n",
    "For example, consider the polynomial of degree 2.\n",
    "$$\n",
    "y = 2 + x^2\n",
    "$$\n",
    "\n",
    "If $x=2$ the corresponding $y$ is 6, if $x=4$, $y=18$ and so on.\n",
    "\n",
    "The polynomials can be plotted as functions.\n",
    "\n",
    "In `numpy` there is a submodule `polynomial` that provides a class `Polynomial` for working with polynomials.\n",
    "\n",
    "Let us consider the polynomial of the 4th degree\n",
    "\n",
    "$$\n",
    "y = 4 + 2 x - 4 x^2 -2 x^3 + 5 x^4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.polynomial import Polynomial as P\n",
    "\n",
    "# We create a polynomial by passing to the class constructior P its coefficients\n",
    "poly1 = P([4, 2, -4, -2, 5])\n",
    "\n",
    "# Print the created polynomial\n",
    "print(\"y =\", poly1)\n",
    "print(\"degree =\", poly1.degree())\n",
    "\n",
    "# Check how it works: create list of x values and conpute the corresponding y\n",
    "x = np.array([-3.8, 4.4, 0.55, 1])\n",
    "y = poly1(x)\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot its graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This functions runs x in the most appropriate rannge and compute the corresponding y\n",
    "xp, yp = poly1.linspace()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xp, yp)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\");"
   ]
  },
  {
   "attachments": {
    "poly_fit.svg": {
     "image/svg+xml": [
      "<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg
   xmlns:osb="http://www.openswatchbook.org/uri/2009/osb"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="167.40735mm"
   height="50.090759mm"
   viewBox="0 0 167.40735 50.090759"
   version="1.1"
   id="svg8"
   inkscape:version="1.0.2 (e86c870879, 2021-01-15)"
   sodipodi:docname="poly_fit.svg">
  <defs
     id="defs2">
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4160"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4158"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4156"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4154"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4152"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4150"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4148"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4146"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4144"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4142"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4140"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4138"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect4136"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2274"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2255"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2236"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2217"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2192"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect2086"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1963"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1890"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1825"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <linearGradient
       id="linearGradient3347"
       osb:paint="solid">
      <stop
         style="stop-color:#000000;stop-opacity:1;"
         offset="0"
         id="stop3345" />
    </linearGradient>
    <linearGradient
       id="linearGradient3341"
       osb:paint="solid">
      <stop
         style="stop-color:#cccccc;stop-opacity:1;"
         offset="0"
         id="stop3339" />
    </linearGradient>
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1241"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1237"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1233"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1229"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1225"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1219"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1211"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1147"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect1143"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <marker
       style="overflow:visible"
       id="marker1121"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="Arrow1Lstart"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,10,0)"
         style="fill:#000000;fill-opacity:1;fill-rule:evenodd;stroke:#000000;stroke-width:1pt;stroke-opacity:1"
         d="M 0,0 5,-5 -12.5,0 5,5 Z"
         id="path1119" />
    </marker>
    <marker
       style="overflow:visible"
       id="Arrow1Lstart"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="Arrow1Lstart"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,10,0)"
         style="fill:#000000;fill-opacity:1;fill-rule:evenodd;stroke:#000000;stroke-width:1pt;stroke-opacity:1"
         d="M 0,0 5,-5 -12.5,0 5,5 Z"
         id="path843" />
    </marker>
    <inkscape:path-effect
       effect="bspline"
       id="path-effect841"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect837"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <pattern
       patternUnits="userSpaceOnUse"
       width="15.345923"
       height="10.518884"
       patternTransform="translate(154.82704,139.74056)"
       id="pattern3334">
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.623622;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         d="M 0.17296163,10.259441 15.172962,0.25944247"
         id="path3321" />
    </pattern>
    <marker
       style="overflow:visible"
       id="marker3345"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343" />
    </marker>
    <inkscape:path-effect
       effect="bspline"
       id="path-effect3341"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <inkscape:path-effect
       effect="bspline"
       id="path-effect3331"
       is_visible="true"
       lpeversion="1"
       weight="33.333333"
       steps="2"
       helper_size="0"
       apply_no_weight="true"
       apply_with_weight="true"
       only_selected="false" />
    <marker
       style="overflow:visible"
       id="marker3345-3"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-6" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker3345-5"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-3" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker3345-6"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-2" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker3345-6-1"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-2-2" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker1121-2"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="Arrow1Lstart"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,10,0)"
         style="fill:#000000;fill-opacity:1;fill-rule:evenodd;stroke:#000000;stroke-width:1pt;stroke-opacity:1"
         d="M 0,0 5,-5 -12.5,0 5,5 Z"
         id="path1119-9" />
    </marker>
    <marker
       style="overflow:visible"
       id="Arrow1Lstart-1"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="Arrow1Lstart"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,10,0)"
         style="fill:#000000;fill-opacity:1;fill-rule:evenodd;stroke:#000000;stroke-width:1pt;stroke-opacity:1"
         d="M 0,0 5,-5 -12.5,0 5,5 Z"
         id="path843-2" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker3345-7"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-0" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker3345-5-9"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-3-3" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker3345-3-6"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-6-0" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker3345-6-1-6"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-2-2-2" />
    </marker>
    <marker
       style="overflow:visible"
       id="marker3345-6-6"
       refX="0"
       refY="0"
       orient="auto"
       inkscape:stockid="DotL"
       inkscape:isstock="true">
      <path
         transform="matrix(0.8,0,0,0.8,5.92,0.8)"
         style="fill:#ff0000;fill-opacity:1;fill-rule:evenodd;stroke:#ff0000;stroke-width:1pt;stroke-opacity:1"
         d="m -2.5,-1 c 0,2.76 -2.24,5 -5,5 -2.76,0 -5,-2.24 -5,-5 0,-2.76 2.24,-5 5,-5 2.76,0 5,2.24 5,5 z"
         id="path3343-2-1" />
    </marker>
  </defs>
  <sodipodi:namedview
     id="base"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageopacity="0.0"
     inkscape:pageshadow="2"
     inkscape:zoom="2.8"
     inkscape:cx="386.9721"
     inkscape:cy="103.11848"
     inkscape:document-units="mm"
     inkscape:current-layer="layer1"
     inkscape:document-rotation="0"
     showgrid="true"
     inkscape:window-width="1920"
     inkscape:window-height="1011"
     inkscape:window-x="0"
     inkscape:window-y="32"
     inkscape:window-maximized="0"
     inkscape:snap-global="false">
    <inkscape:grid
       type="xygrid"
       id="grid833"
       originx="6.3636267"
       originy="-25.065246" />
  </sodipodi:namedview>
  <metadata
     id="metadata5">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title />
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     inkscape:label="Слой 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(6.3636273,-25.065248)">
    <path
       style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-5-9)"
       d="m 113.77083,60.033214 c 0.0712,0.19883 0.17151,0.479103 0.25082,0.700656"
       id="path3339-5-2"
       sodipodi:nodetypes="cc"
       inkscape:original-d="m 113.77083,60.033214 c 0.0713,0.198786 0.17166,0.479049 0.25082,0.700656"
       inkscape:path-effect="#path-effect4152" />
    <g
       id="g3696"
       transform="translate(0,0.07473737)">
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.293112px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;marker-start:url(#marker1121)"
         d="m -4.875133,25.559548 c 0,15.875215 0,31.750216 0,47.624998"
         id="path835"
         inkscape:path-effect="#path-effect837"
         inkscape:original-d="m -4.875133,25.559548 c 3.969e-4,15.875215 3.969e-4,31.750216 0,47.624998" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.3735;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#Arrow1Lstart)"
         d="m 72.515492,73.184548 c -25.797225,0 -51.594103,0 -77.3906241,0"
         id="path839"
         inkscape:path-effect="#path-effect841"
         inkscape:original-d="m 72.515492,73.184548 c -25.797225,3.96e-4 -51.594103,3.96e-4 -77.3906241,0" />
      <text
         xml:space="preserve"
         style="font-size:4.93889px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:none;stroke-width:0.3975;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
         x="70.531113"
         y="69.215797"
         id="text1185"><tspan
           sodipodi:role="line"
           id="tspan1183"
           x="70.531113"
           y="69.215797"
           style="font-style:italic;font-size:4.93889px;fill:#000000;stroke:none;stroke-width:0.3975;stroke-miterlimit:4;stroke-dasharray:none">x<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:65%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub"
   id="tspan911">1</tspan></tspan></text>
      <text
         xml:space="preserve"
         style="font-size:4.93889px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396874px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
         x="-1.0325345"
         y="28.138975"
         id="text1189"><tspan
           sodipodi:role="line"
           id="tspan1187"
           x="-1.0325345"
           y="28.138975"
           style="font-style:italic;font-size:4.93889px;fill:#000000;stroke:none;stroke-width:0.396874px">x<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:65%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub"
   id="tspan913">2</tspan></tspan></text>
      <g
         id="g2521"
         transform="translate(-2.9409167,1.7501433)">
        <path
           style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.265;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
           d="m 5.2916663,54.239583 c 0.8091454,1.725928 1.6181412,3.451537 2.4269884,5.176829"
           id="path2190"
           inkscape:path-effect="#path-effect2192"
           inkscape:original-d="m 5.2916663,54.239583 c 0.8092605,1.725874 1.6182566,3.451483 2.4269884,5.176829"
           sodipodi:nodetypes="cc" />
        <path
           style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
           d="m 27.269659,51.616216 c 1.052627,2.197691 2.105101,4.395062 3.157424,6.592117"
           id="path2190-0"
           inkscape:path-effect="#path-effect2217"
           inkscape:original-d="m 27.269659,51.616216 c 1.052739,2.197637 2.105213,4.395008 3.157424,6.592117"
           sodipodi:nodetypes="cc" />
        <path
           style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
           d="m 39.230561,46.768788 c 1.034418,2.04961 2.068675,4.098902 3.102773,6.147879"
           id="path2190-0-9"
           inkscape:path-effect="#path-effect2236"
           inkscape:original-d="m 39.230561,46.768788 c 1.034522,2.049557 2.068779,4.09885 3.102773,6.147879"
           sodipodi:nodetypes="cc" />
        <path
           style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
           d="m 43.656249,35.71875 c 1.17413,2.47883 2.34811,4.957342 3.521939,7.435537"
           id="path2190-0-9-3"
           inkscape:path-effect="#path-effect2255"
           inkscape:original-d="m 43.656249,35.71875 c 1.174244,2.478776 2.348224,4.957288 3.521939,7.435537"
           sodipodi:nodetypes="cc" />
        <path
           style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
           d="m 63.795083,36.61444 c 0.783715,1.906617 1.567298,3.812915 2.350751,5.718894"
           id="path2190-0-9-3-6"
           inkscape:path-effect="#path-effect2274"
           inkscape:original-d="m 63.795083,36.61444 c 0.783848,1.906562 1.567432,3.81286 2.350751,5.718894"
           sodipodi:nodetypes="cc" />
        <text
           xml:space="preserve"
           style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
           x="7.7171559"
           y="54.701439"
           id="text1189-0"><tspan
             sodipodi:role="line"
             id="tspan1187-6"
             x="7.7171559"
             y="54.701439"
             style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2">1</tspan></tspan></text>
        <text
           xml:space="preserve"
           style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
           x="22.866385"
           y="58.445599"
           id="text1189-0-6"><tspan
             sodipodi:role="line"
             id="tspan1187-6-1"
             x="22.866385"
             y="58.445599"
             style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8">2</tspan></tspan></text>
        <text
           xml:space="preserve"
           style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
           x="43.640316"
           y="50.856781"
           id="text1189-0-6-7"><tspan
             sodipodi:role="line"
             id="tspan1187-6-1-9"
             x="43.640316"
             y="50.856781"
             style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8-2">3</tspan></tspan></text>
        <text
           xml:space="preserve"
           style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
           x="47.071804"
           y="38.854546"
           id="text1189-0-6-7-0"><tspan
             sodipodi:role="line"
             id="tspan1187-6-1-9-2"
             x="47.071804"
             y="38.854546"
             style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8-2-3">4</tspan></tspan></text>
        <text
           xml:space="preserve"
           style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
           x="67.452507"
           y="40.015141"
           id="text1189-0-6-7-0-7"><tspan
             sodipodi:role="line"
             id="tspan1187-6-1-9-2-5"
             x="67.452507"
             y="40.015141"
             style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8-2-3-9">5</tspan></tspan></text>
        <path
           style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345)"
           d="m 5.2916663,54.239583 c 0.097002,0.175847 0.2337022,0.423661 0.341749,0.619531"
           id="path3339"
           inkscape:path-effect="#path-effect3341"
           inkscape:original-d="m 5.2916663,54.239583 c 0.09709,0.175798 0.233793,0.423611 0.341749,0.619531"
           sodipodi:nodetypes="cc" />
        <path
           style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-5)"
           d="m 30.427083,58.208333 c 0.097,0.175847 0.233702,0.423661 0.341749,0.619531"
           id="path3339-5"
           inkscape:path-effect="#path-effect1890"
           inkscape:original-d="m 30.427083,58.208333 c 0.09709,0.175798 0.233793,0.423611 0.341749,0.619531"
           sodipodi:nodetypes="cc" />
        <path
           style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-3)"
           d="m 42.333333,52.916667 c 0.097,0.175847 0.233702,0.423661 0.341749,0.619531"
           id="path3339-7"
           inkscape:path-effect="#path-effect1825"
           inkscape:original-d="m 42.333333,52.916667 c 0.09709,0.175798 0.233793,0.423611 0.341749,0.619531"
           sodipodi:nodetypes="cc" />
        <path
           style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-6-1)"
           d="m 43.65625,35.71875 c 0.097,0.175847 0.233702,0.423661 0.341749,0.619531"
           id="path3339-9-7"
           inkscape:path-effect="#path-effect2086"
           inkscape:original-d="m 43.65625,35.71875 c 0.09709,0.175798 0.233793,0.423611 0.341749,0.619531"
           sodipodi:nodetypes="cc" />
        <path
           style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-6)"
           d="m 66.145833,42.333333 c 0.097,0.175847 0.233702,0.423661 0.341749,0.619531"
           id="path3339-9"
           inkscape:path-effect="#path-effect1963"
           inkscape:original-d="m 66.145833,42.333333 c 0.09709,0.175798 0.233793,0.423611 0.341749,0.619531"
           sodipodi:nodetypes="cc" />
        <path
           style="fill:#0000ff;fill-rule:evenodd;stroke:#0000ff;stroke-width:0.524999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"
           d="M 68.791666,34.395833 C 47.179465,43.217139 25.571825,52.036584 3.9687492,60.854166"
           id="path3329"
           inkscape:path-effect="#path-effect3331"
           inkscape:original-d="M 68.791666,34.395833 C 47.179595,43.217458 25.571955,52.036904 3.9687492,60.854166"
           sodipodi:nodetypes="cc" />
      </g>
      <text
         xml:space="preserve"
         style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
         x="23.468565"
         y="29.878925"
         id="text1189-3"><tspan
           sodipodi:role="line"
           id="tspan1187-5"
           x="23.468565"
           y="29.878925"
           style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">y=a<tspan
   style="font-style:normal;font-size:65%;baseline-shift:sub"
   id="tspan3609">0</tspan>+a<tspan
   style="font-style:normal;font-size:65%;baseline-shift:sub"
   id="tspan3607">1</tspan>x</tspan></text>
    </g>
    <path
       style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.293112px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;marker-start:url(#marker1121-2)"
       d="m 81.409534,25.634285 c 0,15.875215 0,31.750216 0,47.624998"
       id="path835-7"
       inkscape:path-effect="#path-effect4136"
       inkscape:original-d="m 81.409534,25.634285 c 3.97e-4,15.875215 3.97e-4,31.750216 0,47.624998" />
    <path
       style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.3735;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#Arrow1Lstart-1)"
       d="m 158.80016,73.259285 c -25.79723,0 -51.5941,0 -77.390625,0"
       id="path839-9"
       inkscape:path-effect="#path-effect4138"
       inkscape:original-d="m 158.80016,73.259285 c -25.79723,3.96e-4 -51.5941,3.96e-4 -77.390625,0" />
    <text
       xml:space="preserve"
       style="font-size:4.93889px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:none;stroke-width:0.3975;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1"
       x="156.81578"
       y="69.290535"
       id="text1185-2"><tspan
         sodipodi:role="line"
         id="tspan1183-0"
         x="156.81578"
         y="69.290535"
         style="font-style:italic;font-size:4.93889px;fill:#000000;stroke:none;stroke-width:0.3975;stroke-miterlimit:4;stroke-dasharray:none">x<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:65%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub"
   id="tspan911-2">1</tspan></tspan></text>
    <text
       xml:space="preserve"
       style="font-size:4.93889px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396874px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       x="85.252136"
       y="28.213713"
       id="text1189-37"><tspan
         sodipodi:role="line"
         id="tspan1187-59"
         x="85.252136"
         y="28.213713"
         style="font-style:italic;font-size:4.93889px;fill:#000000;stroke:none;stroke-width:0.396874px">x<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:65%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub"
   id="tspan913-22">2</tspan></tspan></text>
    <path
       style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.265;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
       d="m 88.635416,56.064464 c 0.460291,3.072526 0.920536,6.144754 1.380738,9.216686"
       id="path2190-9"
       inkscape:path-effect="#path-effect4140"
       inkscape:original-d="m 88.635416,56.064464 c 0.460511,3.072493 0.920757,6.144721 1.380738,9.216686"
       sodipodi:nodetypes="cc" />
    <path
       style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
       d="m 114.41972,62.373562 c -0.21621,-0.779813 -0.43251,-1.559927 -0.64889,-2.340348"
       id="path2190-0-7"
       inkscape:path-effect="#path-effect4142"
       inkscape:original-d="m 114.41972,62.373562 c -0.21604,-0.779861 -0.43233,-1.559977 -0.64889,-2.340348"
       sodipodi:nodetypes="cc" />
    <path
       style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
       d="m 127.65337,57.062436 c -1.19903,-1.890256 -2.39826,-3.780835 -3.59767,-5.671704"
       id="path2190-0-9-36"
       inkscape:path-effect="#path-effect4144"
       inkscape:original-d="m 127.65337,57.062436 c -1.19896,-1.890298 -2.39819,-3.780876 -3.59767,-5.671704"
       sodipodi:nodetypes="cc" />
    <path
       style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
       d="m 127,37.543631 c 3.71816,3.795984 7.43606,7.591705 11.1537,11.387163"
       id="path2190-0-9-3-1"
       inkscape:path-effect="#path-effect4146"
       inkscape:original-d="m 127,37.543631 c 3.71816,3.795981 7.43606,7.591702 11.1537,11.387163"
       sodipodi:nodetypes="cc" />
    <path
       style="fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:0.53, 0.53;stroke-dashoffset:0;stroke-opacity:1"
       d="m 145.16312,40.737705 c 1.44245,1.140402 2.8846,2.280569 4.32646,3.42051"
       id="path2190-0-9-3-6-2"
       inkscape:path-effect="#path-effect4148"
       inkscape:original-d="m 145.16312,40.737705 c 1.44242,1.140437 2.88457,2.280602 4.32646,3.42051"
       sodipodi:nodetypes="cc" />
    <text
       xml:space="preserve"
       style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       x="91.720612"
       y="60.187775"
       id="text1189-0-9"><tspan
         sodipodi:role="line"
         id="tspan1187-6-3"
         x="91.720612"
         y="60.187775"
         style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-1">1</tspan></tspan></text>
    <text
       xml:space="preserve"
       style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       x="105.97694"
       y="60.49614"
       id="text1189-0-6-9"><tspan
         sodipodi:role="line"
         id="tspan1187-6-1-4"
         x="105.97694"
         y="60.49614"
         style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8-7">2</tspan></tspan></text>
    <text
       xml:space="preserve"
       style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       x="127.0741"
       y="53.540215"
       id="text1189-0-6-7-8"><tspan
         sodipodi:role="line"
         id="tspan1187-6-1-9-4"
         x="127.0741"
         y="53.540215"
         style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8-2-5">3</tspan></tspan></text>
    <text
       xml:space="preserve"
       style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       x="130.99895"
       y="40.044033"
       id="text1189-0-6-7-0-0"><tspan
         sodipodi:role="line"
         id="tspan1187-6-1-9-2-3"
         x="130.99895"
         y="40.044033"
         style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8-2-3-6">4</tspan></tspan></text>
    <text
       xml:space="preserve"
       style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       x="149.93549"
       y="41.220608"
       id="text1189-0-6-7-0-7-1"><tspan
         sodipodi:role="line"
         id="tspan1187-6-1-9-2-5-0"
         x="149.93549"
         y="41.220608"
         style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">d<tspan
   style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:3.21027px;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';baseline-shift:sub;stroke-width:0.396875px"
   id="tspan913-2-8-2-3-9-6">5</tspan></tspan></text>
    <path
       style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-7)"
       d="m 88.635416,56.064464 c 0.097,0.175847 0.233702,0.423661 0.341749,0.619531"
       id="path3339-3"
       inkscape:path-effect="#path-effect4150"
       inkscape:original-d="m 88.635416,56.064464 c 0.09709,0.175798 0.233793,0.423611 0.341749,0.619531"
       sodipodi:nodetypes="cc" />
    <path
       style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-3-6)"
       d="m 124.01895,51.357923 c 0.097,0.175851 0.2337,0.423664 0.34175,0.619531"
       id="path3339-7-0"
       inkscape:path-effect="#path-effect4154"
       inkscape:original-d="m 124.01895,51.357923 c 0.0971,0.175798 0.2338,0.423611 0.34175,0.619531"
       sodipodi:nodetypes="cc" />
    <path
       style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-6-1-6)"
       d="m 127,37.543631 c 0.097,0.175851 0.2337,0.42366 0.34175,0.619531"
       id="path3339-9-7-6"
       inkscape:path-effect="#path-effect4156"
       inkscape:original-d="m 127,37.543631 c 0.0971,0.175798 0.23379,0.423611 0.34175,0.619531"
       sodipodi:nodetypes="cc" />
    <path
       style="fill:#ff0000;fill-rule:evenodd;stroke:#ff0000;stroke-width:0.264999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1;marker-start:url(#marker3345-6-6)"
       d="m 149.48958,44.158214 c 0.097,0.175851 0.2337,0.423664 0.34175,0.619531"
       id="path3339-9-1"
       inkscape:path-effect="#path-effect4158"
       inkscape:original-d="m 149.48958,44.158214 c 0.0971,0.175798 0.2338,0.423611 0.34175,0.619531"
       sodipodi:nodetypes="cc" />
    <path
       style="fill:none;fill-rule:evenodd;stroke:#0000ff;stroke-width:0.524999;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"
       d="m 151.13062,30.934944 c -6.52016,10.553519 -13.03796,21.103208 -23.66734,26.89653 -10.62939,5.793321 -25.36645,6.826395 -40.101938,7.859359"
       id="path3329-5"
       inkscape:path-effect="#path-effect4160"
       inkscape:original-d="m 151.13062,30.934944 c -6.51985,10.553711 -13.03765,21.103397 -19.55808,31.656666 -14.73863,1.033468 -29.47569,2.066541 -44.211198,3.099223"
       sodipodi:nodetypes="ccc" />
    <text
       xml:space="preserve"
       style="font-size:4.9389px;line-height:125%;font-family:'Noto Serif';-inkscape-font-specification:'Noto Serif';letter-spacing:0px;word-spacing:0px;fill:none;stroke:#000000;stroke-width:0.396875px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       x="109.75323"
       y="29.953663"
       id="text1189-3-5"><tspan
         sodipodi:role="line"
         id="tspan1187-5-4"
         x="109.75323"
         y="29.953663"
         style="font-style:italic;font-size:4.9389px;fill:#000000;stroke:none;stroke-width:0.396875px">y=a<tspan
   style="font-style:normal;font-size:65%;baseline-shift:sub"
   id="tspan3609-7">0</tspan>+a<tspan
   style="font-style:normal;font-size:65%;baseline-shift:sub"
   id="tspan3607-6">1</tspan>x+a<tspan
   style="font-style:normal;font-size:65%;baseline-shift:sub"
   id="tspan4372">2</tspan>x<tspan
   style="font-style:normal;font-size:65%;baseline-shift:super"
   id="tspan4376">2</tspan></tspan></text>
  </g>
</svg>
"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial can be used to fit data. \n",
    "\n",
    "The fitting means searching such polynomial the sum of squared distances from data points to the polynomial curve is the smallest\n",
    "\n",
    "![poly_fit.svg](attachment:poly_fit.svg)\n",
    "\n",
    "Fitting of the data is performed with the method `.fit`. \n",
    "\n",
    "In what follows we will fit a dataset with polynomials of various orders. \n",
    "\n",
    "The polynomials will be our models. \n",
    "\n",
    "Parameters of the models are the polynomial coefficients.\n",
    "\n",
    "The training is performed when we fit the data with the polynomial.\n",
    "\n",
    "Finding the bets approximation of the data using certain functions, polynomial for example, is called regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that a certain natural process is described by a polynomial.\n",
    "$$\n",
    "y = -1 + 1.5 x + 0.1 x^2 - 1.5 x^3\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fun_orig is the true dependence from nature that we will try to recover\n",
    "from numpy.polynomial import Polynomial as P\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fun_orig = P([-1, 1.5, 0.1, -1.5])\n",
    "print(\"fun_orig\")\n",
    "print(\"y =\", fun_orig)\n",
    "\n",
    "xp, yp = fun_orig.linspace()\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xp, yp)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pretend that this formula is unknown. \n",
    "\n",
    "Trying to recover it we can do some measurements: $x$ values and the corresponding $y$. \n",
    "\n",
    "Unfortunately the measurements are not so exact, they are spoiled by a noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "n_total = 30\n",
    "data = np.zeros((n_total, 2))\n",
    "\n",
    "# Measured values of x. Simulate them by uniform random numbers\n",
    "xs = rng.uniform(-1, 1, size=(n_total,))\n",
    "\n",
    "# Measured values of are computed from xs and spoiled by noise. \n",
    "ys = fun_orig(xs) + 0.1 * rng.normal(size=(n_total,))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(xs, ys)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_xlabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all the data into a dataset. Values of $x$ is the first column, $y$ values are in the column two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([xs, ys]).T\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create machine learning models that will try to recover `fun_orig`\n",
    "\n",
    "First we need to split the dataset into training, validation and test parts.\n",
    "\n",
    "Here is the function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, p_train, p_valid, shuffle=True):\n",
    "    \"\"\"Split dataset into train, validation and test parts.\n",
    "    p_train and p_valid must be within [0, 1]\n",
    "    \"\"\"\n",
    "    assert 0 < p_train + p_valid < 1 \n",
    "    n_tot = len(data)\n",
    "    n_train = round(p_train * n_tot)\n",
    "    n_valid = round(p_valid * n_tot)\n",
    "    n_test = n_tot - n_train - n_valid\n",
    "    \n",
    "    if shuffle:\n",
    "        # Shuffle data if needed\n",
    "        rng.shuffle(data, axis=0)\n",
    "        \n",
    "    # Extract train dataset\n",
    "    data_train = data[:n_train]\n",
    "    # Validation dataset\n",
    "    data_valid = data[n_train:n_train+n_valid]\n",
    "    # Test dataset\n",
    "    data_test = data[n_train+n_valid:n_train+n_valid+n_test]\n",
    "    return data_train, data_valid, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the splitting. \n",
    "\n",
    "We take 40\\% for training, and 30\\% for validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train, p_valid = 0.4, 0.3\n",
    "\n",
    "data_train, data_valid, data_test = split_dataset(data, p_train, p_valid)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 3))\n",
    "axs[0].scatter(data_train[:,0], data_train[:,1], color='C0')\n",
    "axs[0].set_title(label=f\"training, size={len(data_train)}\")\n",
    "\n",
    "axs[1].scatter(data_valid[:,0], data_valid[:,1], color='C1')\n",
    "axs[1].set_title(f\"validation, size={len(data_valid)}\")\n",
    "\n",
    "axs[2].scatter(data_test[:,0], data_test[:,1], color='C2')\n",
    "axs[2].set_title(f\"test, size={len(data_test)}\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start training models. \n",
    "\n",
    "The model is a polynomial and different models have different order of the polynomial.\n",
    "\n",
    "The training is very simple. All is done in one step: we perform a polynomial fit.\n",
    "\n",
    "Notice that considering polynomials of the order higher then 1 means using the trick with powers: \n",
    "\n",
    "Our original dataset has only two features, $x$ values and $y$ values. \n",
    "\n",
    "And we add more features by taking into account higher powers of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial import Polynomial as P\n",
    "\n",
    "# Max degree of the polynomials\n",
    "max_degree = 12\n",
    "\n",
    "models = []\n",
    "for deg in range(max_degree):\n",
    "    model = P.fit(data_train[:, 0], data_train[:, 1], deg)\n",
    "    print(\"deg=\", deg, \":\", model)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the performance of these models we need to pass $x$ values to each of them and compute the corresponding $y$.\n",
    "\n",
    "Then we compare the computed $y$ with those in the dataset.\n",
    "\n",
    "We will use the MSE metric to estimate the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(a, b):\n",
    "    \"\"\"Mean squared error\"\"\"\n",
    "    return np.mean((a-b)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that computes the approximation error reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_error(model, data):\n",
    "    \"\"\"Approximation error\"\"\"\n",
    "    xs = data[:, 0]\n",
    "    ys_true = data[:, 1]\n",
    "    ys_pred = model(xs)\n",
    "    return mse(ys_true, ys_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the polynomials approximate our data.\n",
    "\n",
    "We plot the graphs and compute approximation errors that serves as estimations of performance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=max_degree//3, ncols=3, figsize=(12, (max_degree//3)*2.5))\n",
    "\n",
    "data_plot = data_train\n",
    "for model, ax in zip(models, axs.reshape(-1)):\n",
    "    xp_orig, yp_orig = fun_orig.linspace()\n",
    "    xp, yp = model.linspace()\n",
    "    ax.scatter(data_plot[:, 0], data_plot[:, 1], color='C0');\n",
    "    ax.plot(xp_orig, yp_orig, color='C0')  # original dependence that we try to recover\n",
    "    ax.plot(xp, yp, color='C1')\n",
    "    error = approx_error(model, data_plot)\n",
    "    ax.set_title(f\"deg={model.degree()}, error={error:.3}\")\n",
    "    ax.grid()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the polynomial of degree 11 approximates the train data perfectly well. \n",
    "\n",
    "Error is actually zero. Small nonzero value appears only due to numerical errors.\n",
    "\n",
    "But what about the validation data?\n",
    "\n",
    "This is the data that are never seen by our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=max_degree//3, ncols=3, figsize=(12, (max_degree//3)*2.5))\n",
    "\n",
    "data_plot = data_valid\n",
    "for model, ax in zip(models, axs.reshape(-1)):\n",
    "    xp_orig, yp_orig = fun_orig.linspace()\n",
    "    xp, yp = model.linspace()\n",
    "    ax.scatter(data_plot[:, 0], data_plot[:, 1], color='C0');\n",
    "    ax.plot(xp_orig, yp_orig, color='C0')  # original dependence that we try to recover\n",
    "    ax.plot(xp, yp, color='C1')\n",
    "    error = approx_error(model, data_plot)\n",
    "    ax.set_title(f\"deg={model.degree()}, error={error:.3}\")\n",
    "    ax.grid()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is different.\n",
    "\n",
    "Approximation error for the polynomial 11 degree is huge.\n",
    "\n",
    "For better visualization let us collect approximation errors as arrays and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "degrees = [model.degree() for model in models]\n",
    "error_train = [approx_error(model, data_train) for model in models]\n",
    "error_valid = [approx_error(model, data_valid) for model in models]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n",
    "\n",
    "# Apprximation error vs degree of the model\n",
    "ax = axs[0]\n",
    "ax.plot(degrees, error_train, '-*', label='train')\n",
    "ax.plot(degrees, error_valid, '-*', label='valid')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.grid()\n",
    "\n",
    "# Same but y scale is linear and large valies are truncated\n",
    "ax = axs[1]\n",
    "ax.plot(degrees, error_train, '-*', label='train')\n",
    "ax.plot(degrees, error_valid, '-*', label='valid')\n",
    "ax.legend()\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 0.05])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the models with low degrees 0 and 1 are underfitted.\n",
    "\n",
    "They perform bad both in training and in validation data.\n",
    "\n",
    "Models with high degrees are overfitted. Model 11 has error 0 on the training data because it merely remembers them. \n",
    "\n",
    "Showing it unseen data results in huge error.\n",
    "\n",
    "Optimal models have degree near 3, the same as the original polynomial.\n",
    "\n",
    "We used training data to compute model parameters. These are the polynomial coefficient.\n",
    "\n",
    "These data can not be used to compute the final model score.\n",
    "\n",
    "The validation data was used to select better model. This is also a sort of training: we take the model that \n",
    "performs the best on the validation data. \n",
    "\n",
    "The validation data also can not be used for computing the final score.\n",
    "\n",
    "That is why we need the test data.\n",
    "\n",
    "They are showed to the selected model just once and no modes selection is done after that.\n",
    "\n",
    "Let us select the model of degree 3 as the result of our training and find its final score on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "the_model = models[3]\n",
    "print(\"the_model:\\n\", the_model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "xp_orig, yp_orig = fun_orig.linspace()\n",
    "xp, yp = the_model.linspace()\n",
    "ax.scatter(data_test[:, 0], data_test[:, 1], color='C0');\n",
    "ax.plot(xp_orig, yp_orig, color='C0')  # original dependence that we try to recover\n",
    "ax.plot(xp, yp, color='C1')\n",
    "\n",
    "error = approx_error(the_model, data_test)\n",
    "ax.set_title(f\"deg={the_model.degree()}, error={error:.3}\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us discuss why the overfitting occurs for the high degree polynomials.\n",
    "\n",
    "These polynomials have to many parameters in comparison with the dataset size.\n",
    "\n",
    "The model can easily remember all the inputs without finding their generalization.\n",
    "\n",
    "Thus we can fight the overfitting either by decreasing number of parameters, i.e., considering lower order polynomials.\n",
    "\n",
    "Or we can (if we can) find more data. The models will be unable to remember all these data and will be forced to search the for generalizing. \n",
    "\n",
    "Another way to avoid the overfitting is called regularization. \n",
    "\n",
    "Applying regularization to the model parameter means limit ranges if their variations. \n",
    "\n",
    "The idea is the same: squeezing the range of allowed variations we prohibit the mere remembering the data.\n",
    "\n",
    "Let us look at our models again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(\"deg=\", model.degree(), '\\n', model, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models of high degree, i.e., those that are highly overfitted, have really high parameters.\n",
    "\n",
    "Regularization means that when searching the optimal parameter we add additional penalty for the parameter amplitudes, requiring that they must be not so large.\n",
    "\n",
    "This method is called ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "3\\. Answer in writing what are overfitting and underfitting. Why dataset must be split into parts?\n",
    "\n",
    "4\\. Describes in writing the ways of fighting with overfitting and underfitting.\n",
    "\n",
    "5\\. Describe in writing  what is cross-validation\n",
    "\n",
    "6\\. List metrics that are used for regression\n",
    "\n",
    "7\\. Reproduce the analysis of a random spam filter for the case when spam messages emerge more often. In what case the accuracy metric become appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "ru",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "113px",
    "width": "228px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "664.467px",
    "left": "79px",
    "top": "68.1167px",
    "width": "241.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
