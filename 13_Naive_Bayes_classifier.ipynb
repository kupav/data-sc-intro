{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 Naive Bayes classifier\n",
    "\n",
    "Part of [\"Introduction to Data Science\" course](https://github.com/kupav/data-sc-intro) by Pavel Kuptsov, [kupav@mail.ru](mailto:kupav@mail.ru)\n",
    "\n",
    "Recommended reading for this section:\n",
    "\n",
    "1. Grus, J. (2019). Data Science From Scratch: First Principles with Python (Vol. Second edition). Sebastopol, CA: Oâ€™Reilly Media\n",
    "1. Muller, A and Guido, S (2017). Introduction to Machine Learning with Python. O'Reilly\n",
    "\n",
    "The following Python modules will be required. Make sure that you have them installed.\n",
    "- `numpy`\n",
    "- `matplotlib`\n",
    "- `sklearn`\n",
    "- `csv`\n",
    "- `requests`\n",
    "- `collections`\n",
    "- `re`\n",
    "- `io`\n",
    "- `zipfile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember probabilities and the Bays' theorem\n",
    "\n",
    "Assume we have a stochastic experiment like rolling a dice or tossing a coin.\n",
    "\n",
    "Consider an event $A$, e.g., an even number has shown up.\n",
    "\n",
    "If there are totally $N$ outcomes of the experiment and $N_A$ of them correspond to the event $A$.\n",
    "\n",
    "Then the probability of $A$ is\n",
    "\n",
    "$$\n",
    "P(A) = \\frac{N_A}{N}\n",
    "$$\n",
    "\n",
    "$P(A)$ is also called marginal probability.\n",
    "\n",
    "For example if we have a dice and the event $A$ is showing up an even number then totally there are $N=6$ \n",
    "outcomes and $N_A=3$ of them fulfills $A$.\n",
    "\n",
    "$$\n",
    "P(A) = \\frac{3}{6}=\\frac{1}{2}\n",
    "$$\n",
    "\n",
    "Consider two events $A$ and $B$. Assume we have a box of balls that are blue and red. Let an event $A$ be a blind taking of a red ball without returning it back. And $B$ is a blind taking of another red ball.\n",
    "\n",
    "Event $B$ and $A$ are dependent since the probability of $B$ depends on the occurrence of $A$. If $A$ has not occurred then $P(B)$ is higher (because more red balls left).\n",
    "\n",
    "But if after taking a ball for the first time we return it back the event $B$ will be independent on $A$.\n",
    "\n",
    "Joint probability of $A$ and $B$ is the probability that both events occur simultaneously. \n",
    "\n",
    "If they are independent then the joint probability is mere the product of their probabilities:\n",
    "\n",
    "$$\n",
    "P(A,B)=P(A)P(B)\n",
    "$$\n",
    "\n",
    "And when the events are dependent the joint probability is computed via their conditional probabilities:\n",
    "\n",
    "$$\n",
    "P(A, B) = P(B | A) P(A)\n",
    "$$\n",
    "\n",
    "Here $P(B|A)$ is the conditional probability of $B$ provided that $A$ has occurred. \n",
    "\n",
    "Since the all dependences in nature are always two sided (if $A$ influences $B$ than $B$ also influences $A$) we can also consider the reversed formula for the joint probability:\n",
    "\n",
    "$$\n",
    "P(A, B) = P(A | B) P(B)\n",
    "$$\n",
    "\n",
    "Now $P(A | B)$ is the conditional probability of $A$ provided that $B$ has occurred. \n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "P(B | A) P(A) = P(A | B) P(B)\n",
    "$$\n",
    "\n",
    "Assume we know $P(A | B)$ and want to find $P(B | A)$. It is found via Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(B | A) = \\frac{P(A | B) P(B)}{P(A)}\n",
    "$$\n",
    "\n",
    "If the marginal probability $P(A)$ is unknown in advance, it can be found via the law of total probability.\n",
    "\n",
    "$$\n",
    "P(A) = P(A | B) P(B) + P(A | \\bar B) P(\\bar B)\n",
    "$$\n",
    "\n",
    "Here over bar means negation: $\\bar B$ means that $B$ has not occurred.\n",
    "\n",
    "Gathering these equations together we obtain more detailed form of Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(B | A) = \\frac{P(A | B) P(B)}{P(A | B) P(B) + P(A | \\bar B) P(\\bar B)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of the naive Bayes classifier\n",
    "\n",
    "Let us remember that the classification mean finding an appropriate label for a new data record.\n",
    "\n",
    "Assume that we have a dataset of $n$ features (table with $n$ columns). \n",
    "\n",
    "Let us denote as $(A_1, A_2, \\ldots A_n)$ a row from this table. In the other word, this is a record describing one entity in the dataset. \n",
    "\n",
    "The data are assumed to be categorical. For example $A_1$ can denote a size of a fruit: \"big\", \"medium\" or \"small\", and $A_2$ can encode its color like \"red\", \"green\" or \"yellow\". Or something like that.\n",
    "\n",
    "In general $A_i$ can have an integer value from a certain range. For the sake of simplicity in what follows we will often assume that $A_i$ can be either 0 or 1.\n",
    "\n",
    "Let us denote via $B$ a class label. For example it can be \"apple\", or \"orange\" or \"tomato\". \n",
    "\n",
    "In general this is also an integer that encodes a class.\n",
    "\n",
    "Since $A_i$ and $B$ are categorical and have finite ranges of integer values in principle we can consider all possible combinations\n",
    "of $(A_1, A_2, \\ldots A_n)$.\n",
    "\n",
    "For example if $A_i$ can be 0 or 1 and there are three features, i.e., $n=3$, there are 8 combinations:\n",
    "- 000\n",
    "- 001\n",
    "- 010\n",
    "- 011\n",
    "- 100\n",
    "- 101\n",
    "- 110\n",
    "- 111\n",
    "\n",
    "If we are able to collect all combinations and assign a label $B$ to each one then we do not need machine learning at all.\n",
    "\n",
    "If a new record is obtained we just can look at the dataset and find the corresponding label.\n",
    "\n",
    "But if $n$ is large it is impossible to collect and label all possible combinations.\n",
    "\n",
    "How can we classify a new data record $(A_1, A_2, \\ldots A_n)$ if we have an incomplete labeled dataset that does not contain this combination?\n",
    "\n",
    "When we solve this problem using a naive Bayes classifiers we compute probabilities that a new data record belongs to each of the existing classes and then choose the one with the largest value of the probability.\n",
    "\n",
    "This approach is also known as maximum likelihood estimation.\n",
    "\n",
    "The staring point of the analysis is a Bayes' theorem. This theorem allows to compute conditional probability. \n",
    "\n",
    "However the naive Bayes classifier can compute only an approximate estimation of the probability. \n",
    "\n",
    "Thus this is usually called likelihood instead of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Bayes theorem\n",
    "\n",
    "$$\n",
    "P(B_j | A_1,A_2,\\ldots A_n) = \\frac{P(A_1,A_2,\\ldots A_n | B_j) P(B_j)}{P(A_1,A_2,\\ldots A_n)}\n",
    "$$\n",
    "\n",
    "To identify a class we need $P(B_j | A_1,A_2,\\ldots A_n)$ for all classes $B_j$ where $j$ runs through all class numbers.\n",
    "\n",
    "Accordingly, conditional probabilities $P(A_1,A_2,\\ldots A_n | B_j)$ are required.\n",
    "\n",
    "In principal it is clear how to compute them from the dataset: take the subset that has label $B_j$ and compute relative frequencies \n",
    "of every combination for $(A_1,A_2,\\ldots A_n)$.\n",
    "\n",
    "But as we already mentioned above if we really have a labeled dataset \n",
    "where all the combinations of $(A_1,A_2,\\ldots A_n)$ can be found, we do not need a classifier. \n",
    "\n",
    "For the most of practically interesting problems such table would be really huge.\n",
    "\n",
    "The problem is how to estimate $P(B_j | A_1,A_2,\\ldots A_n)$ without having a dataset with all combinations of $A_i$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue let us consider how number of possible combinations grows with the number of features $n$.\n",
    "\n",
    "We assume that our features are binary, i.e., each column can contain only 0's or 1's\n",
    "\n",
    "Thus a string of 0's and 1's represents a data record. \n",
    "\n",
    "How many different combinations can be for a given $n$?\n",
    "\n",
    "If $n=1$  there are two combinations: \n",
    "- '0'\n",
    "- '1'\n",
    "\n",
    "If $n=2$ we take each of size 1 string and append one more symbol to the left, either 0 or 1. Two combinations are repeated twice. Thus totally there are 4 combinations\n",
    "- '00'\n",
    "- '01'\n",
    "- '10'\n",
    "- '11'\n",
    "\n",
    "If $n=3$: take all previous 4 strings and repeat each two times with 0 or 1 appended:\n",
    "- '000'\n",
    "- '001'\n",
    "- '010'\n",
    "- '011'\n",
    "- '100'\n",
    "- '101'\n",
    "- '110'\n",
    "- '111'\n",
    "\n",
    "In general: each new symbol doubles the number of combinations. String of $n$ symbols has 2 to the power of $n$ combinations.\n",
    "- $n=1$: $2=2^1$\n",
    "- $n=2$: $2\\times 2=2^2$\n",
    "- $n=3$: $2\\times 2\\times 2=2^3$\n",
    "- $n$: $2\\times 2\\times\\ldots\\times 2=2^n$\n",
    "\n",
    "All these strings can be treated as a binary numbers. \n",
    "\n",
    "It is much simpler to substitute them them with the corresponding decimal numbers. \n",
    "\n",
    "Now the question: if we have $n$ binary features how large dataset can be to have all possible combinations in it?\n",
    "\n",
    "Obviously it must have size $2^n$ provided that each combinations appears exactly one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ns = np.arange(0, 11)\n",
    "sz = 2**ns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ns, sz, 'o-')\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required size grows very fast.\n",
    "\n",
    "And if we collect a dataset more or less at random the size will be even larger since some combinations can repeat.\n",
    "\n",
    "Lets us generate a dataset at random and see when we can have each combinations of features at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "nn = 6             # model number of binary features\n",
    "size_short = 100   # small dataset size\n",
    "size_long  = 1000  # lager dataset size\n",
    "data_short = rng.integers(2**nn, size=size_short)\n",
    "data_long = rng.integers(2**nn, size=size_long)\n",
    "\n",
    "count_short = Counter(data_short)\n",
    "count_long  = Counter(data_long)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(14, 8))\n",
    "\n",
    "axs[0].bar(list(count_short), list(count_short.values()))\n",
    "axs[0].set_title(f'range of data {2**nn}, size of the datset {size_short}')\n",
    "axs[0].set_xlim([0, 2**nn])\n",
    "axs[1].bar(list(count_long), list(count_long.values()))\n",
    "axs[1].set_title(f'range of data {2**nn}, size of the datset {size_long}')\n",
    "axs[1].set_xlim([0, 2**nn]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that if we have $n=6$ features there are 64 combinations and the dataset of 100 elements is sparse: not all variants appear.\n",
    "(Let us remember we consider combinations of 0's and 1's and have 6 of them. Integers along the horizontal axis enumerate these combinations).\n",
    "\n",
    "For the dataset of the length 1000 there are no empty sites.\n",
    "\n",
    "But if we increase $n$ we will see some again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us return to our problem. \n",
    "\n",
    "We have a labeled dataset that does not include all combinations of $(A_1,A_2,\\ldots A_n)$ but want to have a machine learning \n",
    "model that can assign a correct label to every possible combination.\n",
    "\n",
    "When we build a naive Bayes classifier we solve the problem assuming that all features are independent on each other.\n",
    "\n",
    "That is why the classifier is naive. Intuitively we expect that the features can correlate with each other but ignore it.\n",
    "\n",
    "For example assume that one column of the dataset contains temperature sensations like \"hot\", \"mild\" or \"cool\" and another column is snowfall that can be either \"yes\" or \"no\". \n",
    "\n",
    "We can expect that air temperature influences the emergence of a snowfall. However a naive Bayes classifier ignores it.\n",
    "\n",
    "But still it works. The main reason is that the dependencies that we omit are usually not so strong and influential.\n",
    "\n",
    "The benefit of the naive assumption is as follows:\n",
    "\n",
    "In the Bayes' equation\n",
    "\n",
    "$$\n",
    "P(B_j | A_1,A_2,\\ldots A_n) = \\frac{P(A_1,A_2,\\ldots A_n | B_j) P(B_j)}{P(A_1,A_2,\\ldots A_n)}\n",
    "$$\n",
    "\n",
    "we need the conditional probability $P(A_1,A_2,\\ldots A_n | B_j)$. If the features are independent we can easily compute it. \n",
    "\n",
    "Let us remember: joint probability of the independent events is a product \n",
    "of their marginal probabilities.\n",
    "\n",
    "$$\n",
    "P(A_1,A_2,\\ldots A_n | B_j) = P(A_1 | B_j) P(A_2 | B_j) \\ldots P(A_n | B_j)\n",
    "$$\n",
    "\n",
    "To perform classification we will not need the denominator of the Bayes formula, but it can also be easily computed:\n",
    "\n",
    "$$\n",
    "P(A_1,A_2,\\ldots A_n) = P(A_1) P(A_2) \\ldots P(A_n)\n",
    "$$\n",
    "\n",
    "Why it helps? Because we can easily collect the probabilities $P(A_i | B_j)$ just by counting values in columns. \n",
    "\n",
    "The huge set is not needed for it. \n",
    "\n",
    "And then we can construct every combination of features $(A_1,A_2,\\ldots A_n)$ and estimate its probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the naive Bayes classifier computes likelihood (estimated probability) that a feature vector $(A_1,A_2,\\ldots A_n)$  belongs to the class $B_j$ according to the formula:\n",
    "\n",
    "$$\n",
    "P(B_j | A_1,A_2,\\ldots A_n) \\propto P(A_1 | B_j) P(A_2 | B_j) \\ldots P(A_n | B_j) P(B_j)\n",
    "$$\n",
    "\n",
    "Here $P(A_i | B_j)$ and $P(B_j)$ are computed from the training data as relative frequencies. \n",
    "\n",
    "The symbol \"$\\propto$\" means \"proportional\". We use it instead of equality sign because we have omitted the denominator.\n",
    "\n",
    "We have to compute $P(B_j | A_1,A_2,\\ldots A_n)$ for all $j$ and since the denominator $P(A_1) P(A_2) \\ldots P(A_n)$ does not depend on $B_j$ it will be the same for all $j$. That is why we omit it.\n",
    "\n",
    "Then we compare likelihoods $P(B_j | A_1,A_2,\\ldots A_n)$ and find a class $B_j$ that have the largest one. This will be the label for the feature vector $(A_1,A_2,\\ldots A_n)$. \n",
    "\n",
    "Mathematically it reads as\n",
    "\n",
    "$$\n",
    "B_j = \\mathop{\\mathrm{argmax}}_{\\text{$B_j$ runs through all classes}} P(A_1 | B_j) P(A_2 | B_j) \\ldots P(A_n | B_j) P(B_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example of a binary classifier\n",
    "\n",
    "Assume that we have a corpus of eleven email messages labeled as spam or ham. \n",
    "\n",
    "(Ham in this context is a standard term for a non-spam message. 'SPAM' is a brand of canned cooked pork that became famous due to its annoying commercial).\n",
    "\n",
    "Five of them are labeled as spam and six are ham.\n",
    "\n",
    "In these messages we find four keywords \"million\", \"winner\", \"award\" and \"brand\". We hope that their presence or absence \n",
    "in a message can be related with its class, spam or ham.\n",
    "\n",
    "In the table below each row represents one email. Symbol 1 in the column means that the corresponding word, usually it is called token, is encountered in this message and 0 means that this token is absent.\n",
    "\n",
    "List of all tokes is called vocabulary.\n",
    "\n",
    "For example the first email contains all four tokens, the second one contains all except \"award\". In the third message token \"million\" is absent and so on.\n",
    "\n",
    "Such representation of texts is called \"bag of words\". \n",
    "\n",
    "| #  | million | winner | award | brand | label |\n",
    "|----|---------|--------|-------|-------|-------|\n",
    "| 1  | 1       | 1      | 1     | 1     | spam  |\n",
    "| 2  | 1       | 1      | 0     | 1     | spam  |\n",
    "| 3  | 0       | 1      | 1     | 1     | spam  |\n",
    "| 4  | 0       | 1      | 0     | 0     | spam  |\n",
    "| 5  | 0       | 0      | 1     | 0     | spam  |\n",
    "| 6  | 1       | 1      | 0     | 1     | ham   |\n",
    "| 7  | 0       | 0      | 0     | 0     | ham   |\n",
    "| 8  | 0       | 1      | 0     | 0     | ham   |\n",
    "| 9  | 0       | 0      | 0     | 0     | ham   |\n",
    "| 10 | 0       | 0      | 1     | 0     | ham   |\n",
    "| 11 | 1       | 1      | 1     | 0     | ham   |\n",
    "\n",
    "Since we have four tokens and each one can be or can not be in the message there are $2^4=16$ combinations. \n",
    "\n",
    "But also we suspect that\n",
    "four tokens is not enough for unambiguously identify a class. \n",
    "\n",
    "It means that some combinations can appear both in spam and in ham classes, e.g., see messages 2 and 6.\n",
    "\n",
    "We are going to create a naive Bayes classifier for this dataset.\n",
    "\n",
    "Let us compute conditional probabilities for tokens to appear in spam messages:\n",
    "$P(M|S)$, $P(W|S)$, $P(A|S)$, $P(B|S)$.\n",
    "\n",
    "Similar we find probabilities for tokens in ham messages:\n",
    "$P(M|H)$, $P(W|H)$, $P(A|H)$, $P(B|H)$.\n",
    "\n",
    "Also we will need marginal probabilities of spam and ham $P(S)$ and $P(H)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are columns of the table above\n",
    "million = np.array([1,1,0,0,0,1,0,0,0,0,1])\n",
    "winner  = np.array([1,1,1,1,0,1,0,1,0,0,1])\n",
    "award   = np.array([1,0,1,0,1,0,0,0,0,1,1])\n",
    "brand   = np.array([1,1,1,0,0,1,0,0,0,0,0])\n",
    "label = np.array(['spam'] * 5 + ['ham'] * 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical remark. How can we filter values from `numpy` array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content of two arrays\n",
    "print(million)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This statement compare 'orange' with every element of array\n",
    "print(label=='spam')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we pass array of True and False as another array index we extact only those values that \n",
    "# correspond to True\n",
    "print(million[label=='spam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function: we pass it an array and a label. It filters corresponding elements of an array and find a relative frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(arr, lab):\n",
    "    a = arr[label==lab]\n",
    "    return sum(a) / len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the probabilities\n",
    "- 'million' in spam $P(M|S)$\n",
    "- 'winner' in spam $P(W|S)$\n",
    "- 'award' in spam $P(A|S)$\n",
    "- 'brand' in spam $P(B|S)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_m_s = prob(million, 'spam')\n",
    "prob_w_s = prob(winner, 'spam')\n",
    "prob_a_s = prob(award, 'spam')\n",
    "prob_b_s = prob(brand, 'spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the probabilities for ham\n",
    "- 'million' in ham $P(M|H)$\n",
    "- 'winner' in ham $P(W|H)$\n",
    "- 'award' in ham $P(A|H)$\n",
    "- 'brand' in ham $P(B|H)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_m_h = prob(million, 'ham')\n",
    "prob_w_h = prob(winner, 'ham')\n",
    "prob_a_h = prob(award, 'ham')\n",
    "prob_b_h = prob(brand, 'ham')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal probabilities of spam and ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_marg_s = len(label[label=='spam']) / len(label)\n",
    "prob_marg_h = 1 - prob_marg_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more helper function: it returns `prob` if `x==1` and `(1-prob)` when `x==0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pb(prob, x):\n",
    "    return prob if x == 1 else (1-prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all data of the model is ready and we can do predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test, w_test, a_test, b_test = 1, 1, 0, 1  # this is a new data record\n",
    "\n",
    "lh_s = pb(prob_m_s, m_test) * pb(prob_w_s, w_test) * pb(prob_a_s, a_test) * pb(prob_b_s, b_test) * prob_marg_s\n",
    "lh_h = pb(prob_m_h, m_test) * pb(prob_w_h, w_test) * pb(prob_a_h, a_test) * pb(prob_b_h, b_test) * prob_marg_h\n",
    "pred = 'spam' if lh_s > lh_h else 'ham'\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test, w_test, a_test, b_test = 1, 0, 0, 1  # this is a new data record\n",
    "\n",
    "lh_s = pb(prob_m_s, m_test) * pb(prob_w_s, w_test) * pb(prob_a_s, a_test) * pb(prob_b_s, b_test) * prob_marg_s\n",
    "lh_h = pb(prob_m_h, m_test) * pb(prob_w_h, w_test) * pb(prob_a_h, a_test) * pb(prob_b_h, b_test) * prob_marg_h\n",
    "pred = 'spam' if lh_s > lh_h else 'ham'\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical remarks before creating the naive Bayes classifier\n",
    "\n",
    "Now are going to create a fully functional naive Bayes classifier.\n",
    "\n",
    "But before doing that we need to make some technical remarks.\n",
    "\n",
    "For our purposes it will be more convenient to use `defaultdict` instead of a plain Python dictionary.\n",
    "\n",
    "If a plain dictionary does not have a key its reading causes an error, and reading an absent key from `defaultdict` returns a default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = dict()\n",
    "d1['one'] = 0\n",
    "try:\n",
    "    d1['one'] += 1  # it's ok, since we already initialized this key\n",
    "    d1['two'] += 1  # it will be an error since key 'two' is undefined\n",
    "    print(\"Finished successfully\")  # this line will not be executed\n",
    "except KeyError as e:\n",
    "    print(\"KeyError\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d2 = defaultdict(int)\n",
    "d2['one'] = 0\n",
    "try:\n",
    "    d2['one'] += 1  # it's ok, since we already initialized this key\n",
    "    d2['two'] += 1  # it is also ok since default value of int (zero) will be assumed \n",
    "    print(\"Finished successfully\") \n",
    "except KeyError as e:\n",
    "    print(\"KeyError\", e)  # this line will not be executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us remember the Python sets.\n",
    "\n",
    "A set does not allow duplicates. Thus when a list is converted to a set, all duplicates are removed.\n",
    "\n",
    "Then the set can be converted back to a list if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = ['one', 'one', 'one', 'two', 'two', 'three']\n",
    "print(s1)\n",
    "s2 = set(s1)\n",
    "print(s2)\n",
    "s3 = list(s2)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets are convenient when we need to store only unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "print(vocab)\n",
    "vocab.add(\"one\")\n",
    "print(vocab)\n",
    "vocab.add(\"one\")\n",
    "print(vocab)\n",
    "vocab.update([\"one\", \"two\", \"three\"])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if a value belong to a set using `in` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('one' in vocab)\n",
    "print('one' not in vocab)\n",
    "print('once' not in vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us remember regular expressions. \n",
    "\n",
    "Pattern \"\\[a-z'\\]\" matches any letter or apostrophe. And if we add plus like this: \"\\[a-z'\\]+\" it means that the pattern matches \n",
    "one or more occurrences of the symbols. This pattern is the simplest way to match words in a text.\n",
    "\n",
    "Usually we ignore the difference between small and capital letters. Thus it is convenient to convert the text to the lower \n",
    "case using `.lower()` method of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "txt = \"\"\"I've a cat named Vesters,\n",
    "And he eats all day.\n",
    "He always lays around,\n",
    "And never wants to play.\n",
    "\"\"\"\n",
    "\n",
    "rge = re.compile(r\"[a-z']+\")\n",
    "print(rge.findall(txt.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two final technical remarks: We will compute probabilities of a word $W_i$ to appear in spam and in ham messages. \n",
    "\n",
    "Assume that this word appears only in spam messages. Then the probability to see it in ham messages will be zero.\n",
    "\n",
    "Since we compute the product of probabilities $P(W_1|H)P(W_2|H)P(W_3|H)\\ldots$ vanishing of one of the elements, say $P(W_2|H)=0$, zeroing the whole product. \n",
    "\n",
    "In this case all messages with $W_i$ will always be classified as spam since the opposite probability will always be zero.\n",
    "\n",
    "To avoid it we add a pseudocount $k$ when compute the probabilities:\n",
    "\n",
    "$$\n",
    "P(W_i | S) = \\frac{k + \\text{number of spam messages with $W_i$}}{2k + \\text{total number of spam messages}}\n",
    "$$\n",
    "\n",
    "Usually $k=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a classification of a message that contains a set of tokes (words)\n",
    "$(W_1, W_2, \\ldots W_n)$ we need to compute likelihoods that it is a spam and a ham:\n",
    "\n",
    "$$\n",
    "P(S | W_1,W_2,\\ldots W_n) \\propto P(W_1 | S) P(W_2 | S) \\ldots P(W_n | S) P(S)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(H | W_1,W_2,\\ldots W_n) \\propto P(W_1 | H) P(W_2 | H) \\ldots P(W_n | H) P(H)\n",
    "$$\n",
    "\n",
    "Then if $P(S | W_1,W_2,\\ldots W_n) > P(H | W_1,W_2,\\ldots W_n)$ we classify it as a spam and this is a ham in the other case.\n",
    "\n",
    "The expressions contains a lot of multiplications:\n",
    "\n",
    "$$\n",
    "P(W_1 | S) P(W_2 | S) \\ldots P(W_n | S) P(S), \\; P(W_1 | H) P(W_2 | H) \\ldots P(W_n | H) P(H)\n",
    "$$\n",
    "\n",
    "Such computations are numerically unstable. \n",
    "\n",
    "We can easily obtain an underflow when values approach zero. Numerical errors can enormously grow.\n",
    "\n",
    "We can avoid it by using logarithms. Let us remember:\n",
    "\n",
    "$$\n",
    "\\log(a \\times b) = \\log a + \\log b\n",
    "$$\n",
    "\n",
    "It means that if we sum logarithms of the factors we will have a logarithm of their product.\n",
    "\n",
    "Given the logarithms of the probabilities we do not need to compute back the probabilities themselves. \n",
    "\n",
    "Since we want to compare the probabilities we will collect sums of logarithms and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The naive Bayes classifier\n",
    "\n",
    "Let us now create a class that implements the naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, k, drop_short):\n",
    "        \"\"\"\n",
    "        k - pseudocount, usually 1\n",
    "        drop_short - drop too short tokens\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.vocab = set()  # vocabulary, i.e., set of all seen tokens\n",
    "        self.token_in_spam = defaultdict(int)   # counters of tokesn in spam\n",
    "        self.token_in_ham = defaultdict(int)    # ... and in ham messages\n",
    "        self.pcond_spam = self.pcond_ham = None # conditional probabilities of tokesn, will be computed after training\n",
    "        self.spam_total = self.ham_total = 0    # total number of spam and ham messages\n",
    "        self.p_spam_total = self.p_ham_total = None  # marginal probailities of spam and ham messages\n",
    "        self.re_token = re.compile(r\"[a-z']+\")  # regex to extarct tokens\n",
    "        self.drop_short = drop_short  # lengths of short tokens to drop out\n",
    "        \n",
    "    def _text2tokens(self, text):\n",
    "        \"\"\"Convert a text to a list of tokens. \n",
    "        We take just the first line of a message that contains a word Subject\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        s = text_lower.splitlines()[0]\n",
    "        text_lower = s.replace('Subject: ', '')\n",
    "        all_tokens = self.re_token.findall(text_lower)\n",
    "        unique_tokes = list(set(all_tokens))\n",
    "        good_tokens = [tok for tok in unique_tokes if len(tok) > self.drop_short]\n",
    "        return good_tokens\n",
    "    \n",
    "    def fit(self, messages, labels):\n",
    "        \"\"\"Training: computing the probailities for each token \n",
    "        to be enoucontered in spam and ham messages.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Count tokens in spam and in ham messages\n",
    "        for mes, lab in zip(messages, labels):\n",
    "            tokens = self._text2tokens(mes)\n",
    "            if lab == 'spam':\n",
    "                self.spam_total += 1\n",
    "                for tok in tokens:\n",
    "                    self.token_in_spam[tok] += 1\n",
    "            else:\n",
    "                self.ham_total += 1\n",
    "                for tok in tokens:\n",
    "                    self.token_in_ham[tok] += 1\n",
    "            self.vocab.update(tokens)\n",
    "\n",
    "        # Compute probabilities\n",
    "        self.pcond_spam = defaultdict(int)\n",
    "        self.pcond_ham = defaultdict(int)\n",
    "        for tok in self.vocab:\n",
    "            self.pcond_spam[tok] = (self.token_in_spam[tok] + self.k) / (self.spam_total + 2 * self.k)\n",
    "            self.pcond_ham[tok] = (self.token_in_ham[tok] + self.k) / (self.ham_total + 2 * self.k)\n",
    "        self.p_spam_total = self.spam_total / (self.spam_total + self.ham_total)\n",
    "        self.p_ham_total = 1 - self.p_spam_total\n",
    "        \n",
    "    def predict(self, messages):\n",
    "        \"\"\"Prediction: computing labels for messages.\n",
    "        \"\"\"\n",
    "        pred = []\n",
    "        for mes in messages:\n",
    "            message_tokens = self._text2tokens(mes)\n",
    "            log_sum_spam = np.log(self.p_spam_total)  # collect probailities for spam \n",
    "            log_sum_ham = np.log(self.p_ham_total)    # ... and ham messages\n",
    "            for tok in self.vocab:\n",
    "                p_spam = self.pcond_spam[tok] \n",
    "                p_ham = self.pcond_ham[tok]\n",
    "                if tok not in message_tokens:  # if the token absent in the message we take complememnt probailities\n",
    "                    p_spam = 1 - p_spam\n",
    "                    p_ham = 1 - p_ham\n",
    "                log_sum_spam += np.log(p_spam)\n",
    "                log_sum_ham += np.log(p_ham)\n",
    "            # Make a desision, spam or ham\n",
    "            pred.append('spam' if log_sum_spam > log_sum_ham else 'ham')\n",
    "        return pred\n",
    "    \n",
    "    def explore_vocab(self):\n",
    "        \"\"\"Make a predicition for every token separately to see\n",
    "        how they influnce the prediction.\n",
    "        \"\"\"\n",
    "        spam_words = []\n",
    "        for tok in self.vocab:\n",
    "            p_spam = self.pcond_spam[tok] * self.p_spam_total\n",
    "            p_ham = self.pcond_ham[tok] * self.p_ham_total\n",
    "            if p_spam > p_ham:\n",
    "                spam_words.append([tok, p_spam])\n",
    "                \n",
    "        spam_words = sorted(spam_words, key=lambda x: -x[1])\n",
    "        words_only = [s[0] for s in spam_words]\n",
    "        return words_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our classifier using the dataset 'spam_and_ham' found at \n",
    "https://www.kaggle.com/venky73/spam-mails-dataset?select=spam_ham_dataset.csv\n",
    "\n",
    "For convenience its copy is uploaded to the course repository where we download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from io import BytesIO, TextIOWrapper\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def load_zipcsv_categorical(file_name):\n",
    "    \"\"\"Downloads zipped csv dataset from repo and return it as a nested list.\"\"\"\n",
    "    base_url = \"https://raw.githubusercontent.com/kupav/data-sc-intro/main/data/\"\n",
    "    web_data = requests.get(base_url + file_name)\n",
    "    assert web_data.status_code == 200\n",
    "\n",
    "    # unzip the content\n",
    "    zf = ZipFile(BytesIO(web_data.content))\n",
    "    \n",
    "    # zipped file name\n",
    "    zipped_name = zf.namelist()[0]\n",
    "    print(f\"Download {file_name}, unzip {zipped_name}\")\n",
    "    \n",
    "    # Open unpacked file\n",
    "    with zf.open(zipped_name, 'r') as file:\n",
    "        # TextIOWrapper(file) converts byte strings to plain strings\n",
    "        reader = csv.reader(TextIOWrapper(file), delimiter=',')\n",
    "        data = []\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "raw_data = load_zipcsv_categorical(\"spam_and_ham.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print first couple of rows from this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in raw_data[:3]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line is the header.\n",
    "\n",
    "Labels and messages are in the columns 1 and 2, respectively. \n",
    "\n",
    "We drop the header out and extract messages and labels into separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lab = [row[1] for row in raw_data[1:]]\n",
    "data_mes = [row[2] for row in raw_data[1:]]\n",
    "print(data_lab[:3])\n",
    "print(data_mes[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check if the labels are indeed 'spam' and 'ham':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(data_lab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_lab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if the dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_lab, bins=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the dataset is not balanced. We need to use recall, precision and F1-score to evaluate a model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the dataset into training and testing parts. Validations is not required for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "p_test = 0.1\n",
    "n_test = round(p_test * len(data_lab))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_mes, data_lab, random_state=0, \n",
    "                                                    test_size=n_test, shuffle=True)\n",
    "\n",
    "print(f\"train size {len(y_train)}\")\n",
    "print(f\" test size {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classifier and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = NaiveBayes(k=1, drop_short=2)\n",
    "nbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `sklearn` library to evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred, average='binary', pos_label='spam')\n",
    "\n",
    "print(f\"Accuracy = {acc:.4f}\")\n",
    "print(f\"F1-score = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-scroe is not so high. Let us check what is the reason.\n",
    "\n",
    "Here are recall and precision. The function also returns support for multivariate metrics, but now we compute the binary one and it is empty. We ignore it, using `_` symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, f1, _ = metrics.precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label='spam')\n",
    "\n",
    "print(f\"Precision = {prec:.4f}\")\n",
    "print(f\"Recall    = {rec:.4f}\")\n",
    "print(f\"F1-score  = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see very the high precision and not so high recall. \n",
    "\n",
    "Let us remember: precision is a fraction of true positive predictions with respect of all positive predictions done by a binary classifier.\n",
    "\n",
    "Recall is a fraction of true positive predictions with respect of all positive cases in a dataset.\n",
    "\n",
    "In our case high precision indicates that if the filter reports 'spam' it will be true in 89 percent of cases.\n",
    "\n",
    "And it means that 11 percent of good messaged will be wrongly marked as 'spam'.\n",
    "\n",
    "Recall indicates that in the whole stream of messages only 71 percent of spam messages will be revealed and 29 percent of them will pass.\n",
    "\n",
    "This is not so bad for the spam filter. It is better to let some spam go then to mark as spam the expected message.\n",
    "\n",
    "Let us explore the vocabulary that is collected in the course of training. \n",
    "\n",
    "The method `.explore_vocab` computes the spam / ham scores and return the most spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = nbc.explore_vocab()\n",
    "print(spam_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are many words that do not correspond to our intuition about what spam words can be. \n",
    "\n",
    "But nevertheless the filter works and has not bad scores as we have seen above. \n",
    "\n",
    "It means that it detects spam on the basis of the word combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy gaussian naive Bayes classifier\n",
    "\n",
    "Above we considered categorical features: a token can be or can not be in the analyzed message.\n",
    "\n",
    "If the features (data columns) are floating point numbers the similar approach can be developed. \n",
    "\n",
    "But now the basic assumption is that values of each column are sampled from a Gaussian distribution and the columns have no pairwise correlation, i.e., are independent.\n",
    "\n",
    "Let us remember: Gaussian or also called normal distribution reads \n",
    "\n",
    "$$\n",
    "\\rho(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\mathrm{exp}\n",
    "\\left(\n",
    "-\\frac{(x-\\mu)^2}{2\\sigma^2}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are two its parameters, the mean and the standard deviation, respectively.\n",
    "\n",
    "Assume that we have a dataset with two real-valued features and two classes.\n",
    "\n",
    "| weight (g.) | diameter (cm.) | label  |\n",
    "|-------------|----------------|--------|\n",
    "| 150         | 10.2           | orange |\n",
    "| 120         | 12.1           | orange |\n",
    "| 98          | 10.5           | orange |\n",
    "| 140         | 9.6            | orange |\n",
    "| 95          | 10.0           | apple  |\n",
    "| 105         | 8.5            | apple  |\n",
    "| 90          | 9.3            | apple  |\n",
    "| 94          | 9.8            | apple  |\n",
    "| 101         | 10.1           | apple  |\n",
    "\n",
    "Let us create a Gaussian naive Bayes classifier using these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.array([150, 120, 98, 140, 95, 105, 90, 94, 101])\n",
    "diameter = np.array([10.2, 12.1, 10.5, 9.6, 10.0, 8.5, 9.3, 9.8, 10.1])\n",
    "label = np.array(['orange'] * 4 + ['apple'] * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to compute mean $\\mu$ and standard deviation $\\sigma$ for each feature and each class.\n",
    "\n",
    "We compute mean values and standard deviations for conditional distributions:\n",
    "- weights of oranges\n",
    "- weights of apples\n",
    "- diameters of oranges\n",
    "- diameters of apples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_std_w_o = (np.mean(weight[label=='orange']), np.std(weight[label=='orange']))\n",
    "mu_std_w_a = (np.mean(weight[label=='apple']),  np.std(weight[label=='apple']))\n",
    "\n",
    "mu_std_d_o = (np.mean(diameter[label=='orange']), np.std(diameter[label=='orange']))\n",
    "mu_std_d_a = (np.mean(diameter[label=='apple']),  np.std(diameter[label=='apple']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we need the marginal probabilities of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_o = len(label[label=='orange']) / len(label)\n",
    "p_a = len(label[label=='apple']) / len(label)\n",
    "assert p_o + p_a == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Gaussian distribution with given $\\mu$ and $\\sigma$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_gauss(x, mu_std):\n",
    "    mu, std = mu_std\n",
    "    return np.exp(-(x-mu)**2 / (2*std*std)) / (std * np.sqrt(2*np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we a ready to test our model. \n",
    "\n",
    "Lets us find the most probable class for a fruit with a certain weight and diameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_if_orange=2.793432e-03\n",
      "prob_if_apple =3.891247e-07\n",
      "orange\n"
     ]
    }
   ],
   "source": [
    "w_test, d_test = 122, 9.9\n",
    "\n",
    "prob_if_orange = p_gauss(w_test, mu_std_w_o) * p_gauss(d_test, mu_std_d_o) * p_o\n",
    "prob_if_apple  = p_gauss(w_test, mu_std_w_a) * p_gauss(d_test, mu_std_d_a) * p_a\n",
    "\n",
    "print(f\"prob_if_orange={prob_if_orange:e}\")\n",
    "print(f\"prob_if_apple ={prob_if_apple:e}\")\n",
    "print(\"orange\" if prob_if_orange > prob_if_apple else \"apple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `prob_if_orange > prob_if_apple` we conclude that this is an orange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian naive Bayes classifier\n",
    "\n",
    "Let us now consider Gaussian naive Bayes classifier from `sklearn`.\n",
    "\n",
    "We are going to created a model that will predict a car origin using its technical characteristics.\n",
    "\n",
    "First we need to download the dataset. The file is called 'cars.csv' and previously we already used it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "def load_csv_dataset(file_name, dtype=float):\n",
    "    \"\"\"Downloads csv numeric dataset from repo to numpy array.\"\"\"\n",
    "    base_url = \"https://raw.githubusercontent.com/kupav/data-sc-intro/main/data/\"\n",
    "    web_data = requests.get(base_url + file_name)\n",
    "    assert web_data.status_code == 200\n",
    "    \n",
    "    reader = csv.reader(web_data.text.splitlines(), delimiter=';')\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        try:\n",
    "            # Try to parse as a row of floats\n",
    "            float_row = [dtype(x) for x in row]\n",
    "            data.append(float_row)\n",
    "        except ValueError:\n",
    "            # If parsing as floats failed - this is header\n",
    "            print(row)\n",
    "            \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Car' 'MPG' 'Cylinders' 'Displacement' 'Horsepower' 'Weight'\n",
      "  'Acceleration' 'Model' 'Origin']\n",
      " ['STRING' 'DOUBLE' 'INT' 'DOUBLE' 'DOUBLE' 'DOUBLE' 'DOUBLE' 'INT' 'CAT']\n",
      " ['Chevrolet Chevelle Malibu' '18.0' '8' '307.0' '130.0' '3504.' '12.0'\n",
      "  '70' 'US']\n",
      " ['Buick Skylark 320' '15.0' '8' '350.0' '165.0' '3693.' '11.5' '70' 'US']\n",
      " ['Plymouth Satellite' '18.0' '8' '318.0' '150.0' '3436.' '11.0' '70'\n",
      "  'US']]\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_csv_dataset(\"cars.csv\", dtype=str)\n",
    "print(raw_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row contains columns headers, the second one provides information about the types.\n",
    "\n",
    "We use as data the whole table except the first and the last columns.\n",
    "\n",
    "The first one, car name, will be merely omitted, and the last one 'Origin' will be \n",
    "considered as class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MPG' 'Cylinders' 'Displacement' 'Horsepower' 'Weight' 'Acceleration'\n",
      " 'Model']\n",
      "[[  18.     8.   307.   130.  3504.    12.    70. ]\n",
      " [  15.     8.   350.   165.  3693.    11.5   70. ]\n",
      " [  18.     8.   318.   150.  3436.    11.    70. ]\n",
      " [  16.     8.   304.   150.  3433.    12.    70. ]\n",
      " [  17.     8.   302.   140.  3449.    10.5   70. ]]\n",
      "['US' 'US' 'US' 'US' 'US']\n"
     ]
    }
   ],
   "source": [
    "feature_names = raw_data[0, 1:-1]\n",
    "data = raw_data[2:, 1:-1].astype(float)\n",
    "str_targets = raw_data[2:, -1]\n",
    "print(feature_names)\n",
    "print(data[:5])\n",
    "print(str_targets[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate unique labels and dictionaries to map from labels to numerical codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Europe', 'US', 'Japan']\n",
      "[(0, 'Europe'), (1, 'US'), (2, 'Japan')]\n"
     ]
    }
   ],
   "source": [
    "unique_targets = list(set(str_targets))\n",
    "print(unique_targets)\n",
    "enum_targets = list(enumerate(unique_targets))\n",
    "print(enum_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Europe', 1: 'US', 2: 'Japan'}\n"
     ]
    }
   ],
   "source": [
    "code2label = dict(enum_targets)\n",
    "print(code2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Europe': 0, 'US': 1, 'Japan': 2}\n"
     ]
    }
   ],
   "source": [
    "label2code = {v: k for k, v in code2label.items()}\n",
    "print(label2code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dictionary we encode each country name with the corresponding numerical code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 2 1 1 1 2 0 0 0 0 0 1 1 1 1 1 2 1\n",
      " 2 1 0 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "targets = np.array([label2code[lab] for lab in str_targets])\n",
    "print(targets[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(str_targets, bins=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that US appears much more often - the dataset is highly unbalanced.\n",
    "\n",
    "Accuracy metrics will not be informative in this case. F1-scores will be computed.\n",
    "\n",
    "The shape of our dataset is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406, 7)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split it into training and testing parts. \n",
    "\n",
    "No validation will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(325, 7)\n",
      "(81, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "p_test = 0.2\n",
    "\n",
    "n_test = round(len(data) * p_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, targets, random_state=0, test_size=n_test, shuffle=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Europe      0.500     0.308     0.381        13\n",
      "          US      0.955     0.792     0.866        53\n",
      "       Japan      0.448     0.867     0.591        15\n",
      "\n",
      "    accuracy                          0.728        81\n",
      "   macro avg      0.634     0.656     0.613        81\n",
      "weighted avg      0.788     0.728     0.737        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=unique_targets, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model revels good only US cars. Corresponding precision and recall are 96 percent and 79 percent, respectively. \n",
    "\n",
    "For other cars the performance is not so good.\n",
    "\n",
    "The reason is small support, i.e., small number of samples from Japan and Europe, 15 and 13, respectively. \n",
    "\n",
    "The estimates of mean value and standard deviation for such small number of samples are not so representative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strengths and weaknesses of naive Bayes classifier\n",
    "\n",
    "Strong points:\n",
    "\n",
    "- Easy to implement, works fast\n",
    "- Can be used on datasets with combined categorical and numerical data (Gaussian probabilities are combined with the multinomial)\n",
    "\n",
    "Weak points:\n",
    "\n",
    "- If categorical variable has a category, which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. Artificial fix is required, e.g., pseudocount.\n",
    "- Computed values can be used only for finding maximum likelihood. Estimated probabilities are not to be taken too seriously\n",
    "    \n",
    "Applications of naive Bayes classifiers:\n",
    "\n",
    "- Real time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.\n",
    "- Multi class Prediction: This algorithm is also well known for multi class prediction feature. \n",
    "- Text classification, spam filtering, sentiment analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n",
    "- Recommendation system: Naive Bayes classifier are used to predict whether a user would like a given resource or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1\\. Describe in writing what an assumption is made when a naive Bayes classifier is created. Why the classifier is naive?\n",
    "\n",
    "2\\. Describe in writing what means maximum likelihood?\n",
    "\n",
    "3\\. Make a copy of a naive Bayes classifier that we used above to create a spam filter and try to improve its performance.\n",
    "Split the data set into training, validation and test data. Select the best model using the validation dataset and then compute your final score on the testing data. To improve the model for example the whole message content can be taken into account instead of the subject only. Also lengths of tokens that are taken into account can be varied. May be it would be interesting to split the messages into digramms: couples of words going one after another. And so on.\n",
    "\n",
    "4\\. Try to improve the Gaussian naive Bayes classifier. Split the data set into training, validation and test data. Select the best model using the validation dataset and then compute your final score on the testing data. \n",
    "\n",
    "5\\. Previously we discussed that in the most cases data must be standardized before creation of a machine learning model. Why it does not influences the performance of a Gaussian naive Bayes classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "ru",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "113px",
    "width": "228px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
