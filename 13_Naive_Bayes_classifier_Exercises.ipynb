{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 Naive Bayes classifier, Exercises\n",
    "\n",
    "Part of [\"Introduction to Data Science\" course](https://github.com/kupav/data-sc-intro) by Pavel Kuptsov, [kupav@mail.ru](mailto:kupav@mail.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = dict()\n",
    "d1['one'] = 0\n",
    "try:\n",
    "    d1['one'] += 1  # it's ok, since we already initialized this key\n",
    "    d1['two'] += 1  # it will be an error since key 'two' is undefined\n",
    "    print(\"Finished successfully\")  # this line will not be executed\n",
    "except KeyError as e:\n",
    "    print(\"KeyError\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d2 = defaultdict(int)\n",
    "d2['one'] = 0\n",
    "try:\n",
    "    d2['one'] += 1  # it's ok, since we already initialized this key\n",
    "    d2['two'] += 1  # it is also ok since default value of int (zero) will be assumed \n",
    "    print(\"Finished successfully\") \n",
    "except KeyError as e:\n",
    "    print(\"KeyError\", e)  # this line will not be executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us remember the Python sets.\n",
    "\n",
    "A set does not allow duplicates. Thus when a list is converted to a set, all duplicates are removed.\n",
    "\n",
    "Then the set can be converted back to a list if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = ['one', 'one', 'one', 'two', 'two', 'three']\n",
    "print(s1)\n",
    "s2 = set(s1)\n",
    "print(s2)\n",
    "s3 = list(s2)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets are convenient when we need to store only unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "print(vocab)\n",
    "vocab.add(\"one\")\n",
    "print(vocab)\n",
    "vocab.add(\"one\")\n",
    "print(vocab)\n",
    "vocab.update([\"one\", \"two\", \"three\"])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if a value belong to a set using `in` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('one' in vocab)\n",
    "print('one' not in vocab)\n",
    "print('once' not in vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us remember regular expressions. \n",
    "\n",
    "Pattern \"\\[a-z'\\]\" matches any letter or apostrophe. And if we add plus like this: \"\\[a-z'\\]+\" it means that the pattern matches \n",
    "one or more occurrences of the symbols. This pattern is the simplest way to match words in a text.\n",
    "\n",
    "Usually we ignore the difference between small and capital letters. Thus it is convenient to convert the text to the lower \n",
    "case using `.lower()` method of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "txt = \"\"\"I've a cat named Vesters,\n",
    "And he eats all day.\n",
    "He always lays around,\n",
    "And never wants to play.\n",
    "\"\"\"\n",
    "\n",
    "rge = re.compile(r\"[a-z']+\")\n",
    "print(rge.findall(txt.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two final technical remarks: We will compute probabilities of a word $W_i$ to appear in spam and in ham messages. \n",
    "\n",
    "Assume that this word appears only in spam messages. Then the probability to see it in ham messages will be zero.\n",
    "\n",
    "Since we compute the product of probabilities $P(W_1|H)P(W_2|H)P(W_3|H)\\ldots$ vanishing of one of the elements, say $P(W_2|H)=0$, zeroing the whole product. \n",
    "\n",
    "In this case all messages with $W_i$ will always be classified as spam since the opposite probability will always be zero.\n",
    "\n",
    "To avoid it we add a pseudocount $k$ when compute the probabilities:\n",
    "\n",
    "$$\n",
    "P(W_i | S) = \\frac{k + \\text{number of spam messages with $W_i$}}{2k + \\text{total number of spam messages}}\n",
    "$$\n",
    "\n",
    "Usually $k=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a classification of a message that contains a set of tokes (words)\n",
    "$(W_1, W_2, \\ldots W_n)$ we need to compute likelihoods that it is a spam and a ham:\n",
    "\n",
    "$$\n",
    "P(S | W_1,W_2,\\ldots W_n) \\propto P(W_1 | S) P(W_2 | S) \\ldots P(W_n | S) P(S)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(H | W_1,W_2,\\ldots W_n) \\propto P(W_1 | H) P(W_2 | H) \\ldots P(W_n | H) P(H)\n",
    "$$\n",
    "\n",
    "Then if $P(S | W_1,W_2,\\ldots W_n) > P(H | W_1,W_2,\\ldots W_n)$ we classify it as a spam and this is a ham in the other case.\n",
    "\n",
    "The expressions contains a lot of multiplications:\n",
    "\n",
    "$$\n",
    "P(W_1 | S) P(W_2 | S) \\ldots P(W_n | S) P(S), \\; P(W_1 | H) P(W_2 | H) \\ldots P(W_n | H) P(H)\n",
    "$$\n",
    "\n",
    "Such computations are numerically unstable. \n",
    "\n",
    "We can easily obtain an underflow when values approach zero. Numerical errors can enormously grow.\n",
    "\n",
    "We can avoid it by using logarithms. Let us remember:\n",
    "\n",
    "$$\n",
    "\\log(a \\times b) = \\log a + \\log b\n",
    "$$\n",
    "\n",
    "It means that if we sum logarithms of the factors we will have a logarithm of their product.\n",
    "\n",
    "Given the logarithms of the probabilities we do not need to compute back the probabilities themselves. \n",
    "\n",
    "Since we want to compare the probabilities we will collect sums of logarithms and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Describe in writing what an assumption is made when a naive Bayes classifier is created. Why the classifier is naive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Describe in writing what means maximum likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Make a copy of a naive Bayes classifier that we used above to create a spam filter and try to improve its performance.\n",
    "Split the data set into training, validation and test data. Select the best model using the validation dataset and then compute your final score on the testing data. To improve the model for example the whole message content can be taken into account instead of the subject only. Also lengths of tokens that are taken into account can be varied. May be it would be interesting to split the messages into digramms: couples of words going one after another. And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Try to improve the Gaussian naive Bayes classifier. Split the data set into training, validation and test data. Select the best model using the validation dataset and then compute your final score on the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Previously we discussed that in the most cases data must be standardized before creation of a machine learning model. Why it does not influences the performance of a Gaussian naive Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "ru",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "113px",
    "width": "228px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
