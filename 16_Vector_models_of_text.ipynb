{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 Vector models of text\n",
    "\n",
    "Part of [\"Introduction to Data Science\" course](https://github.com/kupav/data-sc-intro) by Pavel Kuptsov, [kupav@mail.ru](mailto:kupav@mail.ru)\n",
    "\n",
    "Recommended reading for this section:\n",
    "\n",
    "1. A. Kedia and M. Rasu (2020) Hands-On Python Natural Language Processing. Packt Publishing\n",
    "1. T. Srivastava. NLP: A quick guide to Stemming. https://medium.com/@tusharsri/nlp-a-quick-guide-to-stemming-60f1ca5db49e\n",
    "1. J. Brownlee. How to Clean Text for Machine Learning with Python. https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "1. S. Prabhakaran. Gensim Tutorial - A Complete Beginners Guide. https://www.machinelearningplus.com/nlp/gensim-tutorial/#14howtotrainword2vecmodelusinggensim\n",
    "1. K. Ganesan. Gensim Word2Vec Tutorial - Full Working Example. https://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/\n",
    "\n",
    "\n",
    "The following Python modules will be required. Make sure that you have them installed.\n",
    "- `spacy`\n",
    "- `gensim`\n",
    "- `pyemd` (requires `gensim` to compute word mover’s distance)\n",
    "- `pot` (requires `gensim` to compute word mover’s distance)\n",
    "- `multiprocessing`\n",
    "- `re`\n",
    "- `nltk`\n",
    "- `sklearn`\n",
    "- `numpy`\n",
    "- `collections`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install spacy and its module for English uncomment and execute the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy && python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the this lecture we will closely follow the book [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary text processing: from natural language to mathematical object\n",
    "\n",
    "Each natural language processing (NLP) problem requires formalizing of the text under consideration.\n",
    "\n",
    "Each text consists of parts: chapters, paragraphs, sentences, words. \n",
    "\n",
    "We can easily reveal the parts manually just by observing the text. \n",
    "\n",
    "But for a computer this is not so simple problem. \n",
    "\n",
    "The text parts have to be found and isolated so that each one becomes an element of a data structure, e.g., of a lists.\n",
    "\n",
    "Parts of each text have complicated semantic and grammar connections. Some of them have to be revealed and preserved others  can be omitted (depends of the final goal of the text processing).\n",
    "\n",
    "Each text has ambiguities that have to be resolved. \n",
    "\n",
    "The result of this work is a vocabulary of the text. \n",
    "\n",
    "Then each vocabulary element have to be represented as a mathematical object, usually a vector.\n",
    "\n",
    "Finally using vocabulary and the corresponding vectors the text as a whole is represented as a mathematical object, a set of vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting\n",
    "\n",
    "If we start working with text the first step can be extraction of its sentences and collecting them e.g. as list elements.\n",
    "\n",
    "The splitting can be done using Python build-in method `.split()`. We only need to specify a symbol of splitting that is \".\" (dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"Oh, God\", he thought, \"what a strenuous career it is \n",
    "that I've chosen! Travelling day in and day out.  Doing business \n",
    "like this takes much more effort than doing your own business at home.  \n",
    "It can all go to Hell!\"  He felt a slight itch up on his belly; \n",
    "pushed himself slowly up on his back towards the headboard so that \n",
    "he could lift his head better. \"\"\"\n",
    "sentences = text.split(\".\")\n",
    "for s in sentences:\n",
    "    # This line is requited to show all new line symbols as '\\n'\n",
    "    a = s.encode('unicode-escape').decode().replace('\\\\\\\\', '\\\\')    \n",
    "    print(f\"=*={a}=*=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe however that other sentence separators like exclamation point is ignored. We could write some code to take it into account.\n",
    "\n",
    "But it is much more convenient to use a library `nltk`, Natural Language Toolkit. It provides a lot of powerful tools for NLP.\n",
    "\n",
    "Function that splits a text into sentences is called `.sent_tokenize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "for s in sentences:\n",
    "    # This line is requited to show all new line symbols as '\\n'\n",
    "    a = s.encode('unicode-escape').decode().replace('\\\\\\\\', '\\\\')    \n",
    "    print(f\"=*={a}=*=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK splitter correctly takes into account all sentence separators and also automatically trims white spaces at sentence ends.\n",
    "\n",
    "Notice that splitting a text into sentences is only required if the sentences are planned to be analyzed individually as special parts of text structure. Otherwise we can just skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning \n",
    "\n",
    "A text contain a lot of non-textual symbols.\n",
    "\n",
    "First of all this are new line symbols \"\\n\". There can be tabulations \"\\t\" and some non textual characters like asterisk \"*\".\n",
    "\n",
    "Punctuation marks are also typically not needed for further text processing.\n",
    "\n",
    "The cleaning can be done using Python regular expressions.\n",
    "\n",
    "The function `text_clean` converts all symbols that are not alphanumerical or apostrophes to spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_clean(text):\n",
    "    new_text = []\n",
    "    for s in text:\n",
    "        s1 = re.sub(pattern=\"[^\\w']\", repl=' ', string=s)\n",
    "        new_text.append(s1)\n",
    "    return new_text\n",
    "\n",
    "print(text_clean(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice however, that the cleaning can be done also lately after splitting sentences into separated tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text tokenization\n",
    "\n",
    "In order to build up a vocabulary, the first thing to do is to break the text into chunks called tokens. \n",
    "\n",
    "Each token carries a semantic meaning associated with it. \n",
    "\n",
    "Tokenization is one of the fundamental things to do in any text-processing activity.\n",
    "\n",
    "Tokenization can be thought of as a segmentation technique wherein you are trying to\n",
    "break down larger pieces of text chunks into smaller meaningful ones. \n",
    "\n",
    "Tokens generally comprise words and numbers, but they can be extended to include punctuation marks,\n",
    "symbols, and, at times, understandable emoticons.\n",
    "\n",
    "The simplest way to do tokenization provides Python built-in method `.split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The capital of China is Beijing\"\n",
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this simple way can not deal correctly with multiple complicated cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Beijing is where we'll go\"\n",
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problematic token here is \"we'll\". \n",
    "\n",
    "Actually there are two tokens here: one is pronoun \"we\" and the other is reduced verb \"will\".\n",
    "\n",
    "If later a grammar analysis is planned these token must be separated. \n",
    "\n",
    "Another complicated case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Let's travel to Hong Kong from Beijing\"\n",
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously here \"Kong\"  must be attached to \"Hong\" and they both must be considered as a single token.\n",
    "\n",
    "But in the sentence below \"Kong\" is a standing along token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The name of the King is Kong\"\n",
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also there is a problem with boundary of sentences detection.\n",
    "\n",
    "In the example below the period between M and S is actually indicative of an abbreviation, nut not the sentence boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"A friend is pursuing his M.S from Beijing. He realy likes it.\"\n",
    "print(sentence.split('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of tokenization has no a single, the perfect for all cases solution. \n",
    "\n",
    "There are many tokenization methods that works better for different applications.\n",
    "\n",
    "Basically the tokenizer use regular expressions may be with some additional more or less smart processing.\n",
    "\n",
    "The basic tokenizer in NLTK is `.word_tokenize()`. It return a tokenized copy of passed text using NLTK's recommended word tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sentence = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenizer woks good in most cases, however here it split dollar sign `$` and the amount of many. \n",
    "\n",
    "In situations like this we need more control on the tokenization by specifying a regular expression explicitly.\n",
    "\n",
    "This cane be done with the class `RegexpTokenizer`. It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regular expression wokrs as follows. \n",
    "\n",
    "The `\\w+|\\$[\\d\\.]+|\\S+` regular expression allows three alternative patterns:\n",
    "\n",
    "- First alternative: `\\w` that matches any word character (equal to `[a-zA-Z0-9_]`). \n",
    "The + is a quantifier and matches between one and unlimited times as many\n",
    "times as possible.\n",
    "- Second alternative: `\\$[\\d\\.]+`. Here, `\\$` matches the character `$`, `\\d` matches a\n",
    "digit between 0 and 9, `\\.` matches the character `.` (period), and `+` again acts as a\n",
    "quantifier matching between one and unlimited times.\n",
    "- Third alternative: `\\S+`. Here, `\\S` accepts any non-whitespace character and `+`\n",
    "again acts the same way as in the preceding two alternatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text in social media is much less formal and requires a specific tokenizers. \n",
    "\n",
    "People tag each other using their social media handles and use a lot of emoticons, hashtags,\n",
    "and abbreviated text to express themselves.\n",
    "\n",
    "For this purpose NLTK provides `TweetTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "sentence = \"\"\"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\"\"\n",
    "tokenizer = TweetTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common thing with social media writing is the use of expressions such\n",
    "as `Rolexxxxxxxx`. \n",
    "\n",
    "Here, a lot of x's are present in addition to the normal one; it is a very\n",
    "common trend and should be addressed to bring it to a form as close to normal as possible.\n",
    "\n",
    "The `TweetTokenizer` provides two additional parameters. \n",
    "\n",
    "The first one `reduce_len` tries to reduce the excessive characters in a token. \n",
    "\n",
    "The word Rolexxxxxxxx is actually tokenized as Rolexxx in an attempt to reduce the number of x's present.\n",
    "\n",
    "The parameter `strip_handles`, when set to True, removes the handles mentioned in a\n",
    "post/tweet. \n",
    "\n",
    "As can be seen in the preceding output, `@amankedia` is stripped, since it is a handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "sentence = \"\"\"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\"\"\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning after tokenization\n",
    "\n",
    "Previously we considered text cleaning done before splitting a text into tokens.\n",
    "\n",
    "NLTK tokenizers extracts non-alphanumeric symbols into separate tokens.\n",
    "\n",
    "And all new line symbols are treated as token separators and thus are removed.\n",
    "\n",
    "We just need to run along token list and filter out non-alphanumeric tokens.\n",
    "\n",
    "For this purpose the method `.isalpha()` can be used. It returns `True` if all characters in the string are alphabets, otherwise, it returns `False`.\n",
    "\n",
    "In addition we convert all tokens to lower case.\n",
    "\n",
    "Pay attention that before checking `.isalpha()` we test if the first symbol is apostrophe. This is required to avoid removing\n",
    "reduced verbs like \"'ve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"Oh, God\", he thought, \"what a strenuous career it is \n",
    "that I've chosen! Travelling day in and day out.  Doing business \n",
    "like this takes much more effort than doing your own business at home.  \n",
    "It can all go to Hell!\"  He felt a slight itch up on his belly; \n",
    "pushed himself slowly up on his back towards the headboard so that \n",
    "he could lift his head better. \"\"\"\n",
    "\n",
    "import nltk\n",
    "raw_tokens = nltk.word_tokenize(text)\n",
    "tokens = []\n",
    "for tok in raw_tokens:\n",
    "    t1 = tok[1:] if tok[0] == \"'\" else tok  # we do not want to remove tokens like \"'ve\" \n",
    "    if t1.isalpha():\n",
    "        tokens.append(tok.lower())\n",
    "        \n",
    "print(raw_tokens)\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word normalization\n",
    "\n",
    "Most of the time, we do not want to have every individual word fragment that we have ever encountered in our vocabulary. \n",
    "\n",
    "Probably we will want to bring words to their root form in the dictionary. \n",
    "\n",
    "For instance, \"am\", \"are\", and \"is\" can be identified by their root form, \"be\". \n",
    "\n",
    "Also we can remove inflections from words to bring them down to the same form: Words car, cars, and car's can all be identified as car.\n",
    "\n",
    "Also, common words that occur very frequently and do not convey much meaning, such as the articles \"a\", \"an\", and \"the\", can be removed. \n",
    "\n",
    "However, all these highly depend on the use cases. \n",
    "\n",
    "\"Wh-\" words, such as \"when\", \"why\", \"where\", and \"who\", do not carry much information in most contexts and are removed as part of a technique called stopword removal.\n",
    "\n",
    "However, in situations such as question classification and question answering, these words become very important and should not be removed. \n",
    "\n",
    "Word normalization process includes the following procedures:\n",
    "- Case folding: converting all letters in the text corpus into lowercase.\n",
    "- Stopword removal: removing words such as a, an, the, in, at, and so on that occur frequently in text corpora\n",
    "and do not carry a lot of information in most contexts.\n",
    "- Stemming: removing all inflections to bring words to their basic form.\n",
    "- Lemmatization is a process wherein the context is used to convert a word to its meaningful base form. \n",
    "- N-grams: grouping multiple tokens.\n",
    "\n",
    "These steps should be performed as part of preprocessing the text corpora before applying any algorithms to the data. \n",
    "\n",
    "However, which steps to apply and which to ignore depend on the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case folding\n",
    "\n",
    "Usually the fist step in word normalization is case folding. \n",
    "\n",
    "As part of case folding, all the letters in the text corpus are converted to lowercase. \n",
    "\n",
    "\"The\" and \"the\" will betreated the same in a scenario of case folding, whereas they would be treated differently in\n",
    "a non-case folding scenario. \n",
    "\n",
    "This technique helps systems that deal with information retrieval, such as search engines.\n",
    "\n",
    "Lamborghini, which is a proper noun, will be treated as lamborghini; whether the user typed\n",
    "Lamborghini or lamborghini would not make a difference, and the same results would be\n",
    "returned.\n",
    "\n",
    "However, in situations where proper nouns are derived from common noun terms, case\n",
    "folding will become a bottleneck as case-based distinction becomes an important feature\n",
    "here. \n",
    "\n",
    "For instance, General Motors is composed of common noun terms but is itself a proper\n",
    "noun. \n",
    "\n",
    "Performing case folding here might cause issues. \n",
    "\n",
    "Another problem is when acronyms are converted to lowercase. \n",
    "\n",
    "There is a high chance that they will map to common nouns. \n",
    "\n",
    "An example widely used here is CAT which stands for Common Admission Test in India getting converted to cat.\n",
    "\n",
    "A potential solution to this is to build machine learning models that can use features from a\n",
    "sentence to determine which words or tokens in the sentence should be lowercase and\n",
    "which shouldn't be; however, this approach doesn't always help when users mostly type in\n",
    "lowercase. \n",
    "\n",
    "As a result, lowercasing everything becomes a wise solution.\n",
    "\n",
    "The language here is a major feature; in some languages, such as English, capitalization\n",
    "from point to point in a text carries a lot of information, whereas in some other languages,\n",
    "cases might not be as important.\n",
    "\n",
    "The following code shows a very straightforward approach that would convert all\n",
    "letters in a sentence to lowercase, making use of the lower() method available in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"At the age of twenty, Susan Calvin had been part of the particular \n",
    "Psycho-Math seminar at which Dr. Alfred Lanning of U. S. Robots had demonstrated \n",
    "the first mobile robot to be equipped with a voice.\"\"\"\n",
    "s = s.lower()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword removal\n",
    "\n",
    "Stopwords are words such as \"a\", \"an\", \"the\", \"in\", \"at\", and so on that occur frequently in text corpora\n",
    "and do not carry a lot of information in most contexts. \n",
    "\n",
    "These words, in general, are required for the completion of sentences and making them grammatically sound. \n",
    "\n",
    "They are often the most common words in a language and can be filtered out in most NLP tasks, and\n",
    "consequently help in reducing the vocabulary or search space. \n",
    "\n",
    "There is no single list of stopwords that is available universally, and they vary mostly based on use cases.\n",
    "\n",
    "However, a certain list of words is maintained for languages that can be treated as stopwords specific\n",
    "to that language, but they should be modified based on the problem that is being solved.\n",
    "\n",
    "Let’s look at the stopwords available for English in the nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\", \".join(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely, you'll notice that Wh- words such as who, what, when, why, how, which,\n",
    "where, and whom are part of this list of stopwords.\n",
    "\n",
    "However, previously it was mentioned that these words are very significant in use cases such as question\n",
    "answering and question classification. \n",
    "\n",
    "Measures should be taken to ensure that these words are not filtered out when the text corpus undergoes \n",
    "stopword removal. \n",
    "\n",
    "Let's learn how this can be achieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_words = set(['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom'])\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "sentence = \"how are we putting in efforts to enhance our understanding of NLP\"\n",
    "\n",
    "# Brfore removing stop words we clean them from the wh-words\n",
    "clean_stop = stop - wh_words\n",
    "    \n",
    "sentence_after_stopword_removal = [token for token in sentence.split() if token not in clean_stop]\n",
    "\" \".join(sentence_after_stopword_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above shows that the sentence \"how are we putting in efforts\n",
    "to enhance our understanding of Lemmatization\" gets modified to \"how putting\n",
    "efforts enhance understanding NLP\". \n",
    "\n",
    "Observe that words \"are\", \"we\", \"in\", \"to\", \"our, \"of\" were removed from the sentence. \n",
    "\n",
    "In some case removing of verbs may undesirable. In this case that must be protected in the same way as \"wh-\" words.\n",
    "\n",
    "Stopword removal is generally the first step that is taken after tokenization while building a vocabulary or preprocessing text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is bringing all of the words like computer, computerization, and computerize into one word, compute. \n",
    "\n",
    "The stemming is removing the inflectional forms of a word and bringing them to a base form called the stem. \n",
    "\n",
    "The chopped-off pieces are referred to as affixes. \n",
    "\n",
    "In the preceding example, compute is the base form and the affixes are r, rization, and rize, respectively. \n",
    "\n",
    "One thing to keep in mind is that the stem need not be a valid word as we know it. \n",
    "\n",
    "For example, the word traditional would get stemmed to tradit, which is not a valid word in the English dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most common methods employed for stemming include the Porter stemmer and the Snowball stemmer. \n",
    "\n",
    "The Porter stemmer supports the English language, whereas the Snowball stemmer, which is an improvement on the Porter stemmer, supports multiple languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now first apply the Porter stemmer to words and see its effects in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n",
    "    'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted', 'having', 'generously']\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see how the Snowball stemmer would do on the same text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer2 = SnowballStemmer(language='english')\n",
    "singles = [stemmer2.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the Snowball stemmer requires the specification of a language parameter. \n",
    "\n",
    "In most of cases, its output is similar to that of the Porter stemmer, except for generously, \n",
    "where the Porter stemmer outputs gener and the Snowball stemmer outputs generous. \n",
    "\n",
    "The example shows how the Snowball stemmer makes minor changes to the Porter algorithm, achieving improvements in some cases.\n",
    "\n",
    "Potential problems with stemming arise in the form of over-stemming and under-\n",
    "stemming. \n",
    "\n",
    "A situation may arise when words that are stemmed to the same root should have been stemmed to different roots. \n",
    "\n",
    "This problem is referred to as __over-stemming__. This is also known as a false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer2 = SnowballStemmer(language='english')\n",
    "words = ['universal', 'university', 'universe']\n",
    "stemmed = [stemmer2.stem(s) for s in words]\n",
    "print(' '.join(stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the above 3 words are stemmed to univers which is wrong behavior.\n",
    "\n",
    "Though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in NLP will likely reduce the relevance of the results\n",
    "\n",
    "In contrast, another problem occurs when words that should have been stemmed to the same\n",
    "root aren't stemmed to it. This situation is referred to as __under-stemming__. This is also known as a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer2 = SnowballStemmer(language='english')\n",
    "words = ['alumnus', 'alumni', 'alumnae']\n",
    "stemmed = [stemmer2.stem(s) for s in words]\n",
    "print(' '.join(stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to Porter and Snowball there are more algorithms for stemming. \n",
    "\n",
    "Other stemmers include the Lancaster, Dawson, Krovetz, and Lovins stemmers, among others. \n",
    "\n",
    "Each stemmer can do under- and over-stemming. \n",
    "\n",
    "The better one should be chosen according to the goals of the study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Unlike stemming, wherein a few characters are removed from words using crude methods,\n",
    "lemmatization is a process wherein the context is used to convert a word to its meaningful\n",
    "base form. \n",
    "\n",
    "It helps in grouping together words that have a common base form and so can\n",
    "be identified as a single item. \n",
    "\n",
    "The base form is referred to as the lemma of the word and is also sometimes known as the dictionary form.\n",
    "\n",
    "Lemmatization algorithms try to identify the lemma form of a word by taking into account\n",
    "the neighborhood context of the word, part-of-speech (POS) tags, the meaning of a word,\n",
    "and so on. \n",
    "\n",
    "The neighborhood can span across words in the vicinity, sentences, or even documents.\n",
    "\n",
    "Also, the same words can have different lemmas depending on the context. \n",
    "\n",
    "A lemmatizer would try and identify the part-of-speech tags based on the context to identify the\n",
    "appropriate lemma. \n",
    "\n",
    "Stemming and lemmatization obviously try to do the same job. But stemming does it a more simple way and thus faster. \n",
    "\n",
    "Sometimes stemming is enough and in more complicated cases the lemmatization resluts in the better performance.\n",
    "\n",
    "The most commonly used lemmatizer is the WordNet lemmatizer.\n",
    "\n",
    "Other lemmatizers include the Spacy lemmatizer, TextBlob lemmatizer, and Gensim lemmatizer, and others. \n",
    "\n",
    "Below we will explore the WordNet and Spacy lemmatizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet lemmatizer\n",
    "\n",
    "WordNet is a lexical database of English that is freely and publicly available. \n",
    "\n",
    "As part of WordNet, nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive\n",
    "synonyms (synsets), each expressing distinct concepts. \n",
    "\n",
    "These synsets are interlinked using lexical and conceptual semantic relationships. \n",
    "\n",
    "It can be easily downloaded, and the nltk library offers an interface to it that enables you to perform lemmatization.\n",
    "\n",
    "Let's try and lemmatize the following sentence using the WordNet lemmatizer:\n",
    "\n",
    "`We are putting in efforts to enhance our understanding of Lemmatization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')  # download dependency\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "token_list = s.split()  # we do not need here more sophisticated tokinizer\n",
    "sl = ' '.join([lemmatizer.lemmatize(token) for token in token_list])\n",
    "print(s)\n",
    "print(sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the WordNet lemmatizer did not do much here. \n",
    "\n",
    "What are we lacking here?\n",
    "\n",
    "The WordNet lemmatizer works well if the POS tags are also provided as inputs.\n",
    "\n",
    "It is really impossible to manually annotate each word with its POS tag in a text corpus.\n",
    "\n",
    "Now, how do we solve this problem and provide the part-of-speech tags for individual\n",
    "words as input to the WordNet lemmatizer?\n",
    "\n",
    "Fortunately, the nltk library provides a method for finding POS tags for a list of words\n",
    "using an averaged perceptron tagger (i.e., a neural network).\n",
    "\n",
    "The POS tags for the sentence \"We are trying our best to understand Lemmatization\" provided by \n",
    "the POS tagging method can be found in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "pos_tags = nltk.pos_tag(token_list)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the abbreviations have the following meaning:\n",
    "- PRP: personal pronoun\n",
    "- VBP: verb, present tense\n",
    "- VBG: verb gerund\n",
    "- IN: preposition/subordinating conjunction \n",
    "- NNS: noun plural\n",
    "- TO: infinite marker\n",
    "- VB: verb\n",
    "- PRP$: possessive pronoun\n",
    "- NN: noun, singular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the POS tags need to be converted to a form that can be understood by the\n",
    "WordNet lemmatizer and sent in as input along with the tokens.\n",
    "\n",
    "The code below does what is needed: extracts from the advanced nltk POS \n",
    "tags only the first letter (like VBG -> V) and\n",
    "then maps it a wordnet built-in POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# This is a common method which is widely used across the NLP community of practitioners and readers\n",
    "def get_part_of_speech_tags(token):\n",
    "    \"\"\"\n",
    "    Get POS tag for the token and then extract the first letter of the tag:\n",
    "      nltk.pos_tag(['Lemmatization']) -> [('Lemmatization', 'NN')]\n",
    "      [('Lemmatization', 'NN')][0] -> ('Lemmatization', 'NN')\n",
    "      ('Lemmatization', 'NN')[1] -> 'NN'\n",
    "      'NN'[0] -> 'N'\n",
    "    \"\"\"\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
    "    We are focusing on Verbs, Nouns, Adjectives and Adverbs here.\n",
    "    And if unknown letter appears in tag, wordnet.NOUN is assumed by default\"\"\"\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s see how the WordNet lemmatizer performs when the POS tags are also provided as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in token_list]\n",
    "sl=' '.join(lemmatized_output_with_POS_information)\n",
    "print(s) \n",
    "print(sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following conversions happened:\n",
    "- are -> be\n",
    "- putting -> put\n",
    "- efforts -> effort\n",
    "- understanding -> understand\n",
    "\n",
    "Let’s compare this with the Snowball stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer2 = SnowballStemmer(language='english')\n",
    "stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n",
    "print(' '.join(stemmed_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the WordNet lemmatizer makes a sensible and context-aware conversion of\n",
    "the token into its base form, unlike the stemmer, which tries to chop the affixes from the\n",
    "token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider one more example of lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "s = \"Susan Calvin had been born in the year 1982, they said, which made her seventy-five now.\"\n",
    "token_list = s.split()  # we do not need here more sophisticated tokinizer\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "pos_tags = nltk.pos_tag(token_list)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the abbreviations have the following meaning:\n",
    "- NNP: proper noun, singular\n",
    "- VBD: verb past tense\n",
    "- VBN: verb past participle\n",
    "- IN: preposition/subordinating conjunction \n",
    "- DT: determiner\n",
    "- NN: noun, singular\n",
    "- CD: cardinal digit \n",
    "- PRP: personal pronoun\n",
    "- WDT: wh-determiner\n",
    "- PRP$: possessive pronoun\n",
    "- JJ: adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in token_list]\n",
    "sl= ' '.join(lemmatized_output_with_POS_information)\n",
    "print(s)  # the original sencence\n",
    "print(sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that \"born\" and \"said\" have been left unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy lemmatizer\n",
    "\n",
    "The Spacy lemmatizer comes with pretrained models that can parse text and figure out the various properties of the text, such as POS tags, named-entity tags, and so on, with a simple function call. \n",
    "\n",
    "The prebuilt models identify the POS tags and assign a lemma to each token,\n",
    "unlike the WordNet lemmatizer, where the POS tags need to be explicitly provided.\n",
    "\n",
    "If spacy and the model for English languages are not installed yet it can be done as described in the very top of this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "s = \"Susan Calvin had been born in the year 1982, they said, which made her seventy-five now.\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(s)\n",
    "sl = \" \".join([token.lemma_ for token in doc])\n",
    "print(s)\n",
    "print(sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spacy lemmatizer performed a decent job without the input information of the POS\n",
    "tags. \n",
    "\n",
    "The advantage here is that there is no need to look out for external dependencies for\n",
    "fetching POS tags as the information is built into the pretrained model.\n",
    "\n",
    "Notice that a tokennization has been also done before the lemmatization: object `doc` is iterable and returns token objects. Token objects in turn have an attribute `.lemma_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token} -> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "Until now, we have focused on tokens of size 1, which means only one word. \n",
    "\n",
    "Sentences generally contain names of people and places and other open compound terms, such as\n",
    "_living room_ and _coffee mug_. \n",
    "\n",
    "These phrases convey a specific meaning when two or more words are used together. \n",
    "\n",
    "When used individually, they carry a different meaning altogether and the inherent meaning \n",
    "behind the compound terms is somewhat lost. \n",
    "\n",
    "The usage of multiple tokens to represent such inherent meaning can be highly beneficial for the\n",
    "NLP tasks being performed. \n",
    "\n",
    "Even though such occurrences are rare, they still carry a lot of\n",
    "information. \n",
    "\n",
    "Techniques should be employed to make sense of these as well.\n",
    "\n",
    "In general, these are grouped under the umbrella term of n-grams. \n",
    "\n",
    "When n is equal to 1, these are termed as unigrams. \n",
    "\n",
    "Bigrams, or 2-grams, refer to pairs of words, such as _dinner table_. \n",
    "\n",
    "Phrases such as the _United Arab Emirates_ comprising three words are termed as\n",
    "trigrams or 3-grams. \n",
    "\n",
    "This naming system can be extended to larger n-grams, but most NLP\n",
    "tasks use only trigrams or lower.\n",
    "\n",
    "Let’s understand how this works for the following sentence:\n",
    "\n",
    "`Natural Language Processing is the way to go`\n",
    "\n",
    "The phrase _Natural Language Processing_ carries an inherent meaning that would be\n",
    "lost if each of the words in the phrase is processed individually.\n",
    "\n",
    "However, when we use trigrams, these phrases can be extracted together and the meaning gets captured. \n",
    "\n",
    "In general, all NLP tasks make use of unigrams, bigrams, and trigrams together to capture all\n",
    "the information.\n",
    "\n",
    "The following code illustrates an example of capturing bigrams:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = s.split()\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print([\" \".join(token) for token in bigrams])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and capture trigrams from the same sentence using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = \"Natural Language Processing is the way to go\"\n",
    "tokens = s.split()\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "print([\" \".join(token) for token in trigrams])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually stemming or lemmatization is done before composing n-gramms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming text into data structures\n",
    "\n",
    "Once tokenization and word nomalization is done we can convert a collection of prepared tokes into a vocabulary.\n",
    "\n",
    "This is a lits of unique tokens that can be found in the text.\n",
    "\n",
    "The next step is to transform it into a data structure.\n",
    "\n",
    "There are multiple approaches for it. Some are very simple and others are very sophisticated even involving intermediate data processing with neural networks.\n",
    "\n",
    "The final goal is to have vectors representing text in the most appropriate way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "A very intuitive approach to representing a document is to use the frequency of the words\n",
    "in that particular document. \n",
    "\n",
    "This is exactly what is done as part of the bag of words (BoW) approach.\n",
    "\n",
    "The vocabulary-building step comes as a prerequisite to the BoW methodology. \n",
    "\n",
    "Once the vocabulary is available, each sentence can be represented as a vector. \n",
    "\n",
    "The length of this vector would be equal to the size of the vocabulary. \n",
    "\n",
    "Each entry in the vector would correspond to a term in the vocabulary, and the\n",
    "number in that particular entry would be the frequency of the term in the sentence under\n",
    "consideration. \n",
    "\n",
    "The lower limit for this number would be 0, indicating that the vocabulary\n",
    "term does not occur in the sentence concerned.\n",
    "\n",
    "The upper limit could possibly be the frequency of the occurrence of the word in the text corpora.\n",
    "\n",
    "This would indicate that the most frequently occurring word occurs in only one sentence.\n",
    "\n",
    "However, this is an extremely rare situation.\n",
    "\n",
    "Let us see the simple example how BoW can looks like. We use `sklearn` module for it. The detailed explanation of its using will be done a bit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X = [\"Computers can analyze text and understand text\", \n",
    "     \"They do it using vectors and matrices\", \n",
    "     \"Computers can process massive amounts of text data\"]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "# All tokens are stored as a dictionary {'token': position}. We print it sorted by positions\n",
    "print(sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1])   )\n",
    "# Here each vector represents one sentence\n",
    "print(X_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first vector corresponds to the sentence \"Computers can analyze text and understand text\". \n",
    "\n",
    "Numbers 1 appear on positions 1, 2 and 8. They correspond to tokens \"analyze\", \"computers\" and \"understand\". Token \"text\" appears in the text twice so that we 2 on position 7.\n",
    "\n",
    "The words \"can\", \"and\" have been omitted as a stopword."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot representation of tokens\n",
    "\n",
    "Building of BoW vectors can be considered as sums of one-hot vectors corresponding to tokens.\n",
    "\n",
    "In course of text preprocessing a list of unique tokens is created. This is called a vocabulary. \n",
    "\n",
    "In `CountVectorizer` the vocabulary is represented as a Python dictionary with tokes as keys and their positions in the vocabulary as values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of tokens can be treated as a set of allowed values of a categorical variable. \n",
    "\n",
    "Let us remember: in machine learning categorical variables are preferably represented in one-hot form.\n",
    "\n",
    "Number of all values is a length of a one-hot vector. This vector always has zeros everywhere except a single position with one. \n",
    "\n",
    "It corresponds to a particular value of the categorical variable. \n",
    "\n",
    "Let us create one-hot vectors for the tokens from the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "size = len(vocab)\n",
    "\n",
    "one_hot = {}\n",
    "\n",
    "for token in vocab:\n",
    "    position = vocab[token]\n",
    "    vector = np.zeros(size, dtype=int)\n",
    "    vector[position] = 1\n",
    "    one_hot[token] = vector\n",
    "    print(f\"{token:15} {position:2}, {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we have a dictionary that maps tokens to their one-hot vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to create a BoW vector for a sentence we have to add one-hot vectors of its tokens taking into account their numbers of repetitions.\n",
    "\n",
    "For the sentence \"computers can analyze text and understand text\" we have:\n",
    "\n",
    "$$\n",
    "\\text{computers} + \\text{analyze} + 2\\times \\text{text} + \\text{understand}\n",
    "$$\n",
    "\n",
    "Token \"text\" appears twice so we multiply its vector by 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = one_hot[\"computers\"] + one_hot[\"analyze\"] + 2 * one_hot[\"text\"] + one_hot[\"understand\"]\n",
    "print(bow_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW from scratch\n",
    "\n",
    "Let us now consider a full process of BoW building. We start from a raw text, perform its tokenization and cleaning. Then we remove stopwords and find stems. Then we created the BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "\n",
    "def tokenize_and_clean(sentence):\n",
    "    \"\"\"Tokenize sentence and clean it.\n",
    "    \"\"\"\n",
    "    raw_tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = []\n",
    "    for tok in raw_tokens:\n",
    "        t1 = tok[1:] if tok[0] == \"'\" else tok  # we do not want to remove tokens like \"'ve\" \n",
    "        if t1.isalpha():\n",
    "            tokens.append(tok.lower())\n",
    "    return tokens\n",
    "\n",
    "def stopwords_removal(tokens):\n",
    "    wh_words = set(['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom'])\n",
    "    stop = set(stopwords.words('english')) - wh_words\n",
    "    return [tok for tok in tokens if tok not in stop]\n",
    "\n",
    "def stemming(tokens):\n",
    "    stemmer = SnowballStemmer(language = 'english')\n",
    "    return [stemmer.stem(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the text to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "We are reading about Natural Language Processing Here. What an interesting topic of data science! \n",
    "Natural Language Processing makes computers to comprehend language data. The field \n",
    "of Natural Language Processing is evolving everyday.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokens_list = [tokenize_and_clean(sentence) for sentence in sentences]\n",
    "print(raw_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = [stopwords_removal(tokes) for tokes in raw_tokens_list]\n",
    "print(tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_tokens_list = [stemming(tokens) for tokens in tokens_list]\n",
    "print(stem_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have fully prepared tokens and can create the vocabulary. We sort it to compare later with the vocabulary created by `CountVectorizer` from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_words = set()\n",
    "for tokens in stem_tokens_list:\n",
    "    for tok in tokens:\n",
    "        set_of_words.add(tok)\n",
    "vocab = sorted(list(set_of_words))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching the position of each word in the vocabulary. We use `OrderedDictt` to preserve ordering of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "position = OrderedDict()\n",
    "for i, token in enumerate(vocab):\n",
    "    position[token] = i\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally create the BoW matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix = np.zeros((len(stem_tokens_list), len(vocab)), dtype=int)\n",
    "\n",
    "for i, tokens in enumerate(stem_tokens_list):\n",
    "    for token in tokens:\n",
    "        bow_matrix[i][position[token]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our text (we clean it a little to make it look better). \n",
    "\n",
    "It has 4 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence.replace('\\n', ' ').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is its BaW representation. Each sentence is mapped to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of vectors equals to the length of the vocabulary. \n",
    "\n",
    "Each vector element correspond to a single token (word) in the text. A number shows how many times this token appears in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `CountVectorizer` to build BoW\n",
    "\n",
    "`CountVectorizer` is a tool provided by the `sklearn` or scikit-learn library in Python\n",
    "that saves all the effort performed above and provides application\n",
    "programming interfaces (APIs) that would conveniently help in building a BoW model.\n",
    "\n",
    "It converts a list of text documents into a matrix such that each entry in the matrix would\n",
    "correspond to the count of a particular token in the respective sentences. \n",
    "\n",
    "Let us look at how to instantiate `CountVectorizer` and fit data to it.\n",
    "\n",
    "First we try the simplest way: we feed the `CountVectorizer` with already stemmed tokens in `stem_tokens_list`.\n",
    "\n",
    "For this purpose this list of lists must be converted into sentences again, i.e., we need join them by spaces. \n",
    "\n",
    "This is because `CountVectorizer` accepts raw sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_sentences = [' '.join(tokens) for tokens in stem_tokens_list]\n",
    "print(stem_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the vectorization is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix2 = vectorizer.fit_transform(stem_sentences)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow_matrix2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this matrix with the one obtained manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow_matrix2.toarray() - bow_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will wrap the whole pipeline of data preparation into `CountVectorizer`.\n",
    "\n",
    "We are going to feed it with the list of raw sentences and obtain BoW as a result.\n",
    "\n",
    "Let us remember that the sentences read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below accepts this list of sentences and returns the BoW.\n",
    "\n",
    "Observe that `CountVectorizer` has built-in method tokenizer that is responsible for tokenization. \n",
    "\n",
    "We fetch it with the method `.build_tokenizer()` and extend its functionality: stemming and stopwords removal is added.\n",
    "\n",
    "Of course we could also use here another tokenizer, for example from NLTK.\n",
    "\n",
    "We do not use built-in stopwords removal since it is applied after our custom tokenizer and stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language = 'english')\n",
    "tokenizer = CountVectorizer().build_tokenizer()\n",
    "\n",
    "wh_words = set(['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom'])\n",
    "stop = set(stopwords.words('english')) - wh_words\n",
    "\n",
    "def stemmed_tokenizer(doc):\n",
    "    tokens = [tok for tok in tokenizer(doc) if tok not in stop]\n",
    "    stem_tokens = [stemmer.stem(tok) for tok in tokens]\n",
    "    return stem_tokens\n",
    "\n",
    "stem_vectorizer = CountVectorizer(tokenizer=stemmed_tokenizer)\n",
    "bow_matrix3 = stem_vectorizer.fit_transform(sentences);\n",
    "\n",
    "print(stem_vectorizer.get_feature_names_out())\n",
    "print(bow_matrix3.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow_matrix3.toarray() - bow_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting N-gramms with `CountVectorizer`\n",
    "\n",
    "Simple BoW totally ignores the order of words, but we know that this is important.\n",
    "\n",
    "N-grams allows to take into account local word ordering. \n",
    "\n",
    "`CountVectorizer` has an option `ngram_range` for it. \n",
    "\n",
    "The tuple must be specified for this option that says which n-grams to create. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 3), stop_words='english')\n",
    "bow_matrix_ngram = vectorizer_ngram.fit_transform(sentences);\n",
    "\n",
    "print(vectorizer_ngram.get_feature_names_out())\n",
    "print(bow_matrix_ngram.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting vocabulary size\n",
    "\n",
    "The standard problem for BoW model is too large vocabulary. For a large text the vocabulary can be really huge. \n",
    "\n",
    "Size of the vocabulary equals to the size of vectors that are used to represent each sentence. \n",
    "\n",
    "And these vectors are usually very sparse: only a few entries are not zero.\n",
    "\n",
    "Subsequent processing of such model will be inefficient and memory consuming. \n",
    "\n",
    "The `CountVectorizer` provides a parameter called `max_features` that will build a vocabulary such \n",
    "that the size of the vocabulary would be less than or equal to `max_features` ordered by the frequency \n",
    "of tokens occurring in a corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_max_features = CountVectorizer(ngram_range=(1, 3), stop_words='english', max_features=6)\n",
    "bow_matrix_max_features = vectorizer_max_features.fit_transform(sentences);\n",
    "\n",
    "print(vectorizer_max_features.get_feature_names_out())\n",
    "print(bow_matrix_max_features.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates that only six of the most frequently occurring n-grams among\n",
    "unigrams, bigrams, or trigrams in the corpus were selected since the value of the\n",
    "`max_features` attribute was set to 6.\n",
    "\n",
    "Observe that 6 is inappropriate for our text since the second vector does not have features at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min_df and Max_df thresholds\n",
    "\n",
    "Now that we are clear on how `max_features` helps by limiting the vocabulary size, we also\n",
    "need to understand that at the top of this limited vocabulary would be terms or phrases\n",
    "that have occurred very frequently in the text corpus under consideration. \n",
    "\n",
    "These phrases might occur very frequently in an individual document or may be present in almost all\n",
    "documents in the corpus, and may not carry any pattern. \n",
    "\n",
    "One approach we have discussed so far to remove such terms is the removal of stopwords.\n",
    "(Too frequent terms are removed because being everywhere in the text they do not carry much \n",
    "of information. Not so frequent terms are expected to be more informative.)\n",
    "\n",
    "Another convenient technique that comes along with `CountVectorizer` is `max_df`, which\n",
    "will ignore terms having a document frequency higher than a provided threshold\n",
    "mentioned as part of the `max_df` parameter. \n",
    "\n",
    "Similarly, we can remove rarely occurring terms that occur fewer times in a document than a given \n",
    "threshold, using a `min_df` parameter. \n",
    "\n",
    "This can potentially have issues as these rarely occurring terms might be very\n",
    "significant for certain documents in the text corpus. \n",
    "\n",
    "We will look into how to capture such information in the TF-IDF vectors section below.\n",
    "\n",
    "The following example illustrates how `max_df` and `min_df` can be put into action and\n",
    "consequently provide minimum and maximum thresholds toward the occurrence of a\n",
    "phrase in a corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_minmax_df = CountVectorizer(ngram_range=(1, 3), stop_words='english', max_df = 3, min_df = 2)\n",
    "bow_matrix_minmax_df = vectorizer_minmax_df.fit_transform(sentences);\n",
    "\n",
    "print(vectorizer_minmax_df.get_feature_names_out())\n",
    "print(bow_matrix_minmax_df.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of the BoW representation\n",
    "\n",
    "The BoW model provides a mechanism for representing text data using numbers. \n",
    "\n",
    "However, there are certain limitations to it. \n",
    "\n",
    "The model only relies on the count of terms in a document. \n",
    "\n",
    "This might work well for certain tasks or use cases with a limited vocabulary,\n",
    "but it would not scale to large vocabularies efficiently.\n",
    "\n",
    "The BoW model also intrinsically provides possibilities for eliminating or reducing the\n",
    "significance of tokens or phrases that occur very rarely. \n",
    "\n",
    "These phrases might be present in a\n",
    "very small number of documents, but they can be very important in the representation of\n",
    "those documents. \n",
    "\n",
    "The BoW model does not support such possibilities.\n",
    "\n",
    "The BoW model can also get extremely huge in terms of the vocabulary for a large text\n",
    "corpus. \n",
    "\n",
    "This can lead to vectors of huge sizes representing every document, which might\n",
    "cause a degradation in the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectors\n",
    "\n",
    "For BoW models the frequency of words across a document was the only criterion for building vectors for documents. \n",
    "\n",
    "The words that occur rarely are either removed or their weights are too low compared to words that occur\n",
    "very frequently. \n",
    "\n",
    "While following this kind of approach, the pattern of information carried\n",
    "across terms that are rarely present but carry a high amount of information for a document\n",
    "or an evident pattern across similar documents is lost. \n",
    "\n",
    "The TF-IDF approach for weighing terms in a text corpus helps mitigate this issue.\n",
    "\n",
    "The TF-IDF approach is by far the most commonly used approach for weighing terms. \n",
    "\n",
    "It is found in many applications. \n",
    "\n",
    "The definition below tells about a text corpus that consists of documents.\n",
    "\n",
    "The text corpus corresponds to the variables `text` or `sentences` leveraged above and\n",
    "one document is one our `sentence`.\n",
    "\n",
    "TF-IDF is a composite of two terms, which are described as follows:\n",
    "\n",
    "- TF (term frequency) is similar to the `CountVectorizer` tool. \n",
    "It takes into account how frequently\n",
    "a term occurs in a document. Since most of the documents in a text corpus are of\n",
    "different lengths, it is very likely that a term would appear more frequently in\n",
    "longer documents rather than in smaller ones. This calls for normalizing the\n",
    "frequency of the term by dividing it with the count of the terms in the document.\n",
    "There are multiple variations to calculate TF, but the following is the most\n",
    "common representation:\n",
    "\n",
    "$$\n",
    "\\mathrm{TF}(w, d)=\\frac{\\text{Number of times the word $w$ occurs in a document $d$}}\n",
    "{\\text{Total number of words in the document $d$}}\n",
    "$$\n",
    "\n",
    "- IDF (inverse document frequency) takes into account those terms that occur \n",
    "not so frequently across documents but might be more meaningful in \n",
    "representing the document. It measures the importance of a term in a document. \n",
    "The usage of TF only would provide more weightage to terms that \n",
    "occur very frequently. As part of IDF, just the opposite is\n",
    "done, whereby the weights of frequently occurring terms are suppressed and the\n",
    "weights of possibly more meaningful but less frequently occurring terms are\n",
    "scaled up. Similar to TF, there are multiple ways to measure IDF, but the\n",
    "following is the most common representation:\n",
    "\n",
    "$$\n",
    "\\mathrm{IDF}(w)=\\log\\frac{\\text{Total number of documents}}\n",
    "{\\text{Number of documents containing word $w$}}\n",
    "$$\n",
    "\n",
    "Final weight of word $w$ in document $d$ is given by the following TF-\n",
    "IDF weighting:\n",
    "\n",
    "$$\n",
    "\\mathrm{Weight}(w,d)=\\mathrm{TF}(w, d)\\times \\mathrm{IDF}(w)\n",
    "$$\n",
    "\n",
    "As can be seen, the weight of word $w$ in document $d$ is a product of the TF of word $w$ in\n",
    "document $d$ and the IDF of word $w$ across the text corpus.\n",
    "\n",
    "Let us understand how all this pans out in action. \n",
    "\n",
    "We will take the same corpus as the one taken for the `CountVectorizer` model for this example \n",
    "to see the differences. \n",
    "\n",
    "Also, the data underwent the same preprocessing pipeline here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "We are reading about Natural Language Processing Here. What an interesting topic of data science! \n",
    "Natural Language Processing makes computers to comprehend language data. The field \n",
    "of Natural Language Processing is evolving everyday.\"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language = 'english')\n",
    "tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "\n",
    "wh_words = set(['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom'])\n",
    "stop = set(stopwords.words('english')) - wh_words\n",
    "\n",
    "def stemmed_tokenizer(doc):\n",
    "    tokens = [tok for tok in tokenizer(doc) if tok not in stop]\n",
    "    stem_tokens = [stemmer.stem(tok) for tok in tokens]\n",
    "    return stem_tokens\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=stemmed_tokenizer)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the preprocessed corpus after TF-IDF vectorization are shown below. \n",
    "\n",
    "The vocabulary is the same as `CountVectorizer`; however, the\n",
    "weights are completely different for the various terms across the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_vectorizer.get_feature_names_out(), \"\\n\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally each vector, i.e., each row of this matrix has been normalized: after computation each vector element \n",
    "has been divided divided by the vector Euclidean norm (also this is called L2 norm). \n",
    "\n",
    "The resulting vectors have unit Euclidean lengths.\n",
    "\n",
    "This is needed for the following computation of their distances.\n",
    "\n",
    "This normalization can be switched off by setting `norm=None` parameter of `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams and maximum features in the TF-IDF vectorizer\n",
    "\n",
    "Similar to `CountVectorizer`, the TF-IDF vectorizer offers the capability of using n-grams\n",
    "and max_features to limit our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), stop_words='english', max_features=6)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences);\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we took the top six features among unigrams, bigrams, and trigrams, and used them\n",
    "to represent the TF-IDF vectors. \n",
    "\n",
    "The TF-IDF vectorizer provides the `min_df` and `max_df` parameters as well, and the usage \n",
    "is exactly the same as `CountVectorizer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of the TF-IDF vectorizer's representation\n",
    "\n",
    "The TF-IDF vectorizer offers an improvement over `CountVectorizer` by scaling the\n",
    "weights of the less frequently occurring terms as well as by using the IDF component. \n",
    "\n",
    "It is also computationally fast. \n",
    "\n",
    "However, it still relies on lexical analysis and does not take into\n",
    "account things such as semantics, the context associated with\n",
    "terms, and the position of a term in a document. \n",
    "\n",
    "It is dependent on the vocabulary size, like `CountVectorizer`, and will get really slow with large vocabulary sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "TF-IDF vectors can be compared using so called cosine similarity.\n",
    "\n",
    "Let us first remember:\n",
    "\n",
    "Dot product is an operation when we sum a componentwise products of two vectors $x$ and $y$:\n",
    "\n",
    "$$\n",
    "(x, y) = \\sum_{i=0}^{N-1} x_i y_i\n",
    "$$\n",
    "\n",
    "Using the dot product we can compute angels between vectors due to the following property:\n",
    "\n",
    "$$\n",
    "(x, y) = |x|_2 |y|_2 \\cos(\\alpha)\n",
    "$$\n",
    "\n",
    "where $|x|_2$ and $|y|_2$ are Euclidean norms and $\\alpha$ is the angle between $x$ and $y$.\n",
    "\n",
    "Since $\\cos 90^\\circ=\\cos \\pi/2=0$ the dot product of two orthogonal vectors is always zero.\n",
    "\n",
    "Cosine between two vectors can be used as a measure of their similarity. This is called a cosine similarity. \n",
    "\n",
    "The cosine similarity is the highest and equals 1 when the angle between two vectors is zero. For nonzero angles the similarity is less then 1.\n",
    "\n",
    "Since by default `TfidfVectorizer` produces TF-IDF vectors already normalized, i.e., $|x|=|y|=1$ the dot product of two such vectors \n",
    "is automatically their cosine similarity.\n",
    "\n",
    "Cosine similarity is used to reveal sentences with similar meanings.\n",
    "\n",
    "Let us check how it works.\n",
    "\n",
    "Consider the following sentences. Number 1 and 2 as well as number 3 and 4 are similar and these \n",
    "two pairs don't resemble each other.\n",
    "\n",
    "The sentence number 5 has a little similarity with all others: it tells about rain and snowing and about Moon and Earth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences2 = [\n",
    "    \"He likes snowing\", \n",
    "    \"Besides of snowing he likes rain\", \n",
    "    \"Earth has a satellite\", \n",
    "    \"Moon rotates around Earth as its satellite\",\n",
    "    \"Unlike Earth on the Moon there is no rain or snowing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a matrix whose rows are TF-IDF vectors and compute all pairwise cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(mat):\n",
    "    res = []\n",
    "    N = len(mat)\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            res.append((i+1, j+1, np.dot(mat[i], mat[j])))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute TF-IDF matrix for the above sentences and check their similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "mat = TfidfVectorizer().fit_transform(sentences2).toarray()\n",
    "cosine_sim(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the numbers we see indeed that pairs 1-2 and 3-4 are similar, there is not similarity between these two pairs and number 5 resembles all others a little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding\n",
    "\n",
    "BoW and TF-IDF vector models of text have two common disadvantages:\n",
    "- Sparsity of vectors. For a large text a vocabulary is large and the length of each vector representing a sentence equals to the size of the vocabulary. But most of sites in this vector are zeros because each sentence contains only a few words from the whole vocabulary.\n",
    "- Ignoring word context. Information about the neighborhood of the word is not taken into account. The neighborhood of\n",
    "a word carries important information in terms of what context the word is carrying in a sentence. \n",
    "\n",
    "The approach free from these disadvantages is called word embedding. \n",
    "\n",
    "Within this approach words in sentences are represented as sufficiently low (not so high) dimensional vector in such a way that words with similar meaning are represented with close vectors. \n",
    "\n",
    "This is done due to taking into account words co-occurrence in the sentences of a text corpus.\n",
    "\n",
    "Word embedding is computed using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec model\n",
    "\n",
    "Word2vec model is one of the widely used models of word embedding.\n",
    "\n",
    "Let us see how it works. We will use `gensim` library for it.\n",
    "\n",
    "As the first example we will download a text corpus `text8` that goes as a part of `gensim` library.\n",
    "\n",
    "This is nothing but the \"First 100,000,000 bytes of plain text from Wikipedia\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "api.info('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = api.load('text8')\n",
    "print(type(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what we have. Variable `corpus` is an instance of a gensim class `Dataset`. \n",
    "\n",
    "It can be converted to a plain list like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [d for d in corpus]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a list or records. Each record is just a part of Wikipedia article already tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][:25])\n",
    "print(data[1][:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of unique tokens is about 254 thousands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set()\n",
    "for record in data:\n",
    "    tokens.update(set(record))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split of the dataset into two parts to show that the Word2vec model can be updated we newly arrived data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part1 = data[:1000]\n",
    "data_part2 = data[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create Word2vec model for this corpus. \n",
    "\n",
    "This is done using class `Word2Vec`. It can be fed by an instance of `Dataset`, that is by our `corpus` itself or by a list of list of tokens.\n",
    "\n",
    "Parameter `worker` specifies how many CPU cores will be used for training. To employ all available CPU cores we use function `cpu_count()` from `multiprocessing` library.\n",
    "\n",
    "Parameter `vector_size` specifies the size of the resulting vectors representing words. Typical values between 100 and 300, the default value is 100. \n",
    "\n",
    "Notice that this vectors size 100 is actually small. Above we have found that the number of unique tokens in this data set is about 12 thousands. This the vocabulary size. If we were building BoW or TF-IDF models we would use vectors of this size.\n",
    "\n",
    "Parameter `min_count` restricts the vocabulary so that word vectors are only built for words that occur at least min_count times in the corpus. Default value of `min_count` is 5. So that if training a model for a small corpus it is reasonable to override the default value to set `min_value=1`.\n",
    "\n",
    "Parameter `window` control how many neighboring words take into account. `window=5` means that we take 2 neighbors at each sides and the word itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "model = Word2Vec(data_part1, workers=cpu_count(), vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is ready. That is how we can get a vector representing a token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv['science'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only vectors for words from a vocabulary can be obtained. The following code raises an exception due to a rare word that is not found in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(model.wv['agastopia'])\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assume that some new data have appeared. We can use it to improve the model and extend its dictionary.\n",
    "\n",
    "Given a trained model, one needs to call the `.build_vocab()` method on the new dataset and then call the `.train()` method. \n",
    "\n",
    "Method `.build_vocab()` is called first because the model has to be apprised of what new words to expect in the incoming corpus.\n",
    "\n",
    "Method `.train()` updates the model using new corpus. \n",
    "\n",
    "Parameters `total_examples` specify number of new data samples. Its appropriate value is stored in `model.corpus_count` after calling `.build_vocab()`. \n",
    "\n",
    "Parameter `epochs` tells how many epochs to train. We will train it the same number of epochs as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(data_part2, update=True)\n",
    "model.train(data_part2, total_examples=model.corpus_count, epochs=model.epochs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us have a look  at a few examples to understand what relationships and analogies can be captured by a Word2vec model. \n",
    "\n",
    "A very frequently used example deals with the embedding\n",
    "of King, Man, Queen, and Woman. \n",
    "\n",
    "Once a Word2vec model is built properly and the embedding from it is obtained for these words, the following relationship is frequently obtained, provided that these words are actually a part of the vocabulary:\n",
    "\n",
    "$$\n",
    "\\mathrm{vector}(\\text{Man}) - \\mathrm{vector}(\\text{King}) + \\mathrm{vector}(\\text{Queen}) = \\mathrm{vector}(\\text{Woman})\n",
    "$$\n",
    "\n",
    "This equation boils down to the following relationship:\n",
    "\n",
    "$$\n",
    "\\mathrm{vector}(\\text{Man}) + \\mathrm{vector}(\\text{Queen}) = \n",
    "\\mathrm{vector}(\\text{Woman}) + \\mathrm{vector}(\\text{King})\n",
    "$$\n",
    "\n",
    "The thought process here is that the relationship of Man:King is the same as Woman:Queen.\n",
    "\n",
    "The Word2vec algorithm is able to capture these semantic relationships when it devises an embedding for each of these words.\n",
    "\n",
    "Let us check that it really works. We use `.most_simular()` method for it.\n",
    " \n",
    "Its parameter `positive` specifies the words that have to be most similar to the hunted word and `neagative` specifies the least similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"man\", \"queen\"], negative=[\"king\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more example: find something like \"vatican\" in \"italy\" but for \"england\". \n",
    "\n",
    "The model find \"westminster\" probably keeping in mind Westminster Abbey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"england\", \"vatican\"], negative=[\"italy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word mover’s distance\n",
    "\n",
    "In the previous section we discussed that measuring word similarity is one of the major use cases of Word2vec. \n",
    "\n",
    "Think of a problem statement, such as one where we are\n",
    "building an engine that can rank resumes based on their relevance to a job description.\n",
    "\n",
    "Here, we ideally need to figure out the distance between the job description and the set of resumes. \n",
    "\n",
    "The smaller the distance between the resume and the job description, the higher the relevance of the resume to the job description.\n",
    "\n",
    "One measure we discussed above was to use cosine similarity to find how close or far text documents are to one another or how far\n",
    "removed they are from one another. \n",
    "\n",
    "Now we will discuss another measure, Word Mover's Distance (WMD), which is more relevant than cosine similarity, especially when\n",
    "we base the distance measure for documents on word embeddings.\n",
    "\n",
    "According to the WDM idea the dissimilarity between two text\n",
    "documents can be measured as the minimum amount of distance that the embedded words of \n",
    "one document need to travel to reach the embedded words of another document.\n",
    "\n",
    "Let us look at an standard example that is most often used for illustration of the idea. \n",
    "\n",
    "Sentence 1: \"Obama speaks to the media in Illinois.\"\n",
    "\n",
    "Sentence 2: \"President greets the press in Chicago.\"\n",
    "\n",
    "Based on the Word2vec model, the embedding for Obama would be very close to\n",
    "President.  Similarly, speaks would be pretty close to greets, media would be pretty\n",
    "close to press, and Illinois would map pretty closely to Chicago.\n",
    "\n",
    "Let us take a look at a third sentence:\n",
    "\n",
    "\"Apple is my favorite company.\"\n",
    "\n",
    "Now, this is likely to be more distant to sentence 1 than sentence 2 is. \n",
    "\n",
    "This is because there is not much of a semantic relationship between the words in the first and third sentences.\n",
    "\n",
    "WMD computes the pairwise Euclidean distance between words across the sentences and it\n",
    "defines the distance between two documents as the minimum cumulative cost in terms of\n",
    "the Euclidean distance required to move all the words from the first sentence to the second\n",
    "sentence.\n",
    "\n",
    "Let's see how we implement this using gensim.\n",
    "\n",
    "We have four sentences. The first two are similar and the last two are also similar. And these two pairs are very different form each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"Obama speaks to the media in Illinois\"\n",
    "sentence_2 = \"President greets the press in Chicago\"\n",
    "sentence_3 = \"Apple is my favorite company\"\n",
    "sentence_4 = \"I like smartphones and laptops produced by Apple\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to remove stopwords from the compared sentences. Otherwise they will strongly influence the result: all sentences will be similar due to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop]\n",
    "\n",
    "sentence_1s = preprocess(sentence_1)\n",
    "sentence_2s = preprocess(sentence_2)\n",
    "sentence_3s = preprocess(sentence_3)\n",
    "sentence_4s = preprocess(sentence_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmd_12 = model.wv.wmdistance(sentence_1s, sentence_2s)\n",
    "print(wmd_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmd_34 = model.wv.wmdistance(sentence_3s, sentence_4s)\n",
    "print(wmd_34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two pairs are indeed similar  to each other.\n",
    "\n",
    "Let us compare pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmd_13 = model.wv.wmdistance(sentence_1s, sentence_3s)\n",
    "wmd_14 = model.wv.wmdistance(sentence_1s, sentence_4s)\n",
    "print(wmd_13, wmd_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmd_23 = model.wv.wmdistance(sentence_2s, sentence_3s)\n",
    "wmd_24 = model.wv.wmdistance(sentence_2s, sentence_4s)\n",
    "print(wmd_23, wmd_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that distance between pairs are higher then within the pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word2vec models\n",
    "\n",
    "Word2vec model can be easily built for a plain text like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"He was professor at the Johannaeum, and was delivering a series of \n",
    "lectures on mineralogy, in the course of every one of which he broke \n",
    "into a passion once or twice at least. Not at all that he was over-anxious \n",
    "about the improvement of his class, or about the degree of attention \n",
    "with which they listened to him, or the success which might eventually \n",
    "crown his labours. Such little matters of detail never troubled him much. \n",
    "His teaching was as the German philosophy calls it, 'subjective'; \n",
    "it was to benefit himself, not others. He was a learned egotist. He was \n",
    "a well of science, and the pulleys worked uneasily when you wanted to draw \n",
    "anything out of it. In a word, he was a learned miser.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the text by sentences using `nltk` and then tokennize each sentence with the help of\n",
    "`gensim` utility `simple_preprocess`. \n",
    "\n",
    "This utility converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\n",
    "\n",
    "Of course `nltk` tokenizer could also be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "corpus = []\n",
    "for sentence in nltk.sent_tokenize(text):\n",
    "    corpus.append(gensim.utils.simple_preprocess(sentence))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the corpus is ready and we can train the model. Observe that since our corpus is small we use `min_count=1`. Otherwise most of tokens will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "model = Word2Vec(corpus, workers=cpu_count(), min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a model from text the access to all Word2vec features occurs via attribute `.wv`.\n",
    "\n",
    "For example this is list of all trained vectors (each one corresponds to a single token):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vectors)\n",
    "print(model.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the modes has kept 78 tokes and created 100-dimensional vector for each.\n",
    "\n",
    "This is an example of a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv['mineralogy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However a model trained for such small corpus will probably be useless. \n",
    "\n",
    "This are the most similar words for \"mineralogy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('mineralogy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really huge corpus is required to have a good model.\n",
    "\n",
    "Usually we do not need to training our own Word2vec model. Better solution is to use a pretrained model that is trained for a problem-specific corpora or at least for a common texts.\n",
    "\n",
    "`gensim` provides a list of corpora and pretrained models.\n",
    "\n",
    "Here are their lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "info = api.info()\n",
    "\n",
    "print(info['corpora'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info['models'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have considered how to use downloaded corpus to create a model. Now let us see how to download a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['models']['glove-wiki-gigaword-50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a model. \n",
    "\n",
    "It can not be trained further and thus we have access to its Word2vec facilities without using the attribute `.wv`.\n",
    "\n",
    "This model also knows a word \"mineralogy\" and it knows better then our simple model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('mineralogy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard large Word2vec model suitable for many purposes is \"word2vec-google-news-300\". \n",
    "\n",
    "This model is trained on part of the Google News dataset, covering approximately 3 million words and phrases. \n",
    "\n",
    "Such a model can take hours to train, but since it is already available, downloading and loading it with `gensim` takes minutes.\n",
    "\n",
    "The model is approximately 2GB, so it requires a decent network connection to be downloaded. Once downloaded it is cashed locally and no more downloadings are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Word2vec\n",
    "\n",
    "\n",
    "Word2vec is a great tool for capturing semantic information from text, and we have seen how well it captures information. \n",
    "\n",
    "However, the Word2vec model has some limitations.\n",
    "\n",
    "Let's take the following two sentences:\n",
    "\n",
    "\"I am eating an apple.\"\n",
    "\n",
    "\"I am using an apple desktop.\"\n",
    "\n",
    "\"apple\" in the first sentence signifies the fruit and, in the second sentence, it signifies the company. \n",
    "\n",
    "However, the word vector generated for apple would be the same for both the\n",
    "company and the fruit. \n",
    "\n",
    "In other words, since a static embedding is created for each word\n",
    "after the training, generating an embedding on the fly based on the context for a word's specific usage is a limitation of the Word2vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other embedding models\n",
    "\n",
    "Previously we looked at how information related to the ordering of words, along with their semantics, can be taken into account when building embeddings to represent words. \n",
    "\n",
    "The idea of building embeddings can be extended. \n",
    "\n",
    "In the `Word2Vec` approach each word in the vocabulary had a vector representation. \n",
    "\n",
    "`Word2Vec` relies heavily on the vocabulary it has been trained to represent. It unable to handle properly words not found in the vocabulary. \n",
    "\n",
    "Ideas of `Word2Vec` have been extended in a model `fastText`. \n",
    "\n",
    "Each word is encapsulated a combination of character n-grams. \n",
    "\n",
    "Each of these n-grams has a vector representation. \n",
    "\n",
    "Word representations are actually a result of the summation of their character n-grams.\n",
    "\n",
    "When certain words are missing from the training vocabulary we can still have a representation for them if their n-grams are present as part of other words.\n",
    "\n",
    "Often instead of vectors for individual words we need vectors for whole sentences. \n",
    "\n",
    "The trivial solution is to average all word vectors including in a sentence.\n",
    "\n",
    "But there are techniques that are able to build straightforward embeddings for documents and sentences.\n",
    "\n",
    "An algorithm called `Doc2Vec` provides sentence- or document-level contextual embeddings. \n",
    "\n",
    "Another technique `Sent2Vec` is focused on obtaining embeddings for sentences based on word n-grams. \n",
    "\n",
    "Research has shown that `Sent2Vec` outperforms `Doc2Vec` in the majority of\n",
    "the tasks it undertakes and that it is a better representation method for sentences or\n",
    "documents. \n",
    "\n",
    "One more approach is called the Universal Sentence Encoder (`USE`). This is a model for fetching embeddings at the sentence level. \n",
    "\n",
    "Several models that have been built using USE model have outperformed state-of-the-art results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "ru",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "113px",
    "width": "228px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
