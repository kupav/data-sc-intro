{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 Vector models of text, Exercises\n",
    "\n",
    "Part of [\"Introduction to Data Science\" course](https://github.com/kupav/data-sc-intro) by Pavel Kuptsov, [kupav@mail.ru](mailto:kupav@mail.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install spacy and its module for English uncomment and execute the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy && python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Describe in writing why stopword removal is required when a vectorized model of a text is prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Describe in writing what are stemming and lemmatization. For what purpose they are leveraged?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Describe in writing what are n-grams and why their using may improve a text model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "We are reading about Natural Language Processing Here. What an interesting topic of data science! \n",
    "Natural Language Processing makes computers to comprehend language data. The field \n",
    "of Natural Language Processing is evolving everyday.\"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language = 'english')\n",
    "tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "\n",
    "wh_words = set(['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom'])\n",
    "stop = set(stopwords.words('english')) - wh_words\n",
    "\n",
    "def stemmed_tokenizer(doc):\n",
    "    tokens = [tok for tok in tokenizer(doc) if tok not in stop]\n",
    "    stem_tokens = [stemmer.stem(tok) for tok in tokens]\n",
    "    return stem_tokens\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=stemmed_tokenizer)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the preprocessed corpus after TF-IDF vectorization are shown below. \n",
    "\n",
    "The vocabulary is the same as `CountVectorizer`; however, the\n",
    "weights are completely different for the various terms across the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_vectorizer.get_feature_names_out(), \"\\n\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally each vector, i.e., each row of this matrix has been normalized: after computation each vector element \n",
    "has been divided divided by the vector Euclidean norm (also this is called L2 norm). \n",
    "\n",
    "The resulting vectors have unit Euclidean lengths.\n",
    "\n",
    "This is needed for the following computation of their distances.\n",
    "\n",
    "This normalization can be switched off by setting `norm=None` parameter of `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Describe in writing the key differences between BoW and TF-IDF models of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Describe in writing what is an idea of word embedding. What are its advantages in comparison with other vectorization techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Come up with two sentences with high cosine similarity and two whose similarity is exactly zero. Compute these similarities using the code that has been used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Compute word mover's distances for the sentences from the previous exercise. Use Word2vec model trained on `text8` corpus or download the pretrained model `glove-wiki-gigaword-50`. Compare the distances with cosine similarity. What method produces more reasonable results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Below you will find a piece of text. Split it to sentences and create BoW model. Above we have used stemming for the analogous model. Use lemmatization instead. Do not forget that lematization may require whole sentences to identify parts of speech. It means that the stopword removal must be done after lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The skull and the upper bones lay beside it in the thick dust, \n",
    "and in one place, where rain-water had dropped through a leak in the \n",
    "roof, the thing itself had been worn away. Further in the gallery was \n",
    "the huge skeleton barrel of a Brontosaurus. My museum hypothesis was \n",
    "confirmed. Going towards the side I found what appeared to be sloping \n",
    "shelves, and clearing away the thick dust, I found the old familiar \n",
    "glass cases of our own time. But they must have been air-tight to \n",
    "judge from the fair preservation of some of their contents.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Below you will find a list of tweets. Create TF-IDF model for them. For tokenization use TweetTokenizer provided by NLTK. Using cosine similarity find two most similar teats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "\"@Tatiana_K nope they didn't have it \",\n",
    "\"@twittera que me muera ? \",\n",
    "\"spring break in plain city... it's snowing ))) \",\n",
    "\"I just re-pierced my ears \",\n",
    "\"@caregiving I couldn't bear to watch it.  And I thought the UA losssssss was embarrassing . . . . .\",\n",
    "\"@octolinz16 It it counts, idk why I did either. you never talk to me anymore \",\n",
    "\"@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\",\n",
    "\"@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!\",\n",
    "\"Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\",\n",
    "\"about to file taxes \",\n",
    "\"@LettyA ahh ive always wanted to see rent  love the soundtrack!!\",\n",
    "\"@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks? \",\n",
    "\"@alydesigns i was out most of the day so didn't get much done ;) \",\n",
    "\"one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh* \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "ru",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "113px",
    "width": "228px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
